<|startofpaper|>
## DeepCritic : Deliberate Critique with Large Language Models

Wenkai Yang , Jingwen Chen 1 ∗ 2 , Yankai Lin † 1 , Ji-Rong Wen 1 1 Gaoling School of Artificial Intelligence, Renmin University of China 2 School of Computer Science and Technology, Beijing Jiaotong University {wenkaiyang, yankailin}@ruc.edu.cn

## Abstract

As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K longform critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback. 3

## 1 Introduction

Large Language Models (LLMs) [29, 32, 23] have demonstrated superior performance that even surpasses human capabilities across a wide range of tasks [16, 25, 30]. LLMs achieve strong and generalizable performance by training on human-provided knowledge data [31]. This makes their evolution highly dependent on the effective human supervision [1]. However, as LLMs become increasingly intelligent, providing effective and scalable human supervision will become highly challenging, as collecting human feedback would be too costly and difficult [35].

LLM critics [27], which leverage LLMs as critique models, have recently emerged as a promising approach to enabling scalable oversight [35] on evolving LLMs. LLM critics generate critiques of LLM-generated content, which identify flaws and errors to help the LLM generator refine its outputs, paving the way for the automatic supervision and continuously improvement of LLMs [26, 17, 7, 38].

∗ The work was done while Jingwen Chen was at internship in Renmin University of China.

† Corresponding author.

3 Data and models are available at https://github.com/RUCBM/DeepCritic .

<!-- image -->

Figure 1: Comparison of critiques generated by current LLM critics and our developed critic. The red highlights in the solution steps represent the erroneous part. The critiques of current LLM critics (e.g., Qwen2.5-7B-Instruct) are overly superficial, primarily consisting of declarative statements rather than in-depth analysis or critical evaluation. In contrast, our critic can generate a deliberate reasoning process before making a judgment, incorporating iterative evaluation , multi-perspective verification , and meta-critiquing .

The effectiveness and the potential of the utilizing LLM critics heavily depends on the inherent critique capabilities of the LLM critics. However, existing studies have shown that current LLM critics still exhibit limited critique capabilities in complex domains such as mathematical reasoning tasks [53, 54], making it difficult to provide accurate and reliable feedback. By analyzing the critiques generated by existing LLM critics on math problems and solutions, we find that their generated critiques are often overly superficial and lack critical thinking, as shown in the examples in Figure 1. In particular, their critiques merely follow and echo the same reasoning process of the original steps, re-iterating rather than challenging them with their own critical reasoning, leading to a premature judgment result. This behavior can lead to two problems: (1) The critiques of reasoning steps lack careful deliberation, leading to low accuracy in the judgment results. (2) The critiques lack informativeness, offering limited guidance for the LLM generator to perform targeted refinements.

In this work, we aim to address the aforementioned limitations of shallow critiques generated by the LLM critics, particularly in the domain of mathematical reasoning. Specifically, we propose the DeepCritic framework, which enhances the critique capabilities of LLMs through a two-stage training pipeline. In the first stage, to enable LLMs to acquire a preliminary capability for generating fine-grained critiques, we first curate a dataset of 4.5K long-form critiques by iteratively prompting Qwen2.5-72B-Instruct [32] to critique on a small subset of labeled solutions in PRM800K [21]. Each above critique includes step-wise critiques of all reasoning steps if the solution is correct, or up to the first erroneous step otherwise. When constructing each step-wise critique, we first generate an initial and preliminary critique of the specified reasoning step. Then, in order to enable our critic to conduct critiques more critically and from more diverse perspectives, we further generate an in-depth critique of the initial critique. The in-depth critique is supposed to validate the step from alternative perspectives and critically examine the initial critique itself. Finally, we merge the initial

and in-depth critique into one deliberate critique for the specified step. By supervised fine-tuning on the curated and filtered critique data, we obtain an initial critique model that is capable of performing multi-perspective evaluation and meta-critiquing through reflection on and correction of its prior erroneous judgments. Then, in the second stage, we perform reinforcement learning (RL) to the SFT model to further boost its deep critique ability. We perform RL under two different settings based on the source of RL data: (1) When human-labeled data is available, such as PRM800K, we directly use it for RL; (2) Otherwise, we automatically generate annotated RL data through Monte Carlo sampling-based correctness estimation on each reasoning step [40] and then perform RL.

Experimental results show that our developed deep critic significantly outperforms existing process reward models (PRMs) and LLM critics (including the advanced R1-distilled reasoning models [6] and GPT-4o [29]) on various error identification benchmarks, demonstrating the effectiveness of our pipeline in enabling LLM critics to provide more accurate supervision. Furthermore, we demonstrate promising test-time scaling properties for both our deep critic and the LLM generator within our framework: (1) the judgment accuracy of the critic consistently improves with increased test-time sampling effort; and (2) the performance of the LLM generator is effectively enhanced either by employing our deep critic as a verifier in majority voting or through refinement guided by the critic's more informative feedback.

## 2 Related Work

Critique Ability of LLMs In the current era of rapidly evolving LLMs, leveraging and improving the critique ability of LLMs to facilitate effective scalable oversight [2, 35] and superalignment [3, 49] has become increasingly important. Regarding the utilization of the critique ability of LLMs, LLM-as-aJudge [10] and LLM-as-a-Critic [27] serve as promising directions for facilitating automatic supervision of LLM generations [55], enabling the self-evolution of LLMs [52, 42], and refining LLM outputs at test time [26]. Benchmarking the critique ability of LLMs [24, 19, 22] paves the way to better understanding the potential and limitations of current LLMs on critique tasks. Finally, a series of studies aim to create more powerful critique models on mathematical reasoning [46, 56, 7, 44, 38], code generation [27, 45] or other open-domain tasks [17]. This work analyzes the limitations of current math critique models and proposes a novel and effective pipeline to enhance the math critique ability of LLMs.

Reasoning Ability of LLMs The reasoning abilities of LLMs has long been a topic of widespread interest in the community. Previous studies explore LLM reasoning in various domains, such as code [34, 14], math [23, 48], commonsense knowledge [33, 9], etc. This work mainly focuses on the math reasoning domain. The studies in this line can be divided into several categories: (1) Designing diverse and challenging math reasoning benchmarks to probe the boundaries of existing LLMs' reasoning abilities [4, 12, 8]. (2) Enhancing the math reasoning abilities of LLMs in the training time either by collecting high-quality math datasets for fine-tuning [51, 20], or by proposing advanced optimization algorithms [18, 5]. (3) Improving reasoning accuracy by increasing the test-time compute either by performing search-based sampling [41, 43] with process reward models (PRMs) [21, 40], or by extending the Chain of Thought (CoT) length [30, 6, 50]. This work, taking a pioneer step to have the critique model provide judgments after detailed and deliberate reasoning, can also improve LLMs' math reasoning by providing accurate and detailed feedback on erroneous solutions and assisting LLMs in correcting them.

## 3 Methodology

## 3.1 Problem Formulation

Here, we introduce the preliminaries of our studied critique problem setting. Let D = {( P,S )} denote a dataset comprising pairs of problems P and their corresponding solutions S . Each solution S is in a step-by-step format denoted as S = ( s , s 1 2 , /uni22EF , s n ) , where s i represents the i -th step. Denote θ c as an LLM critic whose role is to review each step in S sequentially, identify in which step the first error occurs, and return the step index of that first erroneous step. If all steps are deemed correct, it returns -1 to indicate that the entire solution is correct. Formally, the output of the critic can be formulated as

$$( c _ { 1 }, j _ { 1 }, \cdots, c _ { k }, j _ { k }, a ) \sim \pi _ { \theta _ { e } } ( \cdot | P, s _ { 1 }, \cdots, s _ { n } ),$$

Figure 2: The two-stage pipeline of training our deep critique models. In Stage 1, we first utilize Qwen2.5-72B-Instruct to generate an initial step-wise critique for each step in the solution, followed by an in-depth critique of the initial critique. Then, we use Qwen2.5-72B-Instruct to merge these two critiques into one deliberate critique in the long-CoT form. Finally, we perform SFT on above created critique data to teach the model the format of deliberately critiquing. In Stage 2, we perform RL to the SFT model on either existing human-annotated data or auto-labeled data via Monte Carlo sampling-based correctness estimation, to further stimulate the critique ability of the critic.

<!-- image -->

where c i represents the CoT critique of step s i , j i ∈ { 1 , - } 1 represents the judgment result (i.e., 1 for correct or -1 for incorrect) indicating the correctness of s i . The final judgment result a is equal to k if j k = -1 , indicating the first incorrect step; otherwise, if all j i = 1 for i ≤ k = n , then a = -1 .

As mentioned before (refer to Figure 1), current LLM-based critique models exhibit limitations in conducting thoughtful critiques, as their step-wise critiques c i tend to be overly brief and superficial, often echoing the original reasoning content without deeper and critical analysis. Thus, their critique performance is greatly limited as shown in previous studies [53, 54], and the shallow, uninformative critiques fail to provide useful guidance for the LLM generator to revise its solutions. In this work, we aim to improve the LLM's critique ability and enable it to produce more deliberate and thoughtful critiques c i before making the judgment result j i , enhancing both the accuracy and quality of its generated critiques.

## 3.2 DeepCritic : Deliberate Critique Model

In this section, we will introduce our two-stage pipeline to create deliberate critique models in detail, including the SFT data generation and training stage in Section 3.2.1, and the RL data curation and optimization stage in Section 3.2.2.

## 3.2.1 Teaching LLMs to Deliberately Critique

Given that existing LLM critics struggle to produce well-reasoned and in-depth critiques, our first stage aims to teach LLMs how to deliberately critique. In this stage, we first leverage Qwen2.5-72BInstruct [32] to iteratively perform initial and in-depth critiquing, and then merge the two critiques into a single long-form critique to serve as the seed critique. We subsequently perform SFT on the curated seed critique data to teach the target model the format and structure of deliberate critiquing. The brief illustration is displayed in the left part of Figure 2, and the detailed procedure is described below.

Initial Critique Generation First, we sample a small set of labeled data from PRM800K [21] as the seed task inputs. Each task input contains a problem P and a step-by-step solution S = ( s , 1 /uni22EF , s n ) ,

Table 1: Statistics of step-level critiques in our SFT dataset, categorized by the correctness of their corresponding initial critiques.

|   Label of Reasoning Step |   # Correct Initial Critiques |   # Incorrect Initial Critiques |
|---------------------------|-------------------------------|---------------------------------|
|                         1 |                         22968 |                             738 |
|                        -1 |                          3535 |                             565 |

along with the human-labeled label l i ∈ { 1 , - } 1 4 indicating the correctness of each reasoning step s i . Thus, we can create the step-by-step critiques on these seed inputs using an LLM θ ∗ , which is chosen as Qwen2.5-72B-Instruct in this work. However, instead of creating the step-by-step critique of the entire solution directly in a single pass just like Eq. (1), which often leads current LLMs to generate overly brief critiques for each step as mentioned before, we adopt an approach that critiques each step independently. Specifically, in each prompting of Qwen2.5-72B-Instruct, we provide the problem and entire solution as inputs, but instruct Qwen2.5-72B-Instruct to critique only one specified step:

$$( c _ { i } ^ { i n i t }, j _ { i } ^ { i n i t } ) = \pi _ { \theta ^ { * } } ( \cdot | P, s _ { 1 }, \cdots, s _ { n }, s _ { t a r g e t } = s _ { i } ), \ \ i = 1, \cdots, k,$$

where s target is the additional requirement that specifies the i -th step to be critiqued only, ( c init i , j init i ) represents the initial CoT critique and judgment result of the specified step, k represents the index of the first step where l i = -1 , or k = n when all l i is 1.

In-Depth Critique Generation After initial critique generation, we find that many of the initial critiques merely adopt a direct verification approach that directly follows the logic of the original reasoning steps and perform repetitive or similar calculations, resulting in relatively low accuracy when identifying incorrect steps. To enable our critique model to learn to perform critical evaluations, we introduce a second round of in-depth critique generation based on the initial critiques. Specifically, for each reasoning step in the solution, we instruct Qwen2.5-72B-Instruct again to either assess the reasoning step from a different perspective or using a different evaluation method than that used in the initial critique, or to critique the initial critique itself in order to identify whether there exist flaws in the initial critique that lead to the incorrect judgment about the reasoning step. Therefore, the in-depth critique c deep i and its judgment result j deep i are generated as

$$( c _ { i } ^ { d e e p }, j _ { i } ^ { d e e p } ) = \pi _ { \theta ^ { * } } ( \cdot | P, s _ { 1 }, \cdots, s _ { n }, c _ { i } ^ { i n i t }, j _ { i } ^ { i n i t }, s _ { t a r g e t } = s _ { i } ), \ \ i = 1, \cdots, k.$$

This process allows initial critiques that previously led to mismatches between the initial judgment result and the ground truth label (i.e., j init i ≠ l i ) to be revised into correct critiques. Then, we only retain the solutions in which the in-depth judgment results of all steps in the solution align with the ground truth labels (i.e., j deep i = l i , ∀ = i 1 , /uni22EF , n ), as well as their initial and in-depth critiques. Table 1 shows the proportion of step-level critiques in our final SFT dataset that are successfully corrected through the second-round in-depth critique generation process. We observe that a certain number of step-level critiques benefit from the in-depth critique generation process. Incorporating these samples into training equips the model with the capabilities of reflection and self-correction in critiquing.

Final Critique Synthesis In the last step of SFT data generation, we use in-context learning with two manually-written examples to instruct Qwen2.5-72B-Instruct to merge the initial and in-depth critiques of each step into a single deliberate critique:

$$( c _ { i } ^ { f i n a l }, j _ { i } ^ { f i n a l } ) = \pi \theta ^ { * } ( \cdot | c _ { i } ^ { i n i t }, j _ { i } ^ { i n i t }, c _ { i } ^ { d e e p }, j _ { i } ^ { d e e p }, \{ e x _ { l } \} ), \ \ i = 1, \cdots, k,$$

where { ex l } are in-context learning examples. Finally, we only need to concatenate all step-level critiques to form a complete solution-level critique:

$$C = ( c _ { 1 } ^ { f i n a l }, j _ { 1 } ^ { f i n a l }, \cdots, c _ { k } ^ { f i n a l }, j _ { k } ^ { f i n a l }, a ), \quad a = \begin{cases} k & \text{if $j_{k}^{f i n a l } = -1$,} \\ - 1 & \text{if $j_{k}^{f i n a l } = 1$.} \end{cases}$$

Such deliberate critiques enable the model to perform iterative evaluations, multi-perspective verifications, reflection, and meta-critiquing in the inference stage, thereby improving its judgment accuracy. All prompt templates used in the seed data generation stage and the corresponding generation hyper-parameters can be found in Appendix A and Appendix B.1 respectively.

4 In the original PRM800K dataset, there are some steps labeled with 0, indicating that these steps is not incorrect but do not make any progress. We consider the label for these steps to be 1.

Supervised Fine-Tuning In total, we obtain approximately 4.5K seed solution-level critiques, and their label distribution (i.e., the distribution of the step index of the first erroneous step) is shown in Figure 7. We then perform SFT on the target model to teach it the format for performing deliberate critique and obtain an initial critique model θ SFT :

$$\theta _ { S F T } = \underset { \theta } { \arg \min } \mathbb { E } _ { ( P, S, C ) \sim \mathcal { D } _ { S F T } } [ - \log P _ { \theta } ( C | P, S ) ],$$

where D SFT is the SFT critique dataset in which the input includes the problem and the solution, and the output is the solution-level deliberate critique.

## 3.2.2 Incentivizing LLMs to Deliberately Critique

Once the seed critique model has acquired a certain level of critique capability, in the second stage, we aim to stimulate and elicit its full potential through continued incentivization . We follow the recent exciting advancements in LLM reasoning domain [30, 6, 13] to employ reinforcement learning (RL) on θ SFT in the second stage's training.

The acquisition of RL data is critical in RL stage. In the following, we explore RL under two different settings based on the sources of data. (1) First, the ideal source for RL should ne the high-quality labeled data obtained through human annotation. Therefore, in the first setting we directly use the existing human-labeled dataset PRM800K [21] for RL. (2) However, in some cases human annotation may become impractical or even infeasible due to high cost. Thus, in the second setting where human annotation is infeasible, we construct the task data automatically via a Monte Carlo samplingbased correctness estimation method [40]. Specifically, we sample a portion of GSM8K, MATH and Olympiads problems from NuminaMath-CoT dataset [20], and leverage Qwen2.5-1.5B/3B/7BInstruct [32] to generate multiple step-by-step solutions for each problem. Problems where all solutions are either fully correct or fully incorrect are discarded, as such cases are deemed too easy or too challenging. Then, for each incorrect solution, to measure the correctness of a specific step s i , we follow Wang et al. [40] to truncate the solution after s i , and use an LLM generator (i.e., Qwen2.5-7B-Instruct in this work) to rollout the subsequent reasoning path N times independently. We define the first erroneous step as the first step from which, along with all subsequent steps, all rollouts generated by the generator are incorrect; while for its all preceding steps, more than half of rollouts reach the correct answers. If such steps do not exist, we discard those incorrect solutions. For solutions with correct final answers, prior studies [54, 38] have pointed out that their intermediate steps can still be incorrect. Therefore, we perform the same Monte Carlo sampling procedure and assign a label of -1 only to those correct solutions whose all intermediate steps have corresponding rollouts where more than half reach the correct answers .

The illustration of above data construction procedure is shown in the right part of Figure 2. The detailed data generation settings are put in Appendix B.1. In our experiments, we explore training the seed critique model with RL either using 40.7K PRM800K data or 14.2K automatically constructed data. The label distributions of these two data sources are shown in Figure 8 and Figure 9 respectively.

## 4 Experiments and Analysis

## 4.1 Experimental Settings

Base Model We choose Qwen2.5-7B-Instruct as the initial base model. We first perform SFT to get our seed critique model DeepCritic-7B-SFT . Then, we perform RL on two distinct types of RL data separately, resulting in two variants: DeepCritic-7B-RL-PRM800K and DeepCritic-7B-RLNumina .

Benchmarks We select three widely used error identification benchmarks to systematically evaluate the critique and judgment performance of each model, including the subset of MR-GSM8K [53] in which the questions are from original GSM8K [4] dataset, the Phase-2 test set 5 of PRM800K [21], and ProcessBench [54]. Each testing example in all datasets contains a problem, a step-by-step solution and a label that either represents the step index of the first erroneous step or is -1 if the solution is entirely correct. The detailed description of the three benchmarks is provided in Appendix B.2.

5 https://github.com/openai/prm800k/blob/main/prm800k/data/phase2\_test.jsonl

Table 2: The evaluation results of various PRMs, instruction-followed LLMs that are served as critique models and our critique models on three benchmarks assessing the mathematical critique ability. The reported metric is the F1 score [54] (i.e., harmonic mean) of the judgment accuracy on incorrect solutions and the judgment accuracy on correct solutions.

|                                                  |                                                  |                                                  | ProcessBench                                     | ProcessBench                                     | ProcessBench                                     | ProcessBench                                     | Avg.                                             |
|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|
| Model                                            | MR- GSM8K                                        | PRM800K                                          | GSM8K                                            | MATH                                             | Olympiad- Bench                                  | Omni-Math                                        |                                                  |
| Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     | Process Reward Models (PRMs)                     |
| Math-Shepherd-PRM-7B                             | 61.8                                             | 21.7                                             | 48.2                                             | 27.1                                             | 20.5                                             | 16.3                                             | 32.6                                             |
| RLHFlow-PRM-8B-Mistral                           | 66.6                                             | 25.2                                             | 50.9                                             | 32.0                                             | 13.8                                             | 15.7                                             | 34.0                                             |
| RLHFlow-PRM-8B-DeepSeek                          | 44.8                                             | 18.5                                             | 32.3                                             | 34.2                                             | 16.0                                             | 18.3                                             | 27.4                                             |
| Qwen2.5-Math-7B-PRM800K                          | 70.8                                             | 55.6                                             | 70.5                                             | 64.7                                             | 50.0                                             | 42.7                                             | 59.7                                             |
| Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models | Large Language Models, served as Critique Models |
| LLaMA3.1-8B-Instruct                             | 31.6                                             | 16.0                                             | 23.8                                             | 18.9                                             | 18.3                                             | 17.2                                             | 21.0                                             |
| Qwen2.5-7B-Instruct                              | 48.1                                             | 25.6                                             | 42.9                                             | 36.6                                             | 25.5                                             | 25.9                                             | 34.1                                             |
| Qwen2.5-Math-7B-Instruct                         | 35.6                                             | 19.4                                             | 23.1                                             | 22.0                                             | 9.2                                              | 10.4                                             | 20.0                                             |
| DeepSeek-R1-Distill-Llama-8B                     | 69.4                                             | 55.7                                             | 65.0                                             | 62.7                                             | 58.4                                             | 51.7                                             | 60.5                                             |
| DeepSeek-R1-Distill-Qwen-7B                      | 77.9                                             | 57.4                                             | 71.9                                             | 69.9                                             | 56.4                                             | 46.8                                             | 63.4                                             |
| LLaMA3.1-70B-Instruct                            | 72.4                                             | 34.1                                             | 72.5                                             | 47.6                                             | 41.0                                             | 36.8                                             | 50.7                                             |
| Qwen2.5-72B-Instruct                             | 72.6                                             | 45.3                                             | 72.2                                             | 52.4                                             | 41.9                                             | 43.1                                             | 54.6                                             |
| Qwen2.5-Math-72B-Instruct                        | 73.6                                             | 41.0                                             | 68.6                                             | 48.5                                             | 28.6                                             | 27.3                                             | 47.9                                             |
| GPT-4o                                           | 69.7                                             | 45.9                                             | 72.1                                             | 57.3                                             | 50.5                                             | 53.4                                             | 58.2                                             |
| Our Critique Models                              | Our Critique Models                              | Our Critique Models                              | Our Critique Models                              | Our Critique Models                              | Our Critique Models                              | Our Critique Models                              | Our Critique Models                              |
| DeepCritic-7B-SFT                                | 67.1                                             | 48.0                                             | 59.2                                             | 61.2                                             | 46.0                                             | 43.0                                             | 54.1                                             |
| DeepCritic-7B-RL-Numina                          | 77.2                                             | 55.9                                             | 70.7                                             | 65.9                                             | 57.6                                             | 53.5                                             | 63.5                                             |
| DeepCritic-7B-RL-PRM800K                         | 77.3                                             | 60.1                                             | 74.0                                             | 72.9                                             | 60.9                                             | 57.2                                             | 67.1                                             |

Baselines We compare our critique models against two categories of baselines: (1) Process Reward Models (PRMs): In this category, we select Math-Shepherd-PRM-7B [40], RLHFlow-PRM-8BMistral/DeepSeek [47], Qwen2.5-Math-7B-PRM800K [54] for comparison. (2) LLMCritics : We prompt the following leading LLMs to serve as critique models: LLaMA3.1-8B/70B-Instruct [28], Qwen2.5-7B/72B-Instruct [32], Qwen2.5-Math-7B/72B-Instruct [48], and GPT-4o [15]. Also, we include two of the most advanced reasoning LLMs, DeepSeek-R1-Distill-Llama-8B and DeepSeekR1-Distill-Qwen-7B [6], and use them as reasoning-enhanced critique models for comprehensive comparison.

Training Settings In SFT stage, the learning rate is 1 × 10 -5 , the batch size is 64 , and we fine-tune for 3 epochs. In RL stage, we adopt verl [37] as our training framework, and use Group Relative Policy Optimization (GRPO) [36] as RL algorithm. In RL, an accuracy reward of 1.0 is given if the final judgment is correct; otherwise, it is 0.0. During RL, we observe that in very few cases, the policy model generates critiques that are mixed with different languages, which is consistent with the findings in DeepSeek-R1 [6]. However, this issue gradually diminishes as training progresses, so we do not introduce a language consistency reward here. The detailed training settings in both SFT and RL stages are in Appendix B.3.

Evaluation Settings In our main evaluation, we use consistent sampling settings across all critique models, with temperature set to 0.6, top\_p to 0.9, and max\_generation\_length to 32K during inference. We only sample once for each task input, while we explore the performance of majority voting over eight samplings in Section 5.1. The evaluation prompt is mainly based on [54], and is put in Appendix B.4. The main evaluation metric is the F1 score [54], which is the harmonic mean of the judgment accuracy on the step index of first erroneous step in incorrect solutions and the judgment accuracy on correct solutions. Further details on the evaluation settings are provided in Appendix B.4.

## 4.2 Main Results

The overall results on all benchmarks are displayed in Table 2. We put the detailed results of separate accuracy on both incorrect and correct solutions in Appendix F.

<!-- image -->

(a) Results on MR-GSM8K

<!-- image -->

<!-- image -->

(b) Results on PRM800K

<!-- image -->

(c) Results on PB-GSM8K

<!-- image -->

(d) Results on PB-MATH

<!-- image -->

(e) Results on PB-OlympiadBench

(f) Results on PB-Omni-Math

Figure 3: Majority voting results (Maj@8) of each model across all benchmarks. Pass@1 results are from Table 2. 'PB' denotes ProcessBench.

First, we can observe that existing instruct models, especially those of small sizes, exhibit very limited critique capabilities, as reflected in their poor judgment performance. As the model size increases, the corresponding critique capability also increases. Second, improvements in the model's reasoning ability have a positive impact on its critique capability. This is reflected in that the strong reasoning abilities of the DeepSeek-R1-Distill models obtained in other domains can be transferred to the critique task and bring substantial performance gains. Third, our seed critique model DeepCritic7B-SFT, trained on 4.5K carefully curated deliberate critique data, achieves a 20-point F1 score improvement (34.1 → 54.1) over the corresponding base model Qwen2.5-7B-Instruct . Its overall performance is even comparable to that of Qwen2.5-72B-Instruct. This demonstrates the high quality of our constructed seed critique data and validates our motivation that teaching LLMs to perform deliberate critiquing can indeed lead to significant performance improvements.

Regarding the RL performance, we can see that RL with only 14.2K automatically constructed data (i.e., DeepCritic-7B-RL-Numina) can effectively boost the model's critique performance from 54.1 to 63.5 . This validates the potential of automatically constructing supervision data and paves the promising way for the automated scalable oversight. Furthermore, when trained with larger scale RL data with higher quality, the resulted model DeepCritic-7B-RL-PRM800K outperforms all baselines, including GPT-4o and two same-sized DeepSeek-R1-Distill models, in 5 out of 6 evaluation sets and achieves the best overall performance . All in all, the above experimental results demonstrate that our proposed two-stage training paradigm is highly effective in enhancing the critique and verification capabilities of LLMs.

## 5 Test-Time Scaling Results

In this section, we explore the test-time scaling properties within the critique framework, from the perspectives of both critics and generators. In the following experiments, we choose our most powerful critique model so far DeepCritic-7B-RL-PRM800K as the target model, and refer to it as DeepCritic-7B-RL for brevity. We take the base model Qwen2.5-7B-Instruct along with the strongest baseline DeepSeek-R1-Distill-Qwen-7B (abbreviated to DS-R1-Distill-Qwen-7B) for comparison.

## 5.1 Test-Time Scaling Results of Critics

Here, we investigate the effectiveness of the majority voting practice [41] in enhancing the critique performance. For each critique model, the final judgment on each input is the majority voting

Figure 4: Verified majority voting results of Qwen2.5-7B/72B-Instruct on MATH500 and AIME20242025 by taking different models as verifiers.

<!-- image -->

result over eight samplings (Maj@8). We put the comparison results between Maj@8 and Pass@1 in Figure 3. As shown in Figure 3, the majority voting practice improves performance across all models. The Maj@8 results of our critique model outperform DeepSeek-R1-Distill-Qwen-7B in half of the settings, and the average F1 score of our model (70.5) is also higher than that of DeepSeek-R1-Distill-Qwen-7B (69.9), demonstrating the good test-time scaling property of our critique model.

## 5.2 Test-Time Scaling Results of Generators

Critics can also be used to improve the performance of LLM generators via scaling the test-time compute of generators. On the one hand, similar to the role of PRMs, critics can act as verifiers to assess whether the responses sampled by the generator are correct. By filtering out identified incorrect responses, more accurate majority voting results of solutions can be achieved. On the other hand, the generator can revise potentially erroneous responses based on the feedback from the critic, thereby arriving at the correct answer through refinement. In the following, we delve into above two aspects separately. We select two generators of different sizes for experiments: Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct. We conduct evaluations on MATH500 and AIME2024-2025. We use Math-Verify 6 as the rule-based verifier to determine whether the predicted answer matches the ground truth answer.

## 5.2.1 Results of Verified Majority Voting

We put the majority voting results under different numbers of sampled solutions from the generators, filtered taking the critique model as the verifier, 7 in Figure 4. The sampling temperature for generators is set to 1.0. 8 We observe that when the critique model performs poorly (e.g., Qwen2.5-7B-Instruct), using it as the verifier in majority voting can be counterproductive. In contrast, our critique model can yield greater improvements to generators' majority voting performance in most sampling settings compared to baselines.

## 5.2.2 Results of Critique-Based Refinement

In this setting, we first prompt generators to produce step-by-step solutions for each problem. Then, we leverage each critic to critique the solutions and prompt the generators to revise those deemed

6 https://github.com/huggingface/Math-Verify

7 If all candidate solutions are identified as incorrect by the critique model, we fall back to perform majority voting over the original solutions.

8 In this setting and the following refinement experiments, to ensure that the generators produce responses in a required step-by-step format-enabling subsequent critiques by the critique model-we adopt a system prompt (refer to Appendix D) different from the original one used by Qwen2.5 models. As a result, the evaluation results in Figure 4 and Table 3 may differ from the original results.

Table 3: Results of critique-based refinement. 'w → c' represents the proportion of cases where the model initially produces a wrong solution but arrives at the correct answer after judgment and refinement, and 'c → w' represents the the ratio of cases where correct solutions turns into incorrect one after judgment and refinement. 'Acc.' represents the average accuracy on all testing samples. '*' denotes that the refinement results are biased due to DS-R1-Distill-Qwen-7B directly producing the correct answers during critique.

|                                   | Qwen2.5-7B-Instruct               | Qwen2.5-7B-Instruct               | Qwen2.5-7B-Instruct               | Qwen2.5-7B-Instruct               | Qwen2.5-7B-Instruct               | Qwen2.5-7B-Instruct               | Qwen2.5-72B-Instruct              | Qwen2.5-72B-Instruct              | Qwen2.5-72B-Instruct              | Qwen2.5-72B-Instruct              | Qwen2.5-72B-Instruct              | Qwen2.5-72B-Instruct              |
|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|-----------------------------------|
| Critique Model                    | MATH500                           | MATH500                           | MATH500                           | AIME24-25                         | AIME24-25                         | AIME24-25                         | MATH500                           | MATH500                           | MATH500                           | AIME24-25                         | AIME24-25                         | AIME24-25                         |
|                                   | w → c                             | c → w                             | Acc.                              | w → c                             | c → w                             | Acc.                              | w → c                             | c → w                             | Acc.                              | w → c                             | c → w                             | Acc.                              |
| before refinement                 |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |
|                                   | -                                 | -                                 | 74.00                             | -                                 | -                                 | 6.67                              | -                                 | -                                 | 77.00                             | -                                 | -                                 | 11.67                             |
| after refinement                  |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |                                   |
| Qwen2.5-7B-Instruct               | 0.80                              | 2.60                              | 72.20                             | 1.67                              | 0.00                              | 8.33                              | 1.60                              | 2.40                              | 76.20                             | 1.67                              | 0.00                              | 13.33                             |
| DeepCritic-7B-RL                  | 4.60                              | 1.40                              | 77.20                             | 5.00                              | 0.00                              | 11.67                             | 7.00                              | 2.00                              | 82.00                             | 5.00                              | 1.67                              | 15.00                             |
| after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) | after refinement (answer leakage) |
| DS-R1-Distill-Qwen-7B*            | 7.20                              | 1.20                              | 80.00                             | 8.33                              | 0.00                              | 15.00                             | 7.40                              | 1.00                              | 83.40                             | 3.33                              | 0.00                              | 15.00                             |

incorrect, based on the critic's feedback. We use greedy decoding for the generators for determinism. In experiments, we observe that DeepSeek-R1-Distill-Qwen-7B frequently continues critiquing until the end of the solution and produces the correct answer, even though we explicitly instruct the model in the system prompt to stop after identifying the first incorrect step. This issue can cause the refinement results to be biased and greatly influenced by DeekSeek-R1-Distill-Qwen-7B's own problem-solving capability. Therefore, we present its results independently for reference. The refinement results are shown in Table 3. We can see that our critique model can effectively assist the generators in correcting errors by providing more detailed feedback, leading to improved performance of the generators. Notably, our 7B critique model is also capable of supervising and correcting the outputs of a 72B generator, demonstrating a potential of weak-to-strong supervision [3].

## 6 Conclusion

In this work, we propose an effective pipeline to enhance the math critique ability of LLMs. We first carefully construct 4.5K long-form critiques incorporating multi-perspective verification and meta-critiquing. These serve as seed data for supervised fine-tuning, enabling the target model to acquire an initial ability of deliberately critiquing. We then further enhance the critique capability of the model via reinforcement learning. The deep critique model we developed demonstrates superior performance across a range of error identification benchmarks, and exhibits promising potential in supervising and improving the reasoning capabilities of LLM generators that are even more capable than itself. We hope our work provides valuable insights to advance future research in deliberate reasoning and scalable oversight.
<|endofpaper|>