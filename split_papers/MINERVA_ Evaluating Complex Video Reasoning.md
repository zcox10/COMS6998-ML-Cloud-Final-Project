<|startofpaper|>
## MINERVA: Evaluating Complex Video Reasoning

Arsha Nagrani , Sachit Menon , Ahmet Iscen , Shyamal Buch , Ramin Mehran , Nilpa Jha , 1 2 * 1 1 1 1 Anja Hauth , Yukun Zhu , Carl Vondrick , Mikhail Sirotenko , Cordelia Schmid , Tobias Weyand 1 1 2 1 1 1 1 Google DeepMind, 2 Columbia University

## Abstract

human performance on these recent benchmarks [15, 34].

Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-asa-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/googledeepmind/neptune?tab=readme-ov-file#minerva.

## 1. Introduction

Video understanding has long been a holy grail for the field of computer vision. Video benchmarks have evolved from testing action recognition on short clips [17, 26, 50] to more recent benchmarks testing advanced capabilities over longer time horizons (LVBench [45], VideoMME [15], Neptune [34], and others). The development of the latter has been supercharged by the rise of multimodal large language models [1, 6, 39], which perform complex video tasks on videos up to an hour long [45]), and are steadily approaching

* Work done during an internship at Google DeepMind

While these datasets aim to measure complex video understanding capabilities, they do not provide further insight into how or why models succeed or fail beyond the correctness of their final answers. One way to inspect model failures is by exposure to step-by-step rationales generated by the model. Attempts to improve these rationales have led to a paradigm shift towards models (particularly in the text domain), that spend more time 'thinking" [3] i.e. producing intermediate thoughts towards a final answer, often via additional compute at inference time [24, 42]. 'Thinking' or reasoning in the video domain, however, looks very different to reasoning in the text or coding domain. In the text domain, models can directly quote sections or edited versions of the input as thoughts - these intermediates are easier to assess by humans, or by another language model. This is not the case for the video domain. The high dimensionality and multi-modality of video means that solving a video task often requires temporal localization, perceptually recognizing key objects, events and actions (from multiple modalities such as speech or frames) and applying logical reasoning to tie these intermediates all together. Complex queries in video question answering (videoQA) hence naturally require multi-step processes to solve, where each step may require a different skill and/or access to a different modality input. We henceforth refer to this multi-step process as a 'reasoning trace' for videoQA. This includes the set of the steps required to solve the question, including perception and localization.

Yet, despite the fundamentally multi-step nature of this capability, existing video benchmarks only evaluate final answers: they only check the outcome and not the reasoning . It is not clear, however, if a model arrives at a correct answer due to a successful execution of key steps, pure chance, linguistic bias, or the process of elimination of answer choices. Conversely, if it fails to produce the correct answer, was it completely unable to solve the problem, or did it get close but ultimately make a mistake that changed the final answer? Using final answers alone makes it difficult to design datasets that will remain challenging but still provide signal; all the more pertinent as new models improve at breakneck speed.

Question: How many times does the ball change hands on the play that results in Pampanga having 52 points on the scoreboard?

(a) 5 (b) 3 (c) 4 (d) 2 (e) 1

<!-- image -->

I watched the play that increased Pampanga's score in the game to 52, which was played from 07:07 07:19, and counted the number of times the ball changed hands from the start of the play to the score. I counted the first possession change at 07:09, when the player with the number 8 jersey passes the ball to the player with the number 19 jersey. I watched the player with the number 19 jersey hand off the ball to the player in the number 15 jersey at 07:12, marking the second possession change. I then watched the third possession change take place from 07:13 - 07:14 when number 15 passed the ball back to number 19, who then scored at 07:16 to increase Pampanga's score to 52. Thus, three total possession changes occurred during the play before the basket.

Skills: Counting, Reading, Temporal Reasoning, Event Localization

Question: Using the magnitude of the gravitational force of the moon given in the video and a value of d=5, what would the answer be to the equation given at 03:16?

(a) 8.25 (b) 8.0 (c) 8.1 (d) 8.5 (e) 7.9

<!-- image -->

<!-- image -->

I heard the narrator say that weight relates to the force of gravity at 02:20. Then, I observed an example of the same mass on the earth versus the moon at 02:26. I read that the weight was mass times the force of gravity. I read that the force of gravity in the example for the moon was 1.62 m/s^2 at 02:26. I moved to 03:16 to find the equation. I heard the equation was "W = Fd" and heard that the variables were "Work" "Force" and "Distance". Therefore, I used the gravitational force of the moon for F and the value of 5 given from the question for D. I calculated (1.62 m/s^2)(5 m) = 8.1 J.

Question: Approximately how long would it take the woman to knead the dough if she was working twice as fast?

(a) 30 seconds ( b) 45 seconds (c) 2 minutes(d) 3 minutes (d) 20 seconds

<!-- image -->

I watched the video and kept track of how long it takes the woman to knead the dough in the video. She begins mixing the dough at 01:11 with a spoon to incorporate the water. Then, at 01:21, she begins mixing and kneading by hand. She continues kneading the dough until 02:40. Therefore, it takes her 1 minute and 29 seconds to knead the dough in the video, or 89 seconds. If she was working twice as fast, it would have taken her approximately 45 seconds.

Skills: Counting, Numerical Reasoning, Counterfactual

Question: Why does the gravedigger pull a gun on the man in the khaki suit?

(a) To save his parents (b) To save the strangers (c) To save his children (d) To save himself (e) To save his enemies

<!-- image -->

I watched the entire video and never saw the gravedigger interact with the two young men or acknowledge them, and therefore determined he did not know them. I watched the video until the 02:58 mark and saw that the man is digging a grave. At 03:02 there's a clear shot of him noticing the two men being held at gunpoint. I continued to 03:38 where it becomes clear the man in the khaki suit plans to kill the young men by covering them in driveway sealant. At 03:51 the gravedigger pulls a gun on the man in the khaki suit and commands him to stop what he's doing to the two young men. Therefore, it became clear that the gravedigger pulls a gun on the man in the khaki suit to save the lives of strangers.

Skills: Numerical Reasoning, Reading, Listening, Temporal Reasoning

Skills: Goal Reasoning, 'Event Occurrence, Temporal Reasoning

Figure 1. Examples from MINERVA: We introduce MINERVA, a complex video question-answering dataset. Unlike existing video datasets, the answer to each question is accompanied by a detailed reasoning trace , which outlines the steps required to come to the answer. Videos cover multiple domains such as (clockwise) - sports, cooking, short films and science lectures. Reasoning traces are detailed, including timestamps (highlighted in green) and key actions (highlighted in pink). We show a single frame from each video.

In light of this, we release MINERVA 1 - a benchmark for video reasoning which consists of question, answers and reasoning traces. MINERVA is challenging and complex every question requires multiple steps to solve, and even the best-performing frontier model (Gemini 2.5 Pro Thinking) achieves only 66.2% accuracy, while humans are able to achieve 92.5%. The dataset is hence well positioned for hill-climbing and future assessment of multimodal models. Every question in MINERVA requires complex reasoning using two or more skills (for example numerical reasoning, temporal reasoning, spatial navigation). Videos also span multiple domains (short films, sports, instructional videos etc), with various video lengths (from 2 minutes to over 1.5 hours ), making the dataset diverse. For each question we also provide the hand-crafted, detailed reasoning trace, with the steps that are required to come to the correct answer. Unlike datasets that provide auxiliary information in a single format such as timestamps (LITA [21], CG-Bench [10] + others), the reasoning trace is an unconstrained block of text - allowing flexibility. Each reasoning trace does include timestamps where necessary to refer to relevant sections of the video, but also describes key actions, objects, as well as outlines logical reasoning steps (see Fig. 1 for examples).

identify failure modes. We use these failure modes to build a taxonomy of errors (in other words, a rubric) for video reasoning. This rubric is specific to the video domain, and highlights the following broad categories of errors - (1) Perceptual Correctness, (2) Temporal Localization, (3) Logical Reasoning and (4) Completeness. We use this rubric to assess how both humans and LLMs grade reasoning outputs along these criteria when given the ground truth reasoning, illuminating where automatic graders correlate to humans and where they may not, highlighting directions for potential improvement for reference-based metrics. In addition, we also find in some cases that LLM performance on video reasoning improves simply by being prompted with the rubric, in rough analogy to 'Let's think step by step' [27] in text reasoning.

We benchmark both open-source and proprietary models on our dataset, and manually compare some of their reasoning traces to the ground truth reasoning annotations to

1 Multimodal INterpretablE Reasoning Video Annotations. Minerva is also the Roman goddess of thinking and wisdom.

To summarize, we make the following contributions: (i) We introduce MINERVA, a challenging video reasoning benchmark for LMMs consisting of 1,515 hand-crafted questions. For each question, we provide 5 answer choices, as well as detailed, manually-annotated reasoning traces; (ii) We evaluate multiple frontier models on MINERVA, both open-source and proprietary, and perform ablations to show low textual bias and the importance of visual information; (iii) we perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We find that failure modes are primarily related to temporal localization and visual perception. Preliminary analysis of LLM-based metrics show areas

for future work.

## 2. Related Work

## 2.1. Recent VideoQA Datasets

A number of valuable video datasets have been proposed recently. TemporalBench [7] and PerceptionTest [36] include a variety of tasks, such as video QA, captioning, and grounding, but use relatively short videos (most videos a couple of minutes). Similarly, CinePile [38] and VideoVista [29] focus on short-form content (average length of 160 seconds). VideoVista [29] is notable for its broad coverage of 19 understanding and 8 reasoning tasks. These benchmarks, along with InfiniBench [5] (maximum 52-minute videos), and Neptune [34], rely on semi-automatic pipelines using LLMs for annotation. In contrast, our dataset is entirely manually annotated. TVBench [11] specifically addresses the challenge of temporally difficult answer candidates in VideoQA, emphasizing the importance of visual information. E.T. bench [32] focuses on multi-event and time-sensitive understanding, using human-annotated videos with a maximum length of 795 seconds. Video-MME [15] bridges the gap between short and long-form content, and datasets such as LVBench [45] (average length 68 minutes) and HourVideo [8] ( egocentric videos, 20 to 120 minutes) extending to even longer videos. In contrast to these existing benchmarks, our work provides not only the final outputs but also human-annotated reasoning traces, enabling future evaluations to assess the model's reasoning process in addition to its accuracy.

## 2.2. Reasoning Traces for Video

As discussed in the previous section, few datasets provide auxiliary dataset beyond final answers. ScienceQA [40] consists of science questions about images, and provides the accompanying lectures, which when used for training results in increased performance with fewer data points. Some recent works have aimed to develop automatic labeling pipelines to instead generate noisy auxiliary information at scale. Visual CoT [41] provides bounding box pseudolabels generated by off-the-shelf models for image VQA, while VideoEspresso [18] does something similar for video, constructing a pipeline connecting different frozen models together to label bounding boxes for sparse key frames. VideoCoT [47] is perhaps the closest to our work, aiming to generate text-based chain of thought for videos; however, we note key differences here. The primary goal of VideoCoT is to present a semiautomatic pipeline using LLMs and VLMs to scale up for training purposes, while we present a fully manually annotated, high quality dataset for evaluation purposes. The videos in VideoCoT are all taken from established Kinetics700 [26], making them shorter and limited in domain. Finally, their automated rationales tend to contain substantial information about the video that does not relate to the particular query, rather than providing specific reasoning for the given question.

## 2.3. Reasoning Analysis

With the growth of reasoning models, substantial work has been conducted into analyzing reasoning in text-only settings and remains an open problem [33]. Previous works demonstrate that correctness of final answers alone often does not indicate correct reasoning, commonly due to hallucination of reasoning steps or answers [16, 19, 37, 44]. These works fall into (i) reference-based, which use additional human annotation as reference, and (ii) reference-free (evaluation without such additional information). Unsurprisingly, multiple works find that reference-based evaluation proves consistently more reliable [13], and that language models are poor evaluators of reasoning in isolation [22, 25, 44]. In this work, we primarily focus on contributing a high-quality dataset with reference annotations for video reasoning, which may spur the development of and provide a comparison for further research into both reference-based and, by comparison, reference-free metrics for video reasoning.

Earlier work in reference-based evaluation (before LLMs) explored rule based methods such as n -grams for summarization with ROUGE scores [30] or for machine translation with BLEU [35]. ROSCOE [16] observed the need for evaluation of reasoning for early LLMs, inspired by chain-ofthought prompting [48], using various customized metrics such as word vector similarity. As LLMs advanced, they began to be used for evaluation themselves, such as via LLM-as-a-Judge [52]. MLLM as a judge [9] and LLaVACritic [49] show these capabilities in multimodal settings, but this has thus far been largely explored for the image-text domain. Unlike these works, we explore reference-based LLM-as-a-judge strategies for analysis of video reasoning traces. In both text and multimodal contexts, well-designed, itemized evaluation criteria for rating different model responses give substantially more consistent, well-aligned LLM scores [31, 52] - we follow along these lines of work by providing a rubric for scoring video reasoning, and provide a preliminary analysis of its effectiveness.

## 3. MINERVA

We introduce MINERVA, a videoQA dataset consisting of 1,515 challenging questions for videos of varying length. Each question has 5 answer options and a detailed reasoning trace accompanying the final answer (see Fig. 1 for examples). The key features of our dataset are: (i) multistep, complex questions. Each question requires multiple steps and multiple skills to solve, (ii) multimodality, with both frames and ASR needed for some questions, (iii) high quality - the entire dataset is hand crafted by experienced annotators, and most importantly (iv) intermediate reasoning - detailed, hand-crafted reasoning traces. These reasoning

<!-- image -->

Animals

Figure 2. Dataset statistics. Video lengths (left), lengths of answers and reasoning (middle), and domains (right). Videos cover a wide range of lengths, with some longer than 100 minutes. Every question comes with a reasoning trace which is long and detailed, mean number of words is 92 (middle). Domains are hand-selected to include videos that lend themselves well to complex reasoning questions.

traces allow us to perform an analysis of where models are making errors, beyond just their final answer outputs. We do this via reference-based analysis of model outputs, which can be performed solely in the text domain (and is therefore much cheaper than reference-free assessments). We showcase the value of this annotation via both human and LLM-based judgments of model outputs (in Sec. 5), based on a taxonomy of errors and detailed rubric system.

Statistics: Statistics are provided in Fig. 2. The dataset covers a wide range of video lengths, with the longest video being 100 minutes while the shortest video is less than 2 minutes (mean of 12 minutes). There are multiple questions per video, with the distribution following a min/mean/max of 1/6.8/10 questions per video (223 videos in total). Reasoning traces are long and detailed, with the mean number of words in a reasoning trace being 92 (Fig. 2). 99.6% of reasoning traces contain timestamps, with the mean number of timestamps per reasoning trace being 4.

## 3.1. Dataset Construction

As multimodal models continue to improve, it becomes increasingly non-trivial to come up with challenging questions. Questions should be complex, requiring multiple steps to solve, and not solvable with only the speech or external knowledge (full criteria provided to raters can be found in Appendix 8.1.1). Our dataset construction pipeline consists of the following steps:

- 1. Video Selection: We begin by selecting video domains from YouTube that lend themselves well to questions fulfilling the desiderata above.
- 2. Manual Annotation: Raters propose questions, answers and reasoning traces.
- 3. Quality Review: Questions are reviewed by other raters.
- 4. Adversarial Filtering: We attempt to mitigate textual bias using consensus from multiple frontier text-only models.

## 3.1.1. Video Selection

Video selection is non-trivial, as many videos online contain simple storylines, few shots, or are dominated by talking heads. To avoid this, we identify a (non-exhaustive) set of domains that lend themselves well to reasoning. These are described below (and shown in Fig. 2).

Short Films: We search for videos tagged as 'short film", which are complex, multi-event videos that typically have a story line. Short films tend to contain relationship and event arcs over varying time horizons. Note that this set does not include mainstream movies, so there is less chance of training data contamination or external knowledge bias.

Sports and Board Games: This set includes sports such as tennis, basketball, motorsports, hockey, cricket, and board games such as chess, scrabble and risk. These videos lend themselves well to questions that require reasoning against the rule set of the sport or game, as well as recognition of fine-grained actions and identification of player or piece positions on screen.

Educational: Inspired by reasoning works in the maths and textual domain, we identify a number of STEM lectures online. However this domain set makes up only 8% of the dataset (Fig. 2, right), as it is challenging to find lectures where the speech does not dominate (and visual reasoning is important).

Lifestyle: Our final domain category consists of VLOGstyle lifestyle videos, including cooking, general how-to videos, as well as travel vlogs and pet videos. These are typically multi-e vent videos where events may follow a logical causal flow ( e.g . steps in a recipe). Travel and vlogging style videos also add egocentric viewpoints, and lend themselves well to questions requiring spatial reasoning.

## 3.1.2. Annotation and Quality Review

Once videos are identified, raters then propose complex questions, answers, decoys, reasoning traces and label question

Table 1. Frame and ASR ablations on MINERVA .

| Method                | # of Frames   | ASR   |   MCQ-Acc. % |
|-----------------------|---------------|-------|--------------|
| GPT-4o [1]            | 64            | ✓     |        39.74 |
| GPT-4o [1]            | 250 (max)     | ✗     |        44.82 |
| GPT-4o [1]            | 250 (max)     | ✓     |        45.54 |
| Gemini 2.0 Flash [39] | 64            | ✓     |        47.85 |
| Gemini 2.0 Flash [39] | 256           | ✗     |        49.7  |
| Gemini 2.0 Flash [39] | 256           | ✓     |        53.47 |
| Qwen-2.5VL [6]        | 64            | ✓     |        33    |
| Qwen-2.5VL [6]        | 768 (max)     | ✗     |        34.39 |
| Qwen-2.5VL [6]        | 768 (max)     | ✓     |        35.05 |

types. The raters are instructed to attempt to propose questions such that each question requires at least two of the following skills: Temporal Reasoning, Counting, Cause and Effect, Goal Reasoning, Situational Awareness, Event Occurrence, State Changes, Reading (OCR), Listening (identifying a detail in the audio track), Spatial Perception, Numerical Reasoning (all math operations other than counting), Object Recognition, Counterfactual Reasoning ('what if', but with an objective outcome). Each annotation is verified by at least one other rater (details in Appendix 8.1.4). The annotations are also periodically reviewed by the papers' authors, who provide detailed feedback to the raters to ensure annotations are being provided in-line with guidelines.

## 3.1.3. Filtering for Quality and Biases

Finally, we take several steps to address quality and potential for biases in the final dataset. We begin by filtering examples where the groundtruth annotated reasoning traces are of low quality or too short (&lt;20 words). While raters are explicitly instructed to avoid proposing questions that can be solved from the ASR alone, we find (very few) examples that are possible to guess from text alone. We address these potential text-biases (both in terms of QAD-only and ASR-only) with adversarial filtering [20, 28]. Our filtering process consists of taking the consensus agreement (in order to avoid discarding difficult questions that models may have answered correctly by chance) across a diverse range of open- and closed- source text-only baselines (Deepseek, GPT-4o, Gemini-flash-002 and Qwen2.5-VL) with only QAD and/or ASR inputs, and best practices in prior work [45]. We provide additional details in the Appendix.

## 4. Benchmarking

## 4.1. Models

We benchmark a number of open-source and proprietary models, described below.

Blind Baselines: We first evaluate models using a textonly prompt in two settings. (i) The model is given only

Table 2. Blind Baselines on MINERVA. † Random performance.

| Method                | w/o ASR   |   w/ ASR |
|-----------------------|-----------|----------|
| Random                | 20.00     |    20    |
| GPT-4o [1]            | † 19.60   |    29.17 |
| Gemini 2.0 Flash [39] | 23.04     |    28.25 |
| Qwen-2.5VL [6]        | † 18.88   |    23.17 |
| DeepSeek [12]         | 21.45     |    22.51 |

the question, answer and decoys (QAD baseline). (ii) The model is additionally given an ASR transcript of the video (QAD+ASR baseline). This helps identify questions that can be answered by prior or commonsense knowledge, or from ASR alone without requiring visual information.

Video Models: Weexperiment with 3 of the best-performing open source VideoQA models, Qwen2.5-VL [6], VideoLLaMA3 [51] and InternVideo2.5 [46], which represent the current state-of-the-art in long context open multimodal models (we do not evaluate older versions superseded by these, e.g . VideoLLaMA2 or Qwen2). For proprietary models, we experiment with the Gemini model family [39] from Google DeepMind, GPT-4o [1] GPT-4.1 [2] and o1 [3] from OpenAI, as well as Claude 3.5 Sonnet v2 [4] from Anthropic. o1 and Gemini 2.5 [43] are reasoning or 'thinking' models, capable of reasoning through their thoughts before responding. For all models, we attempt to fit as many frames as possible - given API query limitations. We experiment with varying number of frames, and compare performance with and without ASR provided with the frames.

## 4.1.1. Prompts

We conduct an ablation on the impact of prompting styles on MINERVA with our best model. We try out 3 styles of prompting: (i) asking the model to answer the question directly; (ii) asking the model to 'reason step by step'; and (iii) additionally providing the model with the MINERVA rubric for video reasoning (described in Sec. 5.1.1). Prompts are provided in the Appendix 9.1. We then use the best result of this study to prompt all the other baselines. We tried to standardize prompts as far as possible, while attempting to incorporate author-provided model-specific prompts to give models the best possible advantage.

## 4.1.2. Implementation Details

For all models, we follow best practices and guidance from model/API authors. In general, we sample video frames uniformly from the videos and construct prompts by interleaving them with timestamps. Since many questions ask about specific times in the speech and the video, we also interleave the ASR with timestamps at five second intervals. We prompt models to provide their answer choice in a machine-readable format for parsing; if the model fails to provide a response or it is not readily parse-able, then we as-

sign a random answer choice for computing MCQ accuracy. We provide detailed settings in the appendix.

## 4.2. MCQ performance

Modality Ablations: QAD-only and QAD+ASR-only baselines with no visual information are provided in Tab. 2, with further modality ablations in Tab. 1. All QAD-only baselines get close to chance performance, indicating that the decoy answer choices do not offer cues to the correct answer. ASR-only baselines using 4 powerful language models obtain extremely low performance compared to those with frames, showcasing the necessity of visual information for MINERVA. We also note that for all 3 models in Table 1, increasing the number of frames from 64 frames leads to an increase in performance as well, with ASR providing complementary gains. Finer-grained analysis on the impact of varying the number of frames provided for inference can be found in Fig. 10 in the appendix.

Scoring final answers (MCQ): We provide the MCQ results in Tab. 4. Contrary to prior work [34], we observe that the gap between open-source and proprietary models has narrowed, with Qwen2.5-VL and InternVideo2.5 outperforming Claude Sonnet. Gemini 2.5 Pro Thinking sets the state-of-the-art in the dataset at 66.2%. With peak performance still far from human performance, we hope MINERVA will be a challenging benchmark to measure progress on video understanding.

Thinking Ablations: We experiment with the thinking mode of the recently released Gemini 2.5 models [43]. Results are provided in Tab. 5. We note that turning thinking mode on improves the performance of the model, with improvements more pronounced as the number of frames increases. This suggests that the ability to reason about long video content is necessary to achieve high accuracy scores on MINERVA.

Results by Skill/Domain/Length: Model performance by (a) skill, (b) video domain, and (c) video length is provided in Fig. 3. (We note that each question is tagged with multiple skills by construction, rather than each being associated with only one.) Models struggle most with counting questions, followed by counterfactuals and state changes. Gemini 2.5 Pro performs highest across all skills, with GPT-4.1 performing similar to Gemini 2.5 Pro for state changes and cause and effect. All models perform well in the domain of short films, while models struggle most on math videos. Humans perform well across all skills, with the lowest performance for counting, and perform well across all domains except for Tech/AI. The performance of all video models degrades as videos get longer, similar to reports in prior work [15]. Human performance is also highest for videos less than 5 minutes long (95.8%).

Prompt Ablations: Results are provided in Table 3. We find that asking the model to perform step-by-step reasoning

Table 3. Prompting Ablations on MINERVA. Results of Gemini 2.0 Flash, 256 frames + ASR. We provide both MCQ accuracy on the final answers and MiRA on the reasoning traces. † Very few reasoning outputs (by design) to assess.

| Prompting Method   |   MCQAccuracy | MiRA   |
|--------------------|---------------|--------|
| Direct Answer      |         46.47 | †      |
| + Reasoning        |         51.22 | 0.65   |
| + Minerva Rubric   |         53.47 | 0.75   |

rather than directly producing an answer results in a significant boost to MCQ accuracy. What is interesting however, is that explicitly providing the rubric in the prompt improves the final score even further (the reasoning outputs also improve, as shown by an automatic LLM judge (MiRA) which is described in Sec. 5.1.1). Note that this improvement comes with minimal extra inference-time compute (no multiple calls needed), and our rubric was designed to be as general as possible (does not contain any few-shot examples specific to the dataset). This suggests that asking models to provide reasoning along the four axes we identified in the rubric for video can actually improve final outcomes as well.

## 4.3. Human performance

We perform a human study by asking raters (a disjoint set from the ones who annotated the data) to answer the questions in the dataset. Results are provided in Table 4. We note that there is a big gap between model and human performance, with the best model (Gemini 2.5 Pro Thinking) still almost 30% behind human performance.

## 5. Analysis of Reasoning Traces

## 5.1. Assessing Methods for Video Reasoning

In this section, we explore methods to score step-by-step rationales generated by multimodal models. Besides increased performance [14, 48], prompting models to provide reasoning steps also greatly improves interpretability, but objectively studying their correctness and helpfulness (irrespective of the final answer) is still an open problem. While human assessments are usually the gold standard, they are expensive and time-consuming, and so we provide a preliminary exploration of LLM-based assessments as well. Within the field of reasoning analysis, a reference-free assessment would be able to score reasoning traces without access to ground truth traces, however would require access to the original video (and can therefore become prohibitively expensive as video length scales). Armed with the traces from MINERVA, instead we explore reference-based analysis, which can operate entirely in the lower-bandwidth (less expensive) text space. We also note that unlike the math or coding domain,

<!-- image -->

Travel

91.35

95.83

Motorsports

76.32

Tech/AI

90.70

Maths

97.31

Misc Sports

Tennis

How-To

80.49

95.24

95.08

88.43

Animals

Short Films

95.16

Basketball

5-15 min

Video Length

(b)

(c)

Figure 3. Model and human accuracy (MCQ) broken down by (a) skill, (b) video domain, (c) video length. Note that questions can belong to more than one skill for (a). We provide a common legend for all 3 plots (in the right). Best viewed in color and with zoom. Axes in the radar charts are scaled to the highest accuracy per dimension. The full results for (a) are provided in Tab. 13 and for (b) in Tab. 12 in the appendix.

Table 4. Benchmarking performance on MINERVA. We report multiple choice accuracy (MCQ-Acc.). * indicates FPS sampling up to frame limit, following optimal settings from [6, 51]. Since API limits restricted our run of OpenAI o1 and Claude 3.5 Sonnet v2 to 64 frames, we also report results with Flash and GPT-4o with 64 frames in Table 1. † For this model only, we found 256 frames performs slightly better than 2048 (see Appendix).

MiRA Scores (mean, normalized between 0 and 1)

| Method                         | # Frames   | ASR   | MCQ-Acc. %   |
|--------------------------------|------------|-------|--------------|
| Random                         | -          | -     | 20.00        |
| Open-source                    |            |       |              |
| Qwen2.5-VL [6]                 | *768       | ✓     | 35.05        |
| VideoLLaMA3 [51]               | *180       | ✓     | 35.91        |
| InternVideo2.5 [46]            | 256        | ✓     | 35.18        |
| Proprietary                    |            |       |              |
| Claude3.5 Sonnet v2 [4]        | 64         | ✓     | 31.28        |
| OpenAI o1 [23]                 | 64         | ✓     | 43.48        |
| GPT-4o [1]                     | 250        | ✓     | 45.54        |
| GPT-4.1 [2]                    | 256        | ✓     | 53.99        |
| Gemini 2.0 Flash [39]          | † 256      | ✓     | 53.47        |
| Gemini 2.5 Flash Thinking [43] | 256        | ✓     | 57.30        |
| Gemini 2.5 Pro Thinking [43]   | 1024       | ✓     | 66.20        |
| Human performance              | all        | ✓     | 92.54        |

Table 5. Thinking ablations on MINERVA with Gemini 2.5 Pro [43]. MCQ results are provided as Acc.%.

| Model   |   # Frames |   Thinking Off |   Thinking On |
|---------|------------|----------------|---------------|
| 2.5 Pro |        256 |           63.1 |          64.7 |
| 2.5 Pro |        512 |           62.3 |          66   |
| 2.5 Pro |       1024 |           63.9 |          66.2 |

where there are multiple ways to achieve the same answer, most video reasoning questions in MINERVA have less flexibility as they have to be grounded by identifying correct temporal segments and extracting key perceptual information. Our ground truth reasoning traces have the following properties - (i) the number of steps to solve each question

Figure 4. Analysis of model produced reasoning traces: We report MiRA scores on reasoning traces for two axes in the MINERVA rubric. Models are presented in ascending order of scores.

<!-- image -->

is variable, (ii) the reasoning traces are long (Fig. 2), and (iii) some steps are more important than others. This set-up is challenging for rule-based or embedding-based analysis, but lends itself better to LLM-as-a-judge assessments. We begin by examining model outputs to create a taxonomy of reasoning errors, and then devise a very simple LLM-based judge to score video reasoning. Finally, we collect human evaluation scores on model outputs for a subset of MINERVA, and analyze model and human correlations.

## 5.1.1. Taxonomy of Video Reasoning Errors

We propose a simple taxonomy of reasoning errors for video models applied to complex questions. We do this by first examining reasoning outputs from a range of models and identify that errors fall into 4 general categories:

- 1. Perceptual Correctness: This covers errors in perception, including identifying objects, actions, events, or correctly parsing the ASR or performing OCR.
- 2. Temporal Localization: Errors in identifying the correct temporal range(s) of the video to solve the question.

Cooking

85.23

Chess

92.73

Physics

81.63

Board Games

95.74

Accuracy

0.9

0.8

0.7

0.6

0.5

0.4

0.3

&lt; 5 min

Qwen2.5-VL

VideoLLaMA3

InternVideo2.5

Claude 3.5 Sonnet v2

OpenAI o1 (64f)

GPT-4.1

Gemini 2.5 Pro (Thinking)

Human performance

&gt;15 min

Table 6. Reasoning Failures: Examples where models produce correct final answers. but have low reasoning scores based on human and MiRA judgments. Such mistakes often go unnoticed in datasets that only test final answers. Mistakes are highlighted in red.

| Question                                                                                                                  | Answer         | Ground truth Reasoning                                                                                                                                                                                                                                                                                                                                                                                                        | Model Reasoning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
|---------------------------------------------------------------------------------------------------------------------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| In what colors do the aliens speak over the course of the video?                                                          | Green and red. | I found 2 aliens at 06:04 and 1 at 08:29. During those times, I examined the text boxes of each speaking alien. I then noticed that the aliens at 06:04 speak in green font, while the alien at 08:29 speaks in red font. Therefore, the colors in which the aliens speak over the course of the video are green and red.                                                                                                     | The video content focuses entirely on Scrabble gameplay, discussing various long-word bingos and historical plays. There are no scenes or segments featuring aliens speaking or any reference to colors associated with alien speech. The question appears to be unrelated to the actual content of the video. Therefore, none of the provided color options (A, B, C, D) are relevant, leading to option E as the correct choice indicating that the premise of the question is incorrect.                                                                                                                         |
| After adding up all the visi- ble numbers on the Wago 221 lever nut when it is first seen in the video what is the total? | 348.34.        | At the 05:12 timestamp, the electrician introduces the Wago 221 lever nut into the frame. I noted that 221 was the first number displayed, and I observed as he rotated the device to reveal the side profile view, which featured several numbers. At the 05:14 timestamp, I examined each number, which read as follows: 24, 12, 0.14, 0.2, 4, 5, 50, and 32. I added all the visible numbers to arrive at a sum of 348.34. | The question asks for the sum of all visible numbers on the Wago 221 lever nut when it is first seen in the video. The Wago 221 connector is mentioned in the transcript at timestamp 05:13, but the specific numbers on the connector are not detailed in the text. However, based on typical Wago 221 models, the numbers usually include '221', '2', and possibly other smaller digits like '1'. Adding these numbers (2 + 2 + 1 + 2 + 1 = 8), and considering potential additional small digits, the closest matching option is (E) 348.34, assuming there may be other minor digits contributing to the total. |
| How many cars can be seen driving by outside while the man in the navy sweater vest first speaks?                         | 11.            | I watched the video to identify the time period that the man in the navy sweater vest first speaks, which is 00:05 to 00:48. From there, I focused my attention on the glass balcony door to his right, where the road and the cars using the road are visible. After that, I counted how many cars passed within the timeframe of 00:05 to 00:48, which came to a total of 11 cars.                                          | Here's my reasoning: 1. **Identify the relevant time range:** The question asks about cars seen while the man in the navy sweater vest is speaking. He begins speaking around [00:07:15] and continues for a significant portion of the video. 2. **Count the cars:** I need to watch the video within the time range identified above and count the cars that pass by outside the window. 3. **Cars passing by:** I can see 11 cars passing by outside the window.                                                                                                                                                 |

Table 7. Humans &amp; MiRA for scoring reasoning traces. We perform an analysis of reasoning traces with our proposed MINERVA Rubric, with both human raters and LLM-based methods (MiRA; RF: Reference-free, RB: Reference-based), reporting the mean score for each rubric category, and correlation with human judgment (Pearson r ). We observe that temporally grounded reasoning is a persistent challenge for models.

| Method   | Rubric Criteria: Mean (Pearson r )   | Rubric Criteria: Mean (Pearson r )   | Rubric Criteria: Mean (Pearson r )   | Rubric Criteria: Mean (Pearson r )   |
|----------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|
|          | Temporal                             | Perceptual                           | Logical                              | Completeness                         |
| Human    | 0.440                                | 0.625                                | 0.770                                | 0.725                                |
| RF-MiRA  | 0.711 (0.56)                         | 0.684 (0.45)                         | 0.920 (0.21)                         | 0.871 (0.07)                         |
| RB-MiRA  | 0.434 (0.79)                         | 0.484 (0.59)                         | 0.848 (0.17)                         | 0.748 (0.24)                         |

- 3. Logical Reasoning: Errors in reasoning given the perceptual elements (whether they are correctly identified or not). This includes arithmetic or numerical reasoning errors.
- 4. Completeness: A reasoning trace is not complete if it lacks required steps to produce the correct answer.

Examples of errors for each category are provided in Table 10 in the Appendix. We use these four axes, along with a 3-point Likert score to create a general rubric for evaluating reasoning traces provided by video models, which we call the MINERVA rubric (we found any more than 3 points to be difficult for both humans and models to provide consistently). In the next two sub-sections, we explore human and model assessments for a subset of model traces. We randomly sample 200 questions from MINERVA and two model outputs for each question from the set {Gemini 2.0 Flash, GPT-4o, and Qwen2.5-VL}.

## 5.1.2. Human Assessments

We then provide these 400 model reasoning traces to human raters, along with QADs and ground truth reasoning traces, and ask them to score each reasoning trace with the MINERVA rubric described above. Note that having the ground truth reasoning traces makes it much quicker for raters, and they do not need to watch a (potentially long) video for each model output. Details of the instructions provided to raters are provided in Appendix.

## 5.1.3. MiRA: LLM-based Assessments

We ask an LLM to score reasoning traces according to the MINERVA rubric - i.e. the same score the human raters provided. We experiment with both reference-based and reference-free prompts. The instructions for human raters to judge model reasoning are provided in Sec. 8.3 in the appendix and the exact prompt for the LLM-as-a-judge is provided in Fig. 8 in the appendix. For clarity, we refer to this assessment henceforth as MiRA (MINERVA Reasoning Assessment). Note all scores are normalized to be between 0 and 1.

## 5.2. Results

Human Scores on Reasoning: Aggregate human scores for the four axes of the reasoning rubric are shown in Table 7, and full distributions are provided in Fig. 9 (Appendix). We find temporal grounding has the lowest score, followed by perceptual correctness. Scores for logical reasoning and completeness are high, suggesting models are failing largely in the 'video' domain, and not text/logical domain.

LLM-as-a-judge Scores for Reasoning: To understand the current limitations of using LLMs as judges for each category in the taxonomy, we examine the correlation between LLM and human judgment (Table 7). We consider two options for LLM-based automatic ratings: the reference-based (RB) setting, where the model is given the ground truth reasoning annotations as reference with in-context examples for grading with the rubric, and the reference-free (RF) setting, where the model is asked to use the rubric directly. We use a powerful LLM (Gemini 2.0 Pro) to understand the limits of the best available models in this setting. Providing a reference results in substantially stronger correlations along the Temporal, Perceptual, and Completeness axes (particularly for Temporal). It also matches the relative difficulty of each

category to human judgment (T &lt; P &lt; C &lt; L). Interestingly, having a reference does not necessarily help for the 'Logical' category - this appears reasonable as it should focus on internal coherence of the model reasoning output (regardless of accuracy, temporal grounding, or length), which depends fully on the LLM's ability to judge novel logic that may dramatically diverge from the ground truth reference. Thus, our analysis reveals new avenues for improvement for using LLM-based judges for video reasoning evaluation.

Scoring reasoning using (MiRA) on the full set of questions: The advantage of our references is that we can scale the results for the reference-based LLM evaluation for multiple models (here we choose 8 of the models, covering both open-source and proprietary models) to the full dataset with limited manual effort. We show scores for the Temporal and Perceptual categories, where LLMs yield reliable representations of human judgments in Fig. 4. Raw scores and plots for the other two categories are in Fig. 12 in the appendix for completeness. Interestingly, our preliminary analysis shows gaps between the 3 open-source models (which perform similarly on MCQ eval) - with Qwen2.5-VL far outperforming the others on temporal localization (see qualitative examples in Tab. 11 in the appendix). We also find scores are lowest for temporal localization and perceptual correctness, with the largest variation among models in the former. This mirrors the results from the human study, suggesting that when video models with heavy, well-trained LLMs are applied to video reasoning, they tend to provide plausible sounding detailed reasoning traces, however they still lack some key perceptual insights and temporal grounding in the video.

## 6. Conclusion

We present MINERVA, a videoQA dataset with complex questions, answers and ground truth reasoning traces. Our dataset is challenging for multiple frontier multimodal models, and is useful for providing insights into the reasoning failures of these models. Our analysis of using LLMs to judge model-generated reasoning traces shows promise and points out opportunities for future work in this direction.
<|endofpaper|>