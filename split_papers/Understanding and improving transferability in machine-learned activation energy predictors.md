<|startofpaper|>
## Understanding and improving transferability in machine-learned activation energy predictors

Joe Gilkes, † ‡ , Mark Storr, ¶ Reinhard J. Maurer, ∗ † § , , and Scott Habershon ∗ † ,

† Department of Chemistry, University of Warwick, Gibbet Hill Road, CV4 7AL Coventry,

UK

‡ EPSRC HetSys Centre for Doctoral Training, University of Warwick, Gibbet Hill Rd, CV4 7AL, Coventry, UK

¶ AWE Plc, Aldermaston, UK

§ Department of Physics, University of Warwick, Gibbet Hill Road, CV4 7AL Coventry, UK

E-mail: r.maurer@warwick.ac.uk; s.habershon@warwick.ac.uk

UK Ministry of Defence © Crown Owned Copyright 2025/AWE

## Abstract

The calculation of reactive properties is a challenging task in chemical reaction discovery. Machine learning (ML) methods play an important role in accelerating electronic structure predictions of activation energies and reaction enthalpies, and are a crucial ingredient to enable large-scale automated reaction network discovery with &gt; 10 3 reactions. Unfortunately, the predictive accuracy of existing ML models does not yet reach the required accuracy across the space of possible chemical reactions to enable subsequent kinetic simulations that even qualitatively agree with experimental kinetics. Here, we comprehensively assess the underlying reasons for prediction failures within a selection of machine-learned models of reactivity. Models based on difference fingerprints between reactant and product structures lack transferability despite providing good in-distribution predictions. This results in a significant loss of information about the context and mechanism of chemical reactions. We propose a convolutional ML model that uses atom-centered quantum-chemical descriptors and approximate transition state information. Inclusion of the latter improves transferability for out-ofdistribution benchmark reactions, making greater use of the limited chemical reaction space spanned by the training data. The model further delivers atom-level contributions to activation energies and reaction enthalpies that provide a useful interpretational tool for rationalizing reactivity.

## 1 Introduction

Complex chemical reaction networks (CRNs) arise across a wide variety of fields, from catalysis and combustion modelling to atmospheric chemistry and biological synthesis. 1-7 A wide variety of automated reaction discovery (ARD) schemes are available, including contactmap-based sampling strategies, 8-12 heuristic rule-driven methods, 13-16 and strategies that manipulate the potential energy surface (PES) to drive the exploration of chemical reactionspace. 17-20 Such ARD methods have been reviewed elsewhere, but a common thread is that -inevitably - making predictions about the experimentally-observable properties of CRNs demands evaluation of the properties of individual elementary reactions, primarily reaction enthalpy ∆ H r and activation energy E a . 21,22

Given the scale of possible ARD-generated CRNs - with some recent examples comprising 10 3 -10 4 possible elementary reactions - direct ab initio evaluation of reaction properties is time-consuming and resource intensive. As such, a variety of different machine learning (ML) strategies have been employed to predict activation energies and/or reaction enthalpies after training on suitable datasets, typically calculated at the level of density functional theory (DFT). Recent examples include the Chemprop neural network (NN) model, 23,24 as well as the KPM model. 25 These models typically encode reaction information using 'difference' vectors - in other words, subtracting a reactant descriptor from a product descriptor, resulting in a descriptor representing the important functional group changes during the chemical reaction that may subsequently be used for ML. In applications to a dataset of &gt; 10 4 organic chemical reactions reported by Grambow et al. , 26 such models provide predictive accuracy in activation energies to within 5 kcal/mol, which represents the current state-of-the-art.

In recent work, we combined our KPM activation-energy predictor with a streamlined ARD strategy that iteratively constructs a CRN using kinetic simulations with ML-based rate constants, derived from predicted activation energies, to guide the exploration of relevant regions of chemical reaction space. 12 Our results highlighted the challenge of accurately

modelling complex CRNs with ML-based rate predictions, with some final product concentrations disagreeing significantly with previous shock-tube experiments. While KPM shows sufficiently low test set errors within training, deeper analysis performed here reveals a severe lack of transferability and generalisability to reactions outside the distribution of training reactions. This is echoed in similar contemporary ML strategies.

The goal of this article is to explore these failures in more depth to pinpoint key transferability problems with current structure-based ML strategies like KPM and Chemprop and to identify future directions; as we show below, analysis of the descriptors used in these models (and their resulting view of chemical reaction space) explain their lack of transferability to seemingly similar reactions. Using these new insights, we propose a new NN architecture using atom-centered descriptors obtained from quantum-chemical calculations in combination with approximate transition state geometries to improve transferability; the results of this strategy are encouraging, albeit with further room for improvement.

The remainder of this article is organized as follows. First, in Section 2, we perform a thorough benchmarking of contemporary ML-based strategies for reaction-property prediction (KPM, Chemprop and NeuralNEB), highlighting their disparate views of chemical reaction space. In Section 3, we introduce a novel NN architecture that offers a much better route to reaction-property prediction with improved transferability. Finally, we show how such atom-based ML models offer new chemical insights, providing an easily explainable view of reactivity for organic molecules.

## 2 Benchmarking Contemporary Models

Before exploring new ML models with improved transferability to previously-unseen reactions, it is vital to understand both how and why KPM and similar ML models fail at the task of predicting reactive quantities in some areas of chemical reaction space. It is also important to quantify how different these regions must be from the reactions of the training

set of a given model in order to yield poor predictions.

For this reason, an additional validation dataset of reactions were first generated from the CRN produced at the end of our previous work. 12 This CRN, generated with the Kinetica.jl package and labelled C I, 0 01 . 4 , explores the chemical reactions that may occur during ethane pyrolysis at 1000 K. It therefore contains over 8,000 reactions of pure hydrocarbons with no heteroatoms, many of which feature open-shell free radical species that can be formed at such high temperatures. Despite the apparent simplicity of such reactions, rate constants based on KPM's E a predictions were not accurate enough to obtain even qualitatively correct CRN kinetics when compared to experimental data. While we postulated that this was likely due to a missing entropic contribution to the rate constants used for guided exploration of chemical reaction space, the accuracy of KPM's E a predictions on these ARD-generated reactions has not yet been systematically tested.

To examine this potential source of error, minimum energy paths (MEPs) and transition state (TS) geometries of these generated reactions were isolated using the climbing image nudged elastic band (CI-NEB) method, as detailed in Fig. S1 of the Supporting Information . Calculations were performed in the NWChem electronic structure code at the same level of theory as the original datasets of Grambow et al. on which KPM was trained (namely DFT with ω B97X-D3 hybrid functional and def2-TZVP basis set). 26-28 This ensures that the resulting DFT-calculated activation energies are as comparable to the activation energy predictions of KPM as possible. TSs were confirmed through vibrational analysis, by checking that each had only one imaginary mode present.

Of the 637 total reactions from previously-generated CRN C I, 0 01 . 4 that were put through this workflow, 381 produced converged TS geometries that could be used to calculate accurate activation energies. A selection of the converged reactions are presented in Fig. S2. While some of these reactions were barrierless or comprised a single energetic barrier, others consisted of multiple energy barriers. The multi-step reactions were a result of the connectivity-based reaction exploration algorithm used to generate the initial CRN, which

is less restrictive to the types of reactions explored than algorithms based on, for example, bond-order matrices. While these multi-step reactions can in principle be further reduced to multiple single-step reactions by iteratively performing further NEB calculations, we avoid this complication here by simply removing these reaction types. Ultimately, this leaves a validation set of 147 converged, single-step reactions from network C I, 0 01 . 4 (extended to 294 reactions by also extracting the reverse of each calculated 'forward' reaction) with which ML models could be benchmarked for potential out-of-distribution accuracy.

As shown below, this allows quantification of the error incurred by generalising ML E a predictions to unknown regions of chemical reaction space, while simultaneously enabling visualization of the space occupied by both the original training set and this new validation set.

## 2.1 KPM

To test transferability, a KPM model was used to predict the activation energies of the new validation set of chemical reactions. 25 This model was the same as was used in our previous work, and had been trained on a combination of two datasets by Grambow et al.: a general purpose dataset of organic reactions comprised of C, H, O and N atoms in charge-neutral and ionic reactions, and a secondary dataset of radical reactions of the same elements which were excluded from the primary dataset. 26,28 Both datasets contained zeropoint energy-corrected activation energies and reaction enthalpies for all reactions within. It was expected that inclusion of the secondary dataset within the training data would extend the model's understanding of chemical reaction space, allowing the validation set reactions to be predicted from within the training distribution. Training on this combined dataset did introduce some additional error into the test set predictions compared to the original KPM model of Ismail et al. (MAE = 1.98 kcal/mol, RMSE = 5.17 kcal/mol), although not to an extent that the authors' original target of 2-6 kcal/mol was exceeded. 25

This model was applied to the reactions of the validation set and their predicted activation

energies were compared against their corresponding DFT results. All DFT calculations were corrected using the zero-point energies of their respective geometries, to match the energies provided within KPM's training data. The correlation plot for these results is shown in Fig. 1.

Figure 1: Correlation plot for KPM E a predictions of C I, 0 01 . 4 validation set.

<!-- image -->

The poor agreement between the DFT and KPM-predicted activation energies in Fig. 1 helps to explain why our previous kinetic simulations struggled to qualitatively reproduce the experimental reference data. While there is a general correlation between the two sets of activation energies, the errors across the board are simply too large to yield accurate kinetics results - especially when one considers the exponential relationship between E a and reaction rate constant in the Arrhenius equation, which causes these large prediction errors to be exacerbated.

We initially postulate that the large errors in the validation dataset arise due to the lack of representation of the validation set reactions in the KPM training set (noting that the secondary dataset of reactions of free radical species is one tenth of the size of the primary dataset of ionic and charge-neutral reactions). Without enough similar reactions

to learn from, it is sensible to assume that KPM struggles to generalise to new areas of chemical reaction space. To test this hypothesis, we added half of the reactions in the C I, 0 01 . 4 validation set to the training dataset and trained a new KPM model. The held back remainder of the validation set was applied to this model, as we hoped to see some improvement in their prediction accuracy when KPM had further prior knowledge of the relevant chemical reaction space. This resulted in marginally improved validation set errors (MAE = 31.07 kcal/mol, RMSE = 40.13 kcal/mol; correlation plots are shown in Fig. S3), but this could easily be the result of averaging over a smaller validation set.

It therefore appears that KPM fails to generalise to the reactions in network C I, 0 01 . 4 , even when given similar reactions to learn from. However, this conclusion was complicated by the ensemble uncertainties across the predictions of this retrained model - the uncertanities for the out-of-distribution validation set reactions were just as small as for the reactions in the new training set. If the reactions of the validation set were truly out-of-distribution and KPM was failing to generalise to the areas of chemical reaction space they represent, it would be expected that the individual NN ensemble members would disagree over their predictions, yielding a higher predictive uncertainty. The fact that the opposite trend is observed suggests that the large prediction errors in the validation set cannot be explained through poor training alone: a deeper level of analysis is required.

## 2.1.1 Chemical Reaction Space Analysis

To begin this further analysis, we first sought to understand whether the reactions of the validation set truly laid outside of the distribution of reactions in the KPM training set. To assess this, the reaction descriptors used in KPM were recalculated for both the training set and for the C I, 0 01 . 4 validation set. These descriptors are formed from the element-wise difference in 1024-bit Morgan fingerprints 29,30 between reactants and products for each reaction, concatenated with the zero point energy-corrected enthalpy change of that reaction to create a 1025-length descriptor vector.

Initially, we calculated Tanimoto coefficients for every pair of reaction descriptors across both training and validation datasets to measure their similarity. 31,32 We compare the similarities between the reactions of the training set and the reactions of the validation set to the self-similarity between all pairs of reactions in the training set in Fig. S4. While there are some differences, this form of analysis only reveals that the validation set is as similar to the training set as the training set is to itself, suggesting that the validation set should lie within the training distribution. However, Tanimoto similarity is limited to directly comparing features across two descriptors without accounting for the correlation between features, so this may not reveal the true differences between the reaction sets.

Accounting for correlations between features better represents how the NNs within KPM see the reaction descriptors during training. We therefore use t-distributed stochastic neighbor embedding (t-SNE) to map the high-dimensional KPM descriptors into a two-dimensional space. 33 This allows for visualisation of the representative chemical reaction space spanned by the training set, as well as enabling the space spanned by the validation set to be overlaid in order to evaluate the extent of overlap of these distributions. The descriptors for both training and validation sets were concatenated into a single dataset (because t-SNE learns a 'single-shot' non-parametric mapping that cannot be reused for the validation set) and principal component analysis (PCA) 34 was used to reduce the dimensionality of this dataset to 100 (a noise reduction technique used in the original formulation of t-SNE). 33 The t-SNE mapping employed a perplexity parameter of 50, and the two component datasets were split after being dimensionally reduced in order to produce the 'map' of chemical space shown in Fig. 2a.

This mapping shows the reactions from the validation set entirely contained within the space encompassed by the training set; as such, the validation set reactions should be entirely in-distribution and their E a predictions should be accurate which, again, is not observed. To check that t-SNE was learning a valid low-dimensional representation of the chemical reaction space spanned by the datasets, the nearest-neighbours (in 2D t-SNE space) of each

Figure 2: Low-dimensional representations of the chemical reaction space spanned by the radical-extended training set and the C I, 0 01 . 4 validation set, as seen by the KPM reaction difference descriptor. a) t-SNE representation. b) PCA representation, using the two highestvariance PCs.

<!-- image -->

of the validation set reactions were taken from the training set reactions. These were used to establish whether similar reactions were being correctly grouped in t-SNE feature space. A selection of these nearest-neighbour reactions from the high-density central region of Fig. 2a are shown in Table 1.

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

<!-- image -->

Table 1: Selected reactions from the C I, 0 01 . 4 validation set and their nearest neighbours in 2D t-SNE space from the radical extended training set. Atoms directly involved in each reaction are highlighted in red. All energy values are given in units of kcal/mol.

| Selected Reaction   | Selected Reaction                          | Nearest Training Set Reaction   | Nearest Training Set Reaction   |
|---------------------|--------------------------------------------|---------------------------------|---------------------------------|
| A                   | H 3 C H 2 C CH 3 ∆H r = -108.50 E a = 0.00 | CH CH 2 O H 2 C CH 3 C + H C O  | ∆H r = -105.24 E a = 28.70      |
| B                   | C C CH 3 H + ∆H r = 33.95 E a = 36.97      | C H C O H 2 C C CH C H          | ∆H r = 30.99 E a = 106.05       |
| C                   | H C CH 2 + ∆H r = 30.84 E a = 38.12        | C H 2 C                         | ∆H r = 29.38 E a = 114.05       |
| D                   | HC C CH 3 CH 2 + ∆H r = 26.14 E a = 34.36  | HN HN C CH H                    | ∆H r = 29.86 E a = 65.37        |

<!-- image -->

These nearest-neighbours initially appear sensible; in all four cases there are clear similarities between each selected reaction and its nearest neighbour. Each reaction shares a similar value of ∆ H r as its nearest neighbour from the training set, with the maximum difference being around 3 kcal/mol. However, the target activation energies for these reactions can differ greatly, by up to 70 kcal/mol. Closer inspection reveals that, while the reactant/product structures for selected reactions and their nearest-neighbours are typically similar, the underlying details of the reaction can be very different. For example, for reaction A, the selected reaction involves barrierless insertion of a carbene into a C -H bond, while its nearest neighbour reaction inserts a carbene into a C -O bond in a heterocycle. Similarly, for reaction C, the selected reaction involves dissociation of a hydrogen radical while its nearest-neighbour requires dissociation of molecular hydrogen from a stable hydrocarbon.

This nearest-neighbour analysis shows that, while t-SNE with the KPM descriptor cap-

tures structural and enthalpic similarity between reactions, it fails to account for the nature of the reaction. This either indicates that t-SNE is over-emphasizing particular descriptor features when constructing the 2-D representation of chemical reaction space, or that sufficient information is not present in the descriptors to enable adequate discrimination between reactions by t-SNE.

To better understand the mapping generated by t-SNE, we turned to an alternative dimensionality-reduction approach, namely PCA. We note that t-SNE focuses on preserving the structure of local clusters at the expense of preserving larger distances in the underlying high-dimensional space; by contrast, PCA is a transformation approach that better preserves distances between data points at both large and short distances (strictly, when data is transformed using the same number of principal components as input dimensions). 34 PCA can be used as a dimensionality reduction technique by selectively ignoring the principal components (PCs) that explain the least variance within the data. Here, a PCA model was fit using the 1025-dimensional KPM descriptors of the training set reactions, and the reactions of both training set and validation set were subsequently projected into the space spanned by the two PCs that captured the greatest variance; the results are shown in Fig. 2b.

This initially appears to confirm the conclusion from the t-SNE analysis - that the validation set reactions lie within the distribution spanned by the training set. Specifically, we find the large majority of validation set reactions to lie near the center of the chemical reaction-space spanned by the training set; a line of symmetry is also present, which is expected due to the inversion of the forward reactions to create the respective backward reactions. However, the PCA analysis does reveal a stark failure of KPM's reactant-product difference descriptors - the variances within the high-dimensional dataset that are captured by each PC (as shown in the axis labels of Fig. 2b) are extremely low, indicating that there is essentially no correlation between the individual features of the KPM descriptor. For completeness, this analysis was extended to the first ten PCs, which revealed that all

together, these PCs only account for 4.81% of the overall variance of the descriptor.

This is a crucial finding - the inability of PCA to identify correlation between features of KPM's reactant-product difference descriptor explains much of the trained KPM models' inability to generalise the predictive accuracy of its test set results to the C I, 0 01 . 4 validation set. The lack of correlation means that the NNs that make up KPM's predictive ensemble learn very little from the KPM descriptor itself, instead primarily relying on ∆ H r to drive their predictions. This reliance on ∆ H r is a major contributing factor to the high errors seen previously in Fig. 1; learning directly from ∆ H r with only minor corrections contributed by the Morgan difference fingerprint means that reactions with similar values of ∆ H r appear similar to the model, as evidenced by the results in Table 1. Including new radical reactions with similar values of ∆ H r to previously-seen reactions, but which often proceed through very different mechanisms and TSs, inevitably leads to inaccuracies in predicting activation energies which are intrinsically dependent on the structure of the TS. In other words, the descriptors used in KPM do not accurately represent the path a reaction takes.

KPM includes ∆ H r as part of its descriptor in accordance with prior literature, and this approach has been echoed in further contemporary work since. 25,35-38 It is a sensible choice, stemming from the Brønsted-Evans-Polanyi relation that correlates ∆ H r with E a in a variety of applications. 39,40 The approach taken in KPM therefore mimics a ∆-ML model, where ∆ H r is modified to approximate E a by means of learning from the remaining input features. Care must be taken though to ensure that these features are useful for learning; following on from our PCA analysis, we note that recent work has used SHapley Additive exPlanations (SHAP) 41 to examine the relative importance of each feature on resulting predictions, finding that ∆ H r was considerably more useful than any of the features from a Morgan fingerprintderived reaction descriptor. 42 While the authors did not attempt to use their model on an out-of-distribution dataset such as the C I, 0 01 . 4 validation set used here, we anticipate that they would run into the same roadblock of generalisability.

## 2.2 Chemprop

With better insight into the KPM model's lack of generalisability, we subsequently sought to benchmark alternative ML models in order to determine whether descriptors created by the difference between reactants and products - but based on information other than Morgan fingerprints - discard too much information to be useful in generalization. Chemprop is a D-MPNN originally formulated for molecular property prediction. 23 It constructs one-hot encoded feature vectors for atoms and bonds and passes this information along molecular graphs to create a representation with an understanding of the chemical composition of a given chemical species. By adding so-called RDKit fingerprints - a descriptor similar to Morgan fingerprints with a focus on identifying variable-size subgraphs rather than bonding patterns within a fixed radius 43 - Chemprop has been shown to perform well at a wide range of regression and classification tasks.

Chemprop was extended to describe reactions for the purpose of E a prediction by Grambow et al., by means of an atom-mapped difference of the learned representation of a reaction's reactants and products. 24 In their work, this model was trained on the same dataset as the original formulation of KPM, resulting in a test set RMSE of 3 4 . ± 0 3 kcal/mol -. very similar to the RMSE that the authors of KPM originally achieved. 25 While Grambow et al. did not include ∆ H r as a feature explicitly, it was added as a secondary prediction target, allowing for implicitly learning its relationship with E a .

To enable direct comparison between Chemprop and KPM in predicting activation energies of the C I, 0 01 . 4 validation set, we retrained Chemprop from scratch using the same radical-extended training dataset as KPM. In accordance with the original training protocol for reaction-based Chemprop, this involved a two-stage process wherein the model was initially trained on the more abundant, lower accuracy version of this dataset comprising 36,778 reactions from DFT with the B97-D3 exchange-correlation functional, followed by training on the smaller but higher accuracy version of the dataset with 26,656 reactions at the ω B9X-D3 level (the latter of which was used to train KPM). This has the effect of

giving the model a greater appreciation of chemical reaction space from the first dataset, while elevating its predictions to the level of the second. Training was performed using the optimal hyperparameters from ref. 24.

This resulted in an ensemble of ten unique committee members, all trained on different, although overlapping, areas of chemical reaction space defined by performing 10-fold cross validation on the full training dataset. The RMSEs of each of the ensemble members' predictions on their own test sets were averaged to give a final ensemble RMSE of 5 65 . ± 0 51 . kcal/mol. While this is greater than the RMSE of the original ensemble of Grambow et al., 24 this increase in error mirrors the increase seen when retraining KPM, where introduction of the additional dataset of radical reactions caused overall prediction quality to worsen. The new Chemprop ensemble was then used to predict activation energies and reaction enthalpies of the C I, 0 01 . 4 validation set. As in the KPM ensembles employed previously, these properties were predicted for each reaction by every ensemble member and averaged to obtain the final prediction, with predictive uncertainties represented by the standard deviation between the predictions of the members.

Correlation plots for these predictions are shown in Fig. 3. We found the performance of this radical reaction-aware Chemprop ensemble to be broadly comparable to the results seen with KPM, as confirmed in the given error metrics. For KPM (Fig. 1) there was little bias towards over- or under-estimation of activation energies, whereas Chemprop broadly underestimates E a . This bias is not mirrored in Chemprop's ∆ H r predictions however; the reactions lying far from the identity line are equally distributed on either side (and in fact possess a rough line of symmetry that reflects the forward/backward reaction pairs). This is promising, suggesting that Chemprop is capable of transferring some of its predictive accuracy from training to the out-of-distribution validation reactions.

To develop better insight into Chemprop's approach, we extracted the final learned representation of its fingerprints from the penultimate NN layer and subsequently used this 1800-dimensional fingerprint in PCA analysis, following the same procedure as outlined

Figure 3: Correlation plots for a) E a and b) ∆ H r predictions by Chemprop for the reactions of the C I, 0 01 . 4 validation dataset, when trained from scratch on the radical-extended dataset.

<!-- image -->

above (because each NN ensemble member learns its own representation of a reaction, only the NN with the lowest individual RMSE was used in this test). The resulting 2D representation of chemical reaction-space - projected onto the PCs encapsulating the largest variance - is shown in Fig. 4. These results confirm that Chemprop is much more effective at encoding learnable information than KPM, with over half of the total variance in the learned

descriptor being captured by the first PC. The relationship between the spaces spanned by the reactions of the validation set and the training set is correspondingly much clearer than in Fig. 2b, with Chemprop learning a representation that is able to generalise to some of the validation set. However, many of the validation set reactions lie on the fringes of the learned chemical reaction space, suggesting poorer predictive accuracy in these areas.

Figure 4: Low-dimensional PCA representation of the chemical reaction space spanned by the radical-extended training set and the C I, 0 01 . 4 validation set, as seen by the reaction difference descriptor of the best performing model in the Chemprop ensemble.

<!-- image -->

Despite the improvement in useful, learnable information in Chemprop compared to KPM, Chemprop is still unable to yield accurate predictions of the activation energies of the validation set. This may be due to how both Chemprop and KPM describe reactions; they use a difference fingerprint between reactants and products. This approach emphasizes differences between the two end-points, singling out the atoms and bonds that are modified over the course of a reaction. However, this also annihilates much of the information which could provide context to discriminate between otherwise similar reactions, as properties such as the structure of both the reactants and the products, as well as the charges and radical electron counts of their atoms, are discarded.

## 2.3 NeuralNEB

So far, we have shown that two reactant-product-difference NN architectures - KPM and Chemprop - are unable to generalise to a challenging validation set of chemical reactions; as such, it is possible that such approaches simply lack the necessary information about the TS of each reaction. However, a machine-learned interatomic potential (MLIP) trained on sufficient examples of reactive intermediates and TSs may be able to account for this gap in knowledge. In this case, methods such as CI-NEB must be used to approximate the TS structure and activation energy.

Wetherefore consider NeuralNEB, a pre-trained version of the PaiNN equivariant message passing graph neural network. 44,45 The authors of NeuralNEB found that PaiNN performed best in reproducing the activation barriers of a set of test chemical reactions when trained on the Transition1x dataset. 46 Transition1x takes the reaction dataset by Grambow et al., 26 originally used for training KPM, and extends it for use with MLIPs. The authors regenerated the MEPs of each reaction from the Grambow dataset with CI-NEB and sampled molecular geometries from across the available convergence space of each MEP. They obtained a dataset of 9.6 million molecular configurations, with energies and forces calculated at the same level of theory as used in our C I, 0 01 . 4 validation set. NeuralNEB is therefore ideal for determining if MLIPs can perform better at E a prediction than reaction difference descriptor NNs when trained on similar data.

Since NEB calculations reveal entire reaction paths, not only a single activation energy, the multi-step reactions which were previously discarded from the validation set were reintroduced; even if it could only qualitatively reproduce these reaction paths, NeuralNEB could be used as part of a low-cost method for automatically separating these multi-step reactions into their constituent single-step reactions. To assess the performance of NeuralNEB, we therefore took the 381 reactions from network C I, 0 01 . 4 that originally converged to a TS under DFT and subsequently subjected these to re-optimisation of reactant/product structures, followed by interpolation with IDPP and path-optimization through CI-NEB,

using the Atomic Simulation Environment (ASE) software package as a driver for the calculations. 47,48 MEPs were generated with the FIRE optimiser by running NEB until the maximum force experienced by any atom reached 0 1 eV/ ˚ , then enabling the CI forces . A and continuing until this maximum force reached 0 04 eV/ ˚ . . A 49 147 these reactions converged to a TS under NeuralNEB; their calculated activation energies and reaction enthalpies are shown in Fig. 5. Selected NeuralNEB reaction MEPs are additionally compared against their DFT-level counterparts in Fig. S5.

We find that while NeuralNEB is capable of accurately predicting activation energies (and reaction enthalpies) for around half of these converged reactions, the remaining half are characterised poorly; the calculated MAE and RMSE are comparable to the errors predicted by Chemprop. However, it should be noted that the prediction task that MLIPs must undertake to predict MEPs and activation energies is more challenging than predicting a single value (as in KPM and Chemprop), instead demanding accurate prediction of large areas of a high-dimensional PES. This is emphasised by the number of reactions that failed to converge under CI-NEB, which failed due to numerical instability caused by large atomic forces, likely in areas of chemical space outside of NeuralNEB's training distribution. While the inclusion of the reactions of the secondary radical dataset of Grambow et al. 28 within the Transition1x dataset might alleviate these blindspots, as with the previously investigated models, NeuralNEB is unable to use information on radical electron counts and charges within its description of the PES. As such, its predictive accuracy for radical reactions is still fundamentally limited, making it unsuitable for use within kinetic simulations involving radical species.

## 3 Improving transferability

The contemporary ML models tested above all failed to reliably predict the activation energies of the chemical reactions within the validation set of reactions from network C I, 0 01 . 4 ,

Figure 5: Correlation plots for a) E a and b) ∆ H r predictions by NeuralNEB for the reactions of the C I, 0 01 . 4 validation set.

<!-- image -->

at least to an accuracy that would be usable within chemical kinetics simulations. A large part of this likely stems from these reactions containing many open-shell species. While Chemprop does encode information about atomic spin multiplicity in the feature vector that is passed between atoms in a molecule, and the resulting learned reaction descriptors are correlated and usable for the task at hand, it is likely that the subtractive approach used in

creating these descriptors annihilates much of the useful contextual information that would enable a better understanding of radical species and reactions. Meanwhile, MLIPs like NeuralNEB rely on input geometries of species to evaluate the PES; having no information on spin or electron counts makes it unlikely that these models could learn how to properly characterise reactions involving radical species. Some newer models are beginning to emerge that may eliminate this deficiency, but none have been applied to the prediction of reaction energetics at this moment. 50,51

Based on the studies above, there are therefore three key areas of improvement for new ML models for E a prediction:

- · Maximizing information exploitation: Models that rely on evaluating differences between products and reactants to obtain descriptors that solely represent bond changes may yield accurate predictions within a narrow slice of chemical reaction space, but sacrifice vital contextual information which could enable greater transferability. Models that predict reactive properties directly from reactants and products should therefore avoid featurising reactions using differences if transferability or generalisability is important.
- · Accounting for spin: While many models can accurately predict properties of reactions between closed-shell species, neglecting spin multiplicity can lead to erroneous predictions for reactions of open-shell species. Any model hoping to correctly predict the properties of such reactions should be 'spin-aware', preferably building spin into the reaction descriptors.
- · Including TS Information: While information about the true TS of a reaction is typically unknown to ML models based on reactant-product difference descriptors alone, providing a model with a TS approximation could potentially enhance predictive accuracy. 37 Interpolation methods such as IDPP provide one way of obtaining approximate TS information, 47 which could subsequently be used to provide a better

description of the path taken through chemical reaction-space.

Recently, a new representation of local atomic environments was developed which can better satisfy these criteria. This representation - the global spectrum of approximated Hamiltonian matrices (SPA H M) - computes the occupied-orbital eigenvalues of an approximate 'guess' Hamiltonian matrix for a molecular system. 52 These guess Hamiltonians are typically used as the starting point for self-consistent field iterations in DFT calculations; they are cheap to compute, but encode information about the system charge and spin, alongside a translationally- and rotationally-invariant geometry description. SPA M was shown H to enable learning of molecular properties across a range of charge and spin states with accuracy greater than other contemporary global descriptors, such as SLATM (spectrum of London and Axilrod-Teller-Muto). 53

From SPA H M, two further local representations have been developed - SPA H M(a) for encoding local atomic electron density, and SPA H M(b) for encoding electron density of bonds surrounding each atom. 54 These descriptors use the electron density calculated from guess Hamiltonians, decomposing it into atom-centered or bond-centered contributions. Both representations were shown to further improve predictive accuracy in ML models for problems related to spin and charge. SPA M(b) therefore stands to be an excellent choice of descripH tor for predicting reactive properties; it wraps knowledge about spin and charge into its representation, while locally encoding electron density of chemical bonds.

Here, in seeking to enhance accuracy and transferability of activation-energy prediction, we developed a new CNN architecture using SPA H M(b) descriptors alongside approximate TS data obtained from geometry interpolation. As we show below, this approach reduces errors for the C I, 0 01 . 4 validation set over all the methods tested above, albeit with some caveats discussed below.

## 3.1 Reaction Interpolation

We hypothesize that incorporating approximate TS geometries into a ML model could improve the accuracy and transferability of predictions of reactive properties; as such, we required a method for generating approximate TS geometries. Here, we sought to evaluate the performance of geometric interpolation schemes for TS approximation. However, we note our emphasis on using these interpolation strategies as part of an accelerated ML workflow, suggesting that any selected method for TS approximation must be computationally-efficient, broadly applicable, and employ purely geometric information.

With these criteria, we sought to compare the performance of (i) linear Cartesian interpolation and (ii) IDPP, 47 as implemented in ASE, and (iii) geodesic interpolation 55 in approximating TS geometries. To evaluate performance, each of these three interpolation schemes was used to generate an approximate TS for each reaction in the radical-extended training dataset constructed previously. To determine the approximate TS geometry from each interpolation - while also maintaining the low computational cost of these interpolation methods - the highest energy geometry along the interpolated MEP (as characterised by GFN2-xTB calculations) was taken to be the approximate TS for the given reaction and interpolation scheme. 56

The approximate TS geometries for each reaction, generated by each interpolation method, were then compared against their respective TS geometries in the DFT dataset. For this comparison, we chose to use the smooth overlap of atomic positions (SOAP) representation to construct local descriptions of the environments of each atom in both the DFT TS and the interpolated TS. 57,58 SOAPs are commonly used in ML models describing electronic structure, due to their ability to accurately encode regions of local atomic geometry into readily learnable vectors. 59 In this case though, each atom's local SOAP representation in the interpolated TS was compared against its counterpart in the DFT TS to calculate a distance metric in SOAP descriptor space, as follows:

Figure 6: Distributions of r SOAP between exact TSs of the reactions in the radical-extended training set and their interpolated approximations. Included for the IDPP and geodesic interpolation methods are examples of approximate TS geometries at average and maximum SOAP distance, with their corresponding exact DFT TSs overlaid in blue. The statistical averages of the distributions for these methods are shown as r IDPP SOAP and r Geo SOAP respectively.

<!-- image -->

$$r _ { \text{SOAP} } = \frac { 1 } { n _ { a } } \sum _ { i = 1 } ^ { n _ { a } } \left \| \text{S} _ { i } ^ { \text{TS} } - \text{S} _ { i } ^ { \text{interp} } \right \| ^ { 2 }.$$

Here, n a is the number of atoms in the TSs, but S TS i and S interp i are each SOAP vectors of length n s from the n a × n s matrices S TS and S interp . While this geometric similarity metric lacks the interpretability of metrics such as positional root mean squared deviation (RMSD), which operates in standard units of distance, it is translationally and rotationally invariant, allowing it to be much more robust to movements of groups of atoms. The distributions generated by this metric are shown in Fig. 6.

This comparison of DFT TS structures and interpolated TS structures in the SOAP descriptor space reveals that geodesic interpolation significantly outperforms the other interpolation schemes, with the distribution of r SOAP strongly skewed towards lower values when compared to either linear or IDPP interpolation. Linear interpolation performs the worst overall, with comparatively many approximate TSs at high values of r SOAP due to its ten-

dency to let atoms intersect during interpolation. IDPP creates less of these high-distance TSs, but those it does create tend to look like structure 3 in Fig. 6. This explosion of atoms is typical of IDPP edge cases where geometry optimisation under the IDPP potential fails numerically. By contrast, even the worst-performing geodesic TS maintains a sensible structure that, while different from its DFT counterpart, still broadly matches the connectivity and placement of its atoms. Tabulated results and further examples of low-, averageand highr SOAP geometry differences are given in Supporting Information Tables S1 and S2, respectively.

Overall, our results show that geodesic interpolation allows for the generation of sufficiently accurate MEPs that, when combined with energy evaluation at the GFN2-xTB level, yield approximate TS geometries with only minor conformational differences compared to their respective DFT-level geometries across the vast majority of reactions studied. It therefore represents an excellent choice of method for generating approximate TS geometries for ML models that predict reactive properties, as we now discuss.

## 3.2 Convolutional Neural Network Architecture

Armed with geometries for the reactants, approximate TS, and products of reactions, and with the SPA H M(b) descriptor of chemical bonds around local atomic environments, we can begin to learn reactive properties with a suitable ML model. Rather than subtracting these descriptors for a given reaction in the style of KPM and Chemprop, here we choose to learn the optimal combination of each feature across the three geometries with a convolutional neural network (CNN). CNNs are common in many modern image recognition techniques, where large 2D pixel grids are distilled down to smaller representations which maximize the effect of important parts of the image using convolution filters (sometimes called convolution kernels). 60,61 In a CNN, this convolution filter is a matrix of learnable weights, enabling models to learn the best way of performing this size reduction. This is typically followed by a pass through a nonlinear activation function in the same way that a hidden layer in a

<!-- image -->

B

Figure 7: Representation of the architecture of the DCINet CNN. Vectors of f features representing atoms in the reactants, TS and products (denoted here as red, green and blue channels c ) undergo a depthwise convolution to obtain a single reaction descriptor of f features. This is passed through a series of feed-forward layers to output atomic contributions to E a and ∆ H r . The number of atoms in each reaction n i a in a mini-batch of size B is also provided as an input, allowing atomic contributions to target properties to be correctly split and zero-padded for summation.

MLP might be, forming a single convolutional layer.

To apply these techniques to SPA H M(b) descriptors, the local representations of each atom in the reactants, TS and products were fed through such a convolutional layer, enabling prediction of per-atom properties. However, such properties are not available for training; E a and ∆ H r are intrinsically properties of a reaction as a whole. Instead, the CNN must learn its own local contributions to these target properties, so that they can be summed to create the global properties of interest.

The resulting CNN architecture, referred to henceforth as DCINet (depthwise convolutional individual-atom network), is shown in Fig. 7. Each atomic reactant, TS and product descriptor was considered to be a channel c in a CNN. After normalising the features in each channel we chose to perform a depthwise convolution across channels, with each feature in the descriptor being convolved by an independent 1 × 3 learnable convolution filter. The output of this convolutional layer was therefore a single vector of length f = 2 216 for the , SPA M(b) descriptors illustrated here. This was then fed into three feed-forward layers usH

ing the rectified linear unit (ReLU) activation function, reducing the descriptor in size until only two features remained in the output layer: the E a and ∆ H r output predictions.

However, in seeking to learn intrinsic reaction properties from individual atomic environment descriptors, different reactant/TS/product systems will have different numbers of atoms n a . In order to correctly divide reactions into batches of size B for use in the mini-batch gradient descent method commonly employed for training such models, the SPA H M(b) descriptors for all atoms in the reactions of each mini-batch were stacked into a single ( n total a × f ) matrix (one per reactive channel), where n total a = ∑ B i =1 n i a . The number of atoms in each reaction within the mini-batch ( n i a ) was also fed into the CNN as an input.

Once a contribution to E a and ∆ H r has been predicted for each atom in the batch, the model then uses these input atom counts to redistribute these contributions among the input reactions, creating a ( n Bmax a × B ) matrix for each property (where n Bmax a = max i ∈ B n i a ), padded with zeroes in the case that a given reaction has n i a &lt; n Bmax a . Columns of these matrices are then summed to yield the final E a and ∆ H r predictions for each reaction in the batch, with the addition of a ReLU activation function after the E a summation to constrain its final values to be exclusively positive.

## 3.3 DCINet Results

SPA M(b) descriptors were generated for every reactant, TS and product in both the radicalH extended training set and the C I, 0 01 . 4 validation set using the Q-stack Python package. 62 TS descriptors were calculated from both the true DFT NEB TS geometries and the geodesic interpolated TS geometries to compare the quality of model predictions against a best case scenario, where the interpolation exactly matches the true TS.

We implemented DCINet in PyTorch and trained on a randomly shuffled 80% split of the radical-extended reaction dataset, with 20% left as a held-out test set. 63 Its parameters were optimised with the Adam stochastic optimiser using an initial learning rate of 1 × 10 -3 , reduced by a factor of 0 3 whenever the loss of the test set predictions failed to decrease .

for 8 epochs, down to a minimum of 1 × 10 -6 . 64 The loss function was defined as a linear combination of the mean squared error (MSE) of E a and ∆ H r predictions:

$$l \left ( { \mathbf x } ^ { E _ { a } }, { \mathbf y } ^ { E _ { a } }, { \mathbf x } ^ { \Delta H _ { r } }, { \mathbf y } ^ { \Delta H _ { r } } \right ) = \frac { \alpha } { B } \sum _ { i = 1 } ^ { B } L \left ( { \mathbf x } ^ { E _ { a } } _ { i }, { \mathbf y } ^ { E _ { a } } _ { i } \right ) + \frac { 1 - \alpha } { B } \sum _ { i = 1 } ^ { B } L \left ( { \mathbf x } ^ { \Delta H _ { r } } _ { i }, { \mathbf y } ^ { \Delta H _ { r } } _ { i } \right ) \\ & \text{where } L \left ( { \mathbf x }, { \mathbf y } \right ) = \left \{ l _ { 1 }, \dots, l _ { B } \right \} ^ { T }, \, l _ { n } = \left ( { \mathbf x } _ { n } - { \mathbf y } _ { n } \right ) ^ { 2 } \ \ ( 2 )$$

where x E a and x ∆ H r are vectors of activation energy and reaction enthalpy predictions, y E a and y ∆ H r are their respective target values, and α is a tuning parameter that allows for control over the contribution of the two targets towards the overall loss function. In this case, α was set to 0 9, meaning that 90% of the loss was controlled by the accuracy of the . E a predictions. In general, a value over α = 0 5 should be used here as . E a is the primary training target. The batch size B was set to 200.

The CNN was trained on an 80:20 training:testing set split of the radical-extended database, using the DFT-level TS geometries in both cases. Training proceeded until the loss of the test set predictions had stopped changing by more than 1 × 10 -4 for over 12 epochs, in total taking 298 epochs to complete. The correlation plots for the predictions of the training and test set reactions, along with the predictions of the model when applied to the reactions of the C I, 0 01 . 4 validation set using geodesic-interpolated TS geometries, are shown in Fig. 8. Loss function plots visualising the progression of the learning rate are shown in Fig. S6.

The training and test set performance of this DCINet model matches that of both KPM and Chemprop, although it does not require averaging over a committee of models to achieve this accuracy. Relatively few reactions are poorly represented - 81% of the ∼ 5300 test set reactions fall under Ismail et al.'s 6 kcal/mol error target. More impressive are the validation set results, which outperform any of the other models tested here in both E a and ∆ H r predictions. While these predictions are still not of sufficient accuracy to use in kinetic simulations (where errors are made exponentially worse during rate constant calculation), the

Figure 8: Correlation plots for a)-c) E a and d)-f) ∆ H r predictions of the DCINet model when applied to the training set, test set and C I, 0 01 . 4 validation set respectively. All error metrics are given in units of kcal/mol.

<!-- image -->

incorporation of spin-aware descriptors and low-cost, approximate TS information provides a marked improvement in this challenging area of chemical reaction space. This is likely due to the additional context provided to DCINet extending the area of applicability of the training data.

We can verify that the inclusion of approximate TS information has improved E a prediction by constructing a variant of the DCINet architecture that excludes TS information entirely. This means that the SPA H M(b) descriptors for only two reactive channels - the reactants and the products - are convolved over to yield a reaction descriptor that enters DCINet's feed-forward layers. The resulting correlation plots for the same training, testing and validation sets are shown in Fig. S7. While ∆ H predictions are only slightly worsened by this change, E a predictions are significantly worsened in all three datasets. This model performs similarly to Chemprop when predicting activation energies of the validation set,

which indicates that convolution of only the reaction endpoint descriptors yields a similar lack of reactive context as the subtractive approach taken by Chemprop and KPM.

The results in Fig. 8 do not represent this model's true limit of applicability. We consider two avenues of improvement: the training set could be expanded, or the accuracy of the TS approximations could be improved. The potential effect of the former improvement can be determined by examining the learning curve shown in Fig. 9. The lack of a plateau in E a prediction errors as the training set size is increased indicates that the DCINet architecture continues to be capable of learning more if more training reactions were to be provided.

Figure 9: Learning curve for the DCINet model, showing the decrease in test set E a prediction MAEs as training set size is increased.

<!-- image -->

As a proof-of-principle, we can also address the quality of the TS approximation by using the true DFT-level TS geometries when predicting reactive properties, instead of using approximate TS geometries. These predictions, shown in Fig. 10, provide a significant improvement over the approximate TS predictions; the errors within the validation set decrease significantly. While this is not a feasible technique since obtaining these TS geometries requires performing the computationally expensive calculations that the model seeks to avoid, it is nevertheless informative, as these results represent the best-case scenario of predictions

Figure 10: Correlation plots for E a and ∆ H r predictions of the DCINet model when applied to the C I, 0 01 . 4 validation set when using true DFT-level TS geometries, representing a bestcase scenario. Included are four examples of per-atom predictions of these quantities made by the model - atoms are labelled with left and right semicircles according to the colour bars for E a and ∆ H r contributions respectively.

<!-- image -->

with this model architecture. Any remaining errors greater than the test set accuracy are due to reactions being truly out-of-distribution, where additional training data is required to obtain greater accuracy.

While it would be useful to compare the positions of reactions in a low-dimensional latent space as in Figs. 2 and 4, this is unfortunately not possible due to the atomwise predictions generated by the model, which prevent the formation of descriptors for entire reactions that can be dimensionally reduced. However, this architecture does create opportunities for other unique analyses; we can instead analyse the predicted atomic contributions to E a and ∆ H . While such values have little physical meaning, they can nevertheless be interpreted and used for prediction of potential reactive sites and mechanisms. Fig. 10 also includes a handful of examples from the validation set reactions to demonstrate this capability.

The first two examples are well-predicted by the model, resulting in E a and ∆ H r prediction errors under 5 kcal/mol. Example 1 details an attack on a C -C bond in a pentyl radical by a methyl radical, a process with relatively little E a required and which is neither endo- or exothermic, as while the reactants are two unstable radicals, so are the products. The model predicts a positive E a contribution from the two carbons between which a bond is

broken, mainly centred on the carbon receiving a radical electron. The radical pentyl carbon contributes negatively to the E a , possibly due to its perceived destabilising effect on the rest of the molecule being attacked. The radical methyl carbon does not contribute to the E a as it is already reactive and does not require additional energy to attack the C -C bond, but it does contribute negatively towards ∆ H r as it becomes stably bonded to another carbon in the products. This is cancelled out in the final ∆ H r summation by an equivalent positive contribution on the pentyl carbon receiving the radical electron, which is destabilised by the same amount.

Example 2 depicts a very high-barrier process where a C --C double bond is unevenly broken by an attack from a propyl radical, resulting in a 2-pentyl radical and a molecule of carbene. This requires the π -electrons from the bond to migrate to the terminal carbon of the propene, while a σ -electron is used in forming the new bond. This demands high E a contributions from the carbons being split, particularly on the one receiving the radical electron. Both carbons are also destabilised in the products, resulting in positive ∆ H r contributions, while the radical propyl carbon is stabilised by bond formation.

These examples show how the atomwise predictions of this model can be rationalised into reactivity and stability descriptors, similar to Fukui indices, a by-product of electron density calculation in DFT which allow chemical insight into how prone to electrophilic and nucleophilic attack individual atoms may be. 65,66 As such, Fukui indices are commonly used for predicting the reactivity of atoms in molecules. 67 Likewise, these contributions are conceptual indicators which are based on (in this case, through SPA H M(b)) electronic structure calculations, and while they have little physical meaning, can aid in the visualisation and prediction of complex reactive processes.

Likewise, this form of analysis can also be used to determine how and why this DCINet model fails to predict properties for other reactions; the latter two examples are reactions for which the model experiences high predicted errors for E a and ∆ H r . Example 3 shows an attack by ethyne on a C -H bond in methane. This is a highly unlikely reaction to occur in

all but the most extreme conditions, but the DCINet model underpredicts both its E a and its ∆ H r by nearly 100 kcal/mol. Based on the accurately predicted examples, we would expect a larger positive E a contribution on the ethyne carbons, across which a π bond is being broken, and greater destabilisation on the methyl carbon on the form of a larger positive ∆ H r contribution.

In example 4, both E a and ∆ H r are instead overpredicted. This reaction depicts a radical attack of a C -H bond in a vinyl radical by a 2-butenyl radical. The model likely overpredicts the E a contribution of the departing hydrogen, and fails to account for the additional stabilisation gained by both the attacking and the attacked carbons, the former of which loses its radical electron while the latter participates in a strong triple bond.

These reactions may be poorly predicted for a number of reasons, perhaps the most obvious of which is simply that they lie too far outside of the chemical reaction space covered by the training set. Both contain hydrogen abstractions that either form or break C ---C triple bonds; these are poorly represented as only 75 radical reactions containing C ---C bonds exist in the training set, and heterolytic cleavage across C -H bonds is similarly sparse. The validation set reactions that are predicted very well instead tend to involve C -C single and double bonds being modified, making it likely that better prediction would require more examples of these rare reaction types.

## 4 Conclusions

To examine the potential hurdles that need to be overcome for ML models to accurately drive kinetic simulations of large CRNs, we have undertaken a comprehensive analysis of the performance of several contemporary ML models for chemical reaction property prediction. We defined a challenging validation dataset of reactions taken from an algorithmically-generated CRN for the pyrolysis of ethane. These reactions frequently contain free radical species, which are typically not covered by contemporary reaction datasets. We tested the appli-

cability of three previously proposed ML models: KPM, Chemprop and NeuralNEB. 24,25,44 Although the training- and test-set performance of these models seems promising, none were transferable enough to yield activation energy predictions suitable for kinetic simulations. For example, KPM and Chemprop - two models based on a reactant/product-difference descriptor - likely do not know enough about the nature of the TS to distinguish between ionic and radical reaction pathways, leading to significant errors.

Based on these findings, we proposed a model that takes advantage of computationally inexpensive, approximate TS information to enhance transferability. We developed the DCINet architecture - a CNN using the electronic structure-derived SPA H M(b) descriptors to represent atomic environments across reactants, geodesic-interpolated TSs and products - and showed that it achieves higher accuracy when applied to the validation set than any other model tested here. While still far from usable for chemical kinetics, we proved that even cheap interpolated TS information can dramatically improve models and that the prediction improves as the TS becomes more accurate.

We also discovered that the atomic contributions to reactive properties which DCINet produces as an intermediate step can be used as interpretable indicators for atomic chemical reactivity and stability when applied to well-predicted reactions. Conversely, when applied to poorly predicted reactions, the contributions which fail to be rationalised by chemical intuition indicate functional groups and reactive trends that are not present in the training data, highlighting which types of reactions require additional training data to improve the overall predictive accuracy. The DCINet architecture shows room for further improvement as the training set increases in size, indicating that such future additions would likely yield further increases in predictive accuracy.

Aside from the enrichment of the training data, there are further avenues by which DCINet could be improved in the future; the most obvious of these is by improving the accuracy of the TS estimation. While we showed that geodesic interpolation performs well across a diverse range of reactions, machine-learned solutions such as TSDiff have shown

great promise when estimating TS geometries, and could therefore stand to increase the predictive accuracy of DCINet too. 68 Additionally, the convolutional nature of DCINet enables the inclusion of snapshots along the reaction path other than the reactants, TS and products. Such snapshots could be generated through interpolation and added as extra channels in the convolution to provide further context to models about a reaction's movement across the PES. These potential improvements, coupled with the increased transferability already achieved here, may pave the way for substantially enhanced ML predictors of reactive properties.

## Author contributions

Conceptualization : Joe Gilkes and Scott Habershon. Software, data curation and visualisation : Joe Gilkes. Writing and formal analysis : Joe Gilkes, Reinhard Maurer and Scott Habershon. Supervision : Mark Storr, Reinhard Maurer and Scott Habershon.

## Code and data availability

The reaction-discovery code used to generate chemical reactions for further ML testing, Kinetica.jl , is available on GitHub at https://github.com/Kinetica-jl/Kinetica.jl . KineticaKPM.jl is similarly available on GitHub at https://github.com/Kinetica-jl/ KineticaKPM.jl . Both packages are installable through the Julia language Pkg package manager. Data will be made available through the University of Warwick research archive portal (wrap.warwick.ac.uk) upon acceptance.
<|endofpaper|>