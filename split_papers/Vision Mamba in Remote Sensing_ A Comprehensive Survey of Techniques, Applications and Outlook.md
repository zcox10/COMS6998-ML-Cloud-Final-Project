<|startofpaper|>
## Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook

Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Huiyu Zhou, Jinchang Ren, Shiming Xiang, Xiangtai Li, Guangliang Cheng :

Abstract -Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-TransformerMamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github. com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements.

Vision Transformers (ViTs) [3], [4], have demonstrated notable success in this domain. However, the inherent characteristics of remote sensing imagery challenge the further enhancement of CNNs- and ViTs-based networks: (1) Rich Spatial Dependencies : Remote sensing images exhibit complex spatial relationships that often exceed the local modeling capabilities of CNNs with their limited receptive fields, despite linear computational complexity and versatile modeling capabilities [5]-[7]; (2) High Resolution : The extremely high resolutions of remote sensing images impose prohibitive computational demands on Transformer-based models, often resulting in unacceptable levels of computational complexity. Despite these inherent constraints, CNNs and ViTs have long dominated remote sensing applications, prompting researchers to pursue architectures that can achieve global modeling with linear computational efficiency.

Index Terms -Vision Mamba, Remote Sensing, Comprehensive Survey, State Space Models, Scan Strategy

## I. INTRODUCTION

Recent advances in remote sensing have witnessed remarkable progress through deep learning methodologies for extracting features from complex data [1], [2]. Conventional architectures, particularly Convolutional Neural Networks (CNNs) and

Mr. Muyi Bao is with the School of Advanced Technology, Xi'an JiaotongLiverpool University.

- Dr. Shuchang Lyu is with the School of Electronics and Information Engineering, Beihang University.
- Dr. Zhaoyang Xu is with the Department of Paediatrics, Cambridge University.

Prof. Huiyu Zhou is with the School of Computing and Mathematical Sciences, University of Leicester.

Prof. Jinchang Ren is with the Department of Computing Science, Robert Gordon University.

Prof. Shiming Xiang is with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.

- Dr. Xiangtai Li is with Nanyang Technological University, Singapore.
- Dr. Guangliang Cheng is with the Department of Computer Science, University of Liverpool.

: Corresponding author: Guangliang Cheng

To address these challenges, state space models (SSMs)based methods have emerged as a promising alternative, offering both linear computational complexity and global modeling capabilities. These two characteristics of SSMs precisely overcome the limitations of CNNs and ViTs in remote sensing applications. Rooted in classical system theory, SSMs have found widespread applications across disciplines including reinforcement learning [8], computational neuroscience [9], and control systems [10]. The SSM maps input sequences to latent states that encapsulate historical context, enabling sequential prediction according to the hidden state. Some work has tried to incorporate SSM into the deep learning framework [11]-[21]. Early SSM implementations faced computational bottlenecks until Structured State Space sequence (S4) model [11] addressed these limitations through parameterized state matrices. Then, Mamba [20] introduces Selective State Space for Sequences (S6) that incorporates dynamic time-aware mechanisms via regressed step size parameters, enabling context-aware information propagation or forgetting through hidden states. The Mamba architecture [20] further advanced this paradigm through simplified gated SSM blocks, achieving state-of-the-art (SOTA) performance. With its linear scalability for high-resolution imagery, ability to capture long-range spatial dependencies and exceptional capability of feature representation, Mamba-based architectures have demonstrated strong potential as a next-generation solution for remote sensing tasks, bridging the gap between computational efficiency and global modeling.

Originally developed for natural language processing (NLP) with landmark studies [20], Mamba technology has swiftly expanded its application to computer vision (CV). Similar to the ViT [3], innovative Mamba-based architectures like

Fig. 1: A diagram that summarizes the pipeline of the survey.

<!-- image -->

Vision Mamba (Vim) [22] and Visual Mamba (VMamba) [23] employ patch embedding and the multi-directional scan strategy to convert 2D images into 1D sequences, making them compatible with Mamba's processing paradigm. Building on these foundational works, the remote sensing community has rapidly explore Mamba's potential, with numerous studies [24]-[36] adapting Mamba architecture to overcome domainspecific challenges. These innovations have propelled Mambabased models to achieve the SOTA performance across various remote sensing tasks, including object detection [24]-[26], dense prediction [27]-[30], and others [31]-[36].

- , SSM Formula Enhancement: While Mamba [20] established the initial adaptation of SSMs for deep learning, subsequent works have further refined this formulation. To our knowledge, we are the first to systematically review SSM formula improvements in this domain.

Although there are several surveys [1], [37]-[41], they tend to emphasize the problem-solving approaches specific to natural imagery, often overlooking or underrepresenting the distinctive features of remote sensing images. Consequently, we have conducted a comprehensive survey dedicated to the remote sensing domain, detailing the current research progress of Mamba technology in this field, its applications, and some potential future trends. To the best of our knowledge, this represents the first survey in the remote sensing field with Vision Mamba technology, which would further benefit the remote sensing community.

Contribution: This survey makes several key contributions to the emerging field of Mamba-based architectures in remote sensing. First, we provide a systematic introduction to Mamba's foundational concepts, followed by a brief review of approximately 20 vision Mamba backbone papers. Then, we review and synthesize about 120 remote sensing papers. Our analysis adopts both micro- and macro-architectural perspectives to offer a holistic understanding of recent Mamba advancements in the remote sensing domain.

Micro-architecture Advancements: We examine three critical advancements within the inner mechanism of Mamba blocks:

- , Scan Strategy: We propose a novel taxonomy for scan strategies (crucial for processing 2D/3D imagery as 1D sequences) comprising five elements: preprocessing, scan sampling, scan direction, scan pattern, and postprocessing . This framework provides the first comprehensive treatment that includes preprocessing and postprocessing stages into scan strategy.
- , Multi-modal and Bi-temporal Feature Interaction: Focusing on remote sensing applications, we categorize existing approaches into four distinct methodologies for handling multi-modal data and bi-temporal interactions. To our knowledge, our analysis offers the first detailed examination of the multi-modal and bi-temporal interaction based on the Mamba architectures.

Macro-architecture Advancement: We analyze four innovative advancements for overall architecture advancement:

- , Hybrid Architectures: We analyze integration for combining Mamba with CNNs and Transformers, examining both basic stackable blocks and overall architectural designs.
- , Substitution in Framework Adaptation: We survey the incorporation of Mamba blocks into established frameworks including U-Net [42], YOLO [43] and Diffusion models [44].
- , Learning Paradigms: In addition to the conventional supervised learning, our review encompasses applications of unsupervised, self-supervised, and prompt learning paradigm.
- , Frequency Domain Operations: We document implemen-

tations incorporating frequency domain operations within Mamba architecture, including Fast Fourier Transform, 2D discrete cosine transform and wavelet transform.

Then, we conducted an exhaustive summary and analysis of Mamba's applications in remote sensing imagery, such as classification, object detection and semantic segmentation. This involved collating and comparing the performance of modern architectural approaches, including Mamba, Transformers, and CNNs, facilitating a comprehensive comparative study that highlights the strengths and weaknesses of each architecture within this specific application area. Finally, drawing on the current developmental trends of Mamba in remote sensing, we proposed several potential future trends. These suggestions aim to provide valuable insights and serve as a reference for ongoing and future research efforts, potentially guiding the next steps in the evolution of remote sensing technologies.

Survey Scope: This survey comprehensively examines the foundational literature related to Vision Mamba within the field of remote sensing. We restricted our review to works that were published or appeared as preprints on Arxiv prior to February 2025. Although there are many preprints or published works with vision mamba in natural images and videos, we only include the most representative works.

Organization: The rest of the survey is organized as follows. Overall, Fig. 1 shows the pipeline of this survey. We first introduce the preliminary knowledge of Mamba in Section II. This is followed by a detailed review of the backbone architectures of Vision Mamba in Section III. Then, we delve into the specific advancements of Mamba architecture, discussing micro-architecture advancements in Section IV and macroarchitecture advancements in Section V. We compare the experiment results of several downstream tasks in Section VI. Finally, we raise the current challenges and future directions in Section VII and conclude the survey in Section VIII.

## II. PRELIMINARY KNOWLEDGE

## A. State Space Model

Based on the Kalman filter [10], State Space Models (SSMs) [19] input a one-dimensional continuous sequence x t p q P R N and convert x t p q and intermediate hidden state of last moment h t p q P R N into hidden states of this moment h 1 p q t P R N according to x t p q and the previous hidden state. The output y t p q P R is then computed based on x t p q and the hidden state of this moment h t p q . This process can be formulated as follows:

$$h ^ { \prime } ( t ) = \mathbf A h ( t ) + \mathbf B x ( t ) \quad \ \ ( 1 ) \ \stackrel { \text{stack} } { \cdots }.$$

$$y ( t ) = \mathbf C h ( t ) + \mathbf D x ( t ) \quad \quad \ \ ( 2 ) \ \stackrel { \mathbf v o l o c } { \mathbf f o r c } \, \mathbf l }$$

, where A P R N ˆ N is a learnable state matrix; B P R N ˆ 1 , C P R R ˆ 1 , and D P R R ˆ 1 are learnable projection parameters. In Eq. (2), D x t p q is sometimes regarded as a residual connection in deep learning neural networks, and thus may be omitted.

To handle the discrete input sequence x ' p x , x 0 1 , . . . , x L q P R L , the Structured State Space Sequence (S4) model [11] discretizes all learnable parameters in Eq.

(1) and Eq. (2) by introducing a time step size ∆ , which is projected through a simple multi-layer perception (MLP). The time step size ∆ can be interpreted as the resolution of the continuous input. Then, by applying the Zero-Order Hold (ZOH) method, the continuous parameters A and B are converted into their discrete counterparts A B , , and C as follows:

$$\overline { A } = \exp ( \Delta \, A )$$

$$\overline { B } = ( \Delta \mathbf A ) ^ { - 1 } ( \exp ( \Delta \mathbf A ) - \mathbf I ) \cdot \Delta \mathbf B \approx \Delta \mathbf B \quad ( 4 )$$

$$\overline { c } = c$$

, where A P R N ˆ N , B P R D N ˆ and C P R D N ˆ . After discretization, Eq. (1) and Eq. (2) can be reformulated as follows:

$$h _ { t } = \overline { \mathbf A } h _ { t } + \overline { \mathbf B } x _ { t }$$

$$y _ { t } & = \overline { \mathbf C } h _ { t }$$

This linear recurrence calculation can be accelerated by SMM convolution kernel as follows:

$$y & = x * \overline { K } \\ \text{with} \quad \overline { K } & = ( \overline { C B }, \overline { C A B }, \dots, \overline { C A ^ { L - 1 } B } )$$

$$( 8 )$$

, where ˚ is the convolution operation and K P R L is the SSM kernel.

## B. Mamba: Selective SSM

In the SSMs of S4, all learnable parameters remain fixed after training and do not dynamically change with the input. This linear time-invariant (LTI) nature is pointed out by Mamba [20] as a fundamental limitation of SSM in the context-dependent reasoning. To address this limitation, Mamba introduces a selection mechanism. Specifically, the parameters B P R B ˆ ˆ L N , C P R B ˆ ˆ L N , and ∆ P R B ˆ ˆ L N in Eq. (6) and Eq. (7) are generated by projecting the input x P R B ˆ ˆ L N through a simple learnable MLP, which enables input-dependent parameter values. The process can be formulated as follows:

$$\mathbf B, \mathbf C, \Delta = L i n e a r ( \mathbf x )$$

The core building block of Mamba is a simplified attention architecture. Unlike conventional SSM designs that employ a stacked combination of linear attention-like blocks and MLP blocks, similar to Transformer [4] architectures, Mamba unifies these two fundamental components into a single structure, forming the Mamba block. As depicted in Fig. 2a, the Mamba block can be analyzed from two distinct perspectives. First, it substitutes the multiplicative gating mechanism in linear attention-like or H3 [14] blocks with an activation function. Second, it integrates the SSM transformation directly into the primary computational pathway of the MLP block. The overall architecture of Mamba is composed of multiple Mamba blocks, interspersed with standard normalization layers and

<!-- image -->

Sequential Mamba Block

<!-- image -->

<!-- image -->

Parallel Mamba Block

(a) The architecture of Mamba, a simplified attention block that inte- (b) grates the SSM transformation directly into the primary computational quential pathway of the MLP block and directly uses activation function on the series. (Right) Parallel blocks that selected opgating pathway. Mamba-2 Block Architectures: (Left) Seblock that operations are applied in erations executes concurrently before fusion.

residual connections. In detail, the activation function uses SiLU [45] and Swish activation [46], and we normally call the activation score in the gating pathway as z . This architecture effectively captures long-range dependencies while maintaining the linear scalability characteristic of SSMs with respect to sequence length, becoming a new promising foundation model for CV.

## C. Mamba-2

Mamba-2 introduces a unified architecture for sequence modeling through Structured State Space Duality (SSD) , bridging structured state space models (SSMs) and selfattention mechanisms [21]. At its core, SSD reinterprets SSM computations as matrix multiplications over semiseparable matrices -structured matrices with subquadratic parameterization and linear-time algorithms. This dual view enables two computation modes:

- , A linear-time recurrent mode using scalar-identity transitions ( A t ' a I t ), optimized via tensor contractions;
- , A quadratic attention-like mode with data-dependent semiseparable masks ( L ij ' ś i k ' ` j 1 a k ), replacing softmax with cumulative gating [47].

1) Architectural Innovations: Key advancements include: 1) Parallel Parameterization : Co-projecting SSM parameters ( A,B,C ) and values ( X ) akin to attention's QKV mappings, reducing synchronization bottlenecks by 50% compared to Mamba-1 [20]; 2) Multi-Input SSM (MIS) Heads : Sharing B C { projections across channels while keeping A headspecific, enabling 8 ˆ larger state dimensions (e.g., N ' 512 ) with minimal overhead; 3) Hybrid Blocks : As illustrated in Fig. 2b, Mamba-2 supports both sequential and parallel configurations of its core components (e.g., convolutional layers, SSM transformations, and nonlinear activations), allowing flexible trade-offs between computational efficiency and model capacity.

2) Efficiency and Performance: Mamba-2 achieves Pareto dominance over Transformers and Mamba-1, attaining 6.09 perplexity on the Pile (2.7B parameters) with 3 ˆ faster training than FlashAttention-2 at 16K context [48], [49]. Its blockwise SSD algorithm splits sequences into chunks, combining intra-chunk matrix multiplications and inter-chunk state transitions for 2-8 ˆ speedups. System optimizations include tensor parallelism with single all-reduce per block and sequence-parallel state propagation for billion-token sequences. Evaluations highlight 98% accuracy on memoryintensive tasks (e.g., multi-query associative recall), outperforming Mamba-1 by 16% [50].

## III. VISION MAMBA BACKBONES

In this section, we provide an overview of Vision Mamba backbones. For an in-depth exploration, readers are encouraged to refer to other related surveys [1], [37], [38], [41]. We begin by introducing two pioneering studies that first integrate Mamba into the computer vision domain, highlighting their core ideas, and then proceed to discuss related subsequent advancements.

Vision Mamba (Vim) [22] is the pioneering work that introduces Mamba to the field of computer vision. Inspired by the ViT framework [3], Vim first segments images into 2D patches, which are subsequently vectorized using CNNs. These vectorized patches are enhanced with positional embeddings to preserve spatial context, establishing a standard procedure adopted by subsequent studies. Additionally, Vim appends a class token in order to conduct classification. To overcome the inherent causality limitations of SSMs, Vim introduces a bidirectional scan strategy. Specifically, it first flattens the 2D patches into a 1D sequence and then employs both forward and backward pathways, scanning the sequence from the beginning to the end and from the end to the beginning, respectively. This non-hierarchical architecture of Vim comprises multiple identical Vim blocks.

Visual Mamba (VMamba) [23] addresses two critical challenges in applying SSMs to 2D images: 1) the inadequacy of using 1D CNNs before SSMs for capturing 2D spatial structures; 2) the intrinsic causal nature of SSMs, which converts 2D image patches into sequential 1D data, losing spatial information. To overcome these issues, VMamba introduces the Cross-Scan Module (CSM). Specifically, CSM replaces traditional 1D CNNs with 2D Depth-Wise CNN layers (DWConv). This adaptation better preserves and extracts spatial contextual information within images and has subsequently become a standard approach in subsequent researches. Additionally, CSM incorporates a 2D-Selective-Scan (SS2D) mechanism that systematically scans the entire image from four distinct directions: vertically and horizontally from both the top-left and bottom-right patches. It then aggregates the resulting four 1D sequences into a unified representation. This multi-directional scan strategy effectively mitigates the mismatch between the causal nature of selective SSMs and the spatial characteristics of images, enhancing the overall representation capability.

Considering one fundamental challenge in adapting SSMs to vision tasks lies in effectively transforming 2D spatial relationships into sequential representations, numerous work has focused on modifying scan strategies to increase the model's understanding of non-causal 2D data. Building upon the approaches of Vim and VMamba, some work focus on scan strategies (discussed in Section IV-B4). Mamba-ND [51] generalizes scan strategies to accommodate 3D data by employing block-level alternation of scan directions along the height, width, and time axes. LocalVMamba [52] integrates local window-based scanning (Fig. 8.D) with learnable pathway weights through DARTS-inspired architecture search [53]. PlainMamba [54] and FractalMamba [55] employ a continuous 2D scan pattern (Fig. 8.B) and Hilbert curve pattern (Fig. 8.E), respectively, aiming to enhance contextual understanding capabilities and address the issue of discontinuity. Subsequently, some researchers have leveraged preprocessing methods (discussed in Section IV-C2) to enhance the performance of SSMs. MS-VMamba [56] applies multi-scale SSMs, while GroupMamba [57] divides the input feature into four groups in channel dimensions and applies a separate SSM independently to each group.

Some researchers have developed Mamba-based architecture on designs that are proven effective in CNN/Transformer blocks. MambaMixer [58] follows the core idea of MLPMixer [59] to combine Mamba-based token mixing with channel mixing, and follows the design of DenseNet [60] and DenseFormer [61] to allow blocks to access the previous features. Following [62], MambaR [63] evenly inserts register tokens into the input sequences and recycle registers for final decision predictions, which can help model focus on more semantic regions. ARM [64] follows the GPT families [65]-[67], which utilizes the autoregressive modeling self-supervised framework. Similar to EViT [68], MambaPruning [69] introduces a token pruning method specifically designed for Mamba-based model. Inspired by [70], ShuffleMamba [71] introduces a training regularization technique that randomly shuffles token sequences during training to improve positional transformation invariance and overall model performance.

On one hand, some papers focus on the micro-architecture designs to enhance model's ability. VSSD [72] introduces noncausal processing for image data by decoupling interaction magnitudes between hidden states and tokens. SparX-Mamba [73] introduces a efficient sparse cross-layer feature aggregation method specifically designed for SSMs. Vim-F [74] integrates frequency domain information into the basic block to achieve a better global receptive field. On the other hand, some work focuses on the macro-architecture advancement. SiMBA [75] utilizes hybrid sequence-channel modeling, effectively resolving the instability issues in large-size vision Mamba. StableMamba [76] establishes a robust interleaved Mamba-Attention framework that successfully scales SSMs to over 100 million parameters without resorting to knowledge distillation techniques. MambaVision [77] redesigns the Mamba architecture and integrates CNNs and ViT block on the architecture level, aiming at enhancing modeling capability.

## IV. MICRO-ARCHITECTURE ADVANCEMENT

This section focus on the inner mechanism within microarchitecture of Mamba block. Three core points are summarized: (1) SSM Formula Improvement (Section IV-A); (2) Scan Strategy (Section IV-B) and (3) Multi-Modal and Bi-Temporal Feature Interaction via Mamba (Section IV-C).

## A. SSM Formula Improvement

Vanilla Mamba initially adopts the ZOH rule to discretize continuous-time SSM and utilizes a selection mechanism to dynamically adjust model parameters based on the input sequence. Building upon this, several subsequent studies have modified the basic SSM formula to achieve enhanced performance and fulfill specific objectives. In total, 9 papers have contributed to the refinement of the SSM formula, which can be categorized into enhancing local context understanding [78], [79], interacting multi-modal and bi-temporal features [35], [36], [80]-[82], utilizing alternative discretization methods for continuous signals [83] and using alternative definition of hidden states [84]. These contributions are visually summarized in Fig. 2.

## 1) Context-SSM for Local Feature Enhancement :

Context-SSM aims to naturally incorporate local context information into modeling long-range dependencies. MambaMOCO [78], building upon insights from MambaIRv2 [85], identifies two critical insights for improving SSMs in CV: (1) the output matrix C in Eq. 7 functions as a 'Query' for local context information, which can address the inherent limitation of transforming 2D images into 1D sequences; (2) enhancing local context allows the model to better focus on local interactions while preserving its ability to model global dependencies. In response to these insights, Mamba-MOC [78] introduces the Context-SSM, as depicted in Fig. 2.a. ContextSSM incorporates two new parameters, P and D , which are generated by multi-scale CNNs and a 1 ˆ 1 CNN. The parameter P and D capture local context information, ensuring that the model can effectively capture both local context and long-range dependencies for the input data. Similarly,

Fig. 2: Overview of Recent Advancements in SSM Formulations. This figure categorizes improvements into four major aspects: (a) Context-SSM [78], [79] for local feature enhancement, integrating local context through CNNs; (b) Cross-SSM [35], [36], [80]-[82] for multi-modal and bi-temporal feature interaction, enabling cross-modal/temporal feature interaction at the parameter level; (c) New discretization methods for SSM [83], introducing second-order Runge-Kutta (RK2) for improved continuous-time approximation; and (d) Alternative hidden state definitions [84], utilizing a Minimum Search Tree (MST) to redefine hidden state transitions. These refinements collectively enhance the efficiency and expressiveness of SSMs in various applications

<!-- image -->

AFA-Mamba [79] simply adds local context feature eg L p p q with output equation together, which improves the ability of understanding local context.

- 2) Cross-SSM for Multi-Modal and Bi-temporal Feature Interaction : In multimodal and bi-temporal settings, enabling interactions between features from different modalities is crucial for obtaining comprehensive representations. Recent advancements have introduced modifications to the SSM formula to inherently support cross-modal interactions, as highlighted in [35], [36], [80]-[82] and illustrated in Fig. 2.b.

modality. By influencing both the output equations and the hidden state updating processes, these methods comprehensively integrate cross-modal information, significantly strengthening the interactions between modalities.

The COMO approach [80] adopts an intuitive design, generating outputs by integrating hidden states from one modality with feature representations from another. This design facilitates dynamic and interactive information fusion between modalities. In contrast, S2CrossMamba [81] introduces an alternative mechanism by combining the parameters B and C derived from each modality to form combined parameters B and C . This approach enables modality interaction directly at the parameter level of the SSM. Consequently, the resulting output incorporates feature information from both modalities, differing from approaches that initially combine modality features before projecting them into the state matrix.

Furthermore, FusionMamba [35], MSFMamba [36], and SegMamba-OS [82] share the objective of leveraging SSM parameters (such as A B C , , , and ∆ ) from one modality to influence another modality's feature representations. Specifically, SegMamba-OS [82] applies only the state matrix C of the second modality directly to the output equation of the first modality's features. This strategy effectively narrows the semantic gap between modalities, enhancing cross-modal relevance. Expanding upon this, FusionMamba [35] and MSFMamba [36] utilize all SSM parameters ( A B C , , , and ∆ ) from one modality to impact the feature representations of another

3) New Discretization Method for SSM : Vanilla Mamba employs the ZOH rule to discretize continuous data, which serves as a fundamental approach for integrating SSMs within deep learning frameworks. Despite its widespread adoption, the ZOH rule exhibits a notable limitation, as highlighted by RSVMamba [83]: it approximates hidden states solely at discrete sampling points, overlooking dynamic variations occurring within intervals between these points. Consequently, this approximation can lead to local errors that accumulate as the frequency of sampling points increases.

To address this limitation, RSVMamba introduces the second-order Runge-Kutta (RK2) method [86] for discretization, as illustrated in Fig. 2.c. The RK2 approach enhances accuracy by computing intermediate states within each discrete interval, providing a more precise approximation of the underlying continuous-time dynamics. Moreover, the RK2 method maintains manageable computational complexity compared to higher-order alternatives, such as the fourth-order Runge-Kutta method, thus achieving an effective balance between accuracy and computational efficiency.

4) New Definition of the Hidden States : Beyond adjusting the SSM discretization, another promising direction involves redefining the hidden states themselves. TTMGNet [84] exemplifies this by first converting 2D features into a Minimum Spanning Tree (MST) based on cosine similarity. In this model, each token's hidden state is associated with the state transmission matrices of neighboring tokens rather than being sequentially iterated through Eq. 6. This conceptual modification is illustrated in Fig. 2.d. Although tailored specif-

Fig. 3: Illustration of the scan strategy pipeline, comprising five key components, including feature preprocessing, scan sampling, scan directions, scan pattern, and feature post-processing. For clarity, an additional example is provided demonstrating the case without any preprocessing or postprocessing operations.

<!-- image -->

ically to the tree-based MST topology, this approach highlights an innovative perspective for enhancing SSM performance by rethinking the construction and definition of hidden states.

## B. Scan Strategy

multispectral data) as C . Upon converting a 2D feature map into a 1D sequence, the number of resulting tokens is calculated as N ' H ˆ W . To emphasize the contribution of each method while preserving naming simplicity and consistency, we may modify the name of original name.

Scan strategies have become a critical area of focus for enhancing Mamba-based models. A fundamental challenge in CV lies in effectively transforming 2D image features into the 1D sequences required by Mamba models. Several recent surveys [1], [37]-[41] have reviewed existing scanning strategies. Nevertheless, these surveys have two significant limitations: (1) Being published earlier, they do not comprehensively cover all current scanning strategies. (2) Most existing surveys simply enumerate scanning techniques without providing a systematic classification framework. Although two surveys [1], [41] made initial attempts to categorize scanning strategies, their classification methods have become inadequate due to the rapid and continuous evolution of scanning techniques.

To overcome these limitations, this survey paper conducts a comprehensive review of all existing scanning strategies within the remote sensing domain and introduces an innovative, unified classification framework applicable to all current scanning methods. Specifically, we propose a detailed classification framework comprising five key components: feature preprocessing methods , scan sampling methods , scan direction methods , scan pattern methods , and post-processing methods . These components can be employed either sequentially or concurrently. Fig. 3 provides a visual representation of this scanning pipeline, summarizing existing methods within remote sensing and presenting an illustrative workflow. In total, 44 scanning strategies have been identified and systematically categorized in Tab. I. It is important to note that all of these methods are designed for single-modality images. For multimodality images, an additional feature preprocessing method and four feature post-processing methods are available, which are elaborated in Sections IV-C2 and IV-C3, respectively, and visually represented in Fig. 10.

Here, we define image height as H , image width as W , and latent dimension (or spectral dimension for hyperspectral and

1) Feature Preprocessing : Feature preprocessing is distinct from conventional feature extraction methods such as CNNs and ViTs. Instead of extracting features directly from input data, feature preprocessing focuses on modifying existing feature maps through various operations, including transformations, spectral exchanges, padding, and others. These preprocessing techniques can lead to significant differences in the output 1D sequences, thus impacting the effectiveness and performance of the overall scan strategies. In total, nine distinct preprocessing methods have been identified and classified into four categories.

a) Spectral-as-Tokens (SaT): In remote sensing, hyperspectral and multispectral data provide abundant spectral information. Improving spectral modeling capabilities can substantially enhance the performance of deep learning models. This paper refers to these approaches as Spectral-as-Tokens (SaT). The SaT methods enable SSMs to model extensive spectral dependencies, converting 2D or 3D features naturally into 1D sequential formats. The first variant, termed SaT-1 [36], [95], [109], [119], [165], [167], rearranges the spectral ( C ) and spatial ( H ˆ W ) dimensions to form a feature map comprising C tokens, each possessing an H ˆ W latent representation, as depicted in Fig. 4.A. This permutation naturally facilitates SSMs in effectively modeling spectral correlations. The second variant, called SaT-2 [120], employs multi-scale 3D NNs to generate feature representations. These features are subsequently averaged and partitioned into n intervals, each having m latent dimensions, as illustrated in Fig. 4.B.

b) Feature Map Shift between Blocks: While SaT methods operate within blocks (WB), this category focuses on the feature transformation between blocks (BB), enabling Mamba models to attend to varying feature regions across multiple contiguous blocks. MaIR [168] introduces the Stripe-shift Mechanism (SsM) , which alters the scanning region for its

Fig. 4: The seven feature preprocessing methods for scan strategy. For clarity, two additional preprocessing methods are visually shown in Fig. 5.

<!-- image -->

nested S-shape scan pattern to create diverse 1D sequences, as depicted in Fig. 4.C. Similarly, CDLamba [151] proposes the Window Shifting and Perception Mechanism (WSPM) , shifting feature windows similar to Swin-Transformer [169], thereby generating diverse window configurations as shown in Fig. 4.D. These techniques enhance the model's capacity to focus on distinct regions between blocks, significantly improving feature representation.

[36] and HTD-Mamba [166] adopt Multi-Scale Downsampling (MSD) , which utilizes strided CNNs for feature downsampling, thereby improving long-range modeling of multiscale features, as depicted in Fig. 5.B.

c) Feature Map Rearranging: Unlike Transformers [4], [170] processing tokens in parallel, Mamba performs sequential computation, meaning that the initial scan order and the starting token influences subsequent token interactions. This category aims to optimize spatial-spectral feature extraction through feature map rearrangement [113], [122], [150]. HSRMamba [122] introduces the Global Spectral Reordering Mechanism (GSRM) , which rearranges spectral features based on the average correlation matrix values between spectrum, ensuring that highly correlated pixels are positioned closer together, as illustrated in Fig. 4.E. This improves longrange spectral feature modeling. VMambaSCI [150] proposes Spectral Exchange (SE) , which reorders spectral features using a predefined sequence, as shown in Fig. 4.F. This enhances the long-range modeling of spectral features within the same spatial channel. DTAM [113] introduces Dynamic Token Argumentation (DTA) to enhance global feature learning. As depicted in Fig. 4.G, DTA consists of two routes: the left route activates object-related tokens while masking the others, and the right route activates contextual tokens while using random shuffling and Adaptive Instance Normalization (AdaIN) [171] for augmentation. This approach helps the model focus on meaningful object-related features while mitigating spectral variability caused by acquisition conditions.

d) Other Preprocessing Operations: Additional preprocessing techniques further aim to enhance Mamba's longrange modeling capabilities. ColorMamba [149] identifies that certain scan patterns (discussed in Section IV-B4) can disrupt spatial relationships. To address this, it introduces Learnable Padding (LP) , embedding learnable tokens around feature maps, illustrated in Fig. 5.A. These tokens enhance spatial representations and maintain sequential continuity. MSFMamba

Fig. 5: Two additional feature preprocessing methods for scan strategy.

<!-- image -->

2) Scan Sampling : Scan sampling refers to the process of sampling the original feature map into multiple nonoverlapping sub-feature maps. Most studies utilize Vanilla Sampling , which is essentially equivalent to not performing any sampling at all or represents a special case of other scan sampling methods, as illustrated in Fig. 6.A. Atrous Sampling [162], inspired by atrous CNNs [172], samples feature maps at an interval of atrous step s , resulting in s 2 sub-feature maps with dimensions of H s ˆ W s ˆ C . This method effectively enhances the receptive field, as demonstrated in Fig.6.B. Interval Channel Grouping Sampling (ICG) [167] selects feature maps at an interval of g along the C dimension, generating g sub-feature maps, each with dimensions H ˆ W ˆ C g . This method facilitates the construction of non-redundant global information within each group while reducing data dimensionality, thereby enhancing computational efficiency [167], as illustrated in Fig. 6.C. Window Sampling [107], [120], similar to the windowing mechanism in Swin-Transformer [169], partitions feature maps into several windows, each consisting of n ˆ m tokens. Consequently, it produces H n ˆ W m windows (sub-feature maps) with dimensions of n ˆ m ˆ C . This approach primarily focuses on modeling local features

| Bran                                                                           | Pre                                                                            | Scan                                                                           | Strategy                                                                       | Pattern                                                                        | Post                                                                           | Papers                                                                                       |
|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|
| Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                | Sampling Direction Vallian Scan-like Strategies                                              |
| -                                                                              | -                                                                              | -                                                                              | ÔpÑq                                                                           | Z                                                                              | -                                                                              | [87] [88] [78] [81] [89]-[106]                                                               |
| -                                                                              | -                                                                              | Window                                                                         | ÔpÑq                                                                           | Z                                                                              | -                                                                              | [107]                                                                                        |
| -                                                                              | -                                                                              | -                                                                              | ÖpÓq                                                                           | Hibert                                                                         | PC                                                                             | [108]                                                                                        |
| -                                                                              | WB: CIAM                                                                       | -                                                                              | ÔpÑq                                                                           | Z                                                                              | -                                                                              | [109]                                                                                        |
| Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                             | Bidirectional Scan-like Strategies                                                           |
| -                                                                              | -                                                                              | -                                                                              | ÔŒpÑq                                                                          | Z                                                                              | -                                                                              | [35], [82], [110]-[117]                                                                      |
| - -                                                                            | -                                                                              | -                                                                              | ÔŒpÑq                                                                          | Z                                                                              | PC                                                                             | [118] [112], [119]                                                                           |
| WB: WB:                                                                        | SaT-1 SaT-2                                                                    | - -                                                                            | ÐÑpÑq ÐÑpÑq                                                                    | Z Z                                                                            | - -                                                                            | [36], [95], [120]                                                                            |
| - -                                                                            | -                                                                              | -                                                                              | ÔpÑÓq                                                                          | Z                                                                              | -                                                                              | [79]                                                                                         |
| -                                                                              | -                                                                              | -                                                                              | ÔpÑÓq                                                                          | Local                                                                          | -                                                                              | [121]                                                                                        |
| -                                                                              | GSRM                                                                           | -                                                                              | ÔŒpÑq                                                                          | Z                                                                              | -                                                                              | [122]                                                                                        |
| WB:                                                                            | -                                                                              | -                                                                              | ÔŒpÑq                                                                          | Z                                                                              | ConN                                                                           | [24], [117], [123]                                                                           |
| - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                    | - SS2D Scan-like Strategies                                                                  |
| -                                                                              | -                                                                              | -                                                                              | ÔŒpÑÓq                                                                         | Z                                                                              | -                                                                              | [24]-[26], [28], [29] [31]-[35], [80], [82] [83], [96], [98], [102] [109], [110] [123]-[147] |
| - -                                                                            | -                                                                              | ICG                                                                            | ÔŒpÑÓq                                                                         | Z                                                                              | -                                                                              | [98]                                                                                         |
| -                                                                              | - -                                                                            | - -                                                                            | ÔŒpÑÓq                                                                         | S                                                                              | -                                                                              | [115], [148]                                                                                 |
| -                                                                              |                                                                                |                                                                                | ÔŒpÑÓq                                                                         | Local                                                                          | -                                                                              | [119]                                                                                        |
| -                                                                              | WB: LP                                                                         | -                                                                              | ÔŒpÑÓq                                                                         | Z                                                                              | -                                                                              | [149]                                                                                        |
| -                                                                              | WB: SE                                                                         | -                                                                              | ÔŒpÑÓq                                                                         | Z                                                                              | -                                                                              | [150]                                                                                        |
| -                                                                              | -                                                                              | -                                                                              | ÔŒpÑÓq                                                                         | Z                                                                              | ConN                                                                           | [123]                                                                                        |
| -                                                                              | -                                                                              | -                                                                              | ÔŒpÑÓq                                                                         | Z                                                                              | ConC                                                                           | [123], [138]                                                                                 |
| -                                                                              | - -                                                                            | - -                                                                            | ÔŒpÑÓq                                                                         | Z                                                                              | CConN                                                                          | [123], [138], [151] [126]                                                                    |
| ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                        | ÔŒpÑÓq Z E1DS Novel Scanning Strategies                                                      |
| -                                                                              | -                                                                              | -                                                                              | Center Token                                                                   | Helical                                                                        | -                                                                              | [152]                                                                                        |
| -                                                                              | -                                                                              | -                                                                              | Clockwise ÔpÑq                                                                 | Tree                                                                           | -                                                                              | [84]                                                                                         |
| -                                                                              | -                                                                              | -                                                                              | 12 Directions                                                                  | S                                                                              | -                                                                              | [153]                                                                                        |
| Para                                                                           | -                                                                              | -                                                                              | ÔŒpÑÓq                                                                         | Z Diagonal                                                                     | -                                                                              | [27], [154],                                                                                 |
| [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                           | [155] Two Paradigms of Omnidirectional Scan Strategy                                         |
| -                                                                              | -                                                                              | -                                                                              | ÔÕŒÖpÑÓq                                                                       | S                                                                              | -                                                                              | [156]                                                                                        |
| Para                                                                           | -                                                                              | -                                                                              | ÔŒpÑq                                                                          | Z                                                                              |                                                                                |                                                                                              |
| ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies | ÔpÑq Random - [109], [157], [158] Random Scan Strategy + Other Scan Strategies               |
| Para                                                                           | - WB: SaT-1                                                                    | - -                                                                            | ÔŒpÑq ÐpÑq                                                                     | Z Random                                                                       | -                                                                              | [109]                                                                                        |
| Para                                                                           | -                                                                              | -                                                                              | ÔŒpÑÓq ÔpÑq                                                                    | Z Random                                                                       | -                                                                              | [159]                                                                                        |
| Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                 | Multi-Router(Sampling/Directions/Patterns) Scanning Strategies                               |
| Para                                                                           | -                                                                              | -                                                                              | ÔpÑÓq                                                                          | Z S                                                                            | -                                                                              | [160]                                                                                        |
| Para                                                                           | -                                                                              | -                                                                              | ÔpÑq                                                                           | Z S Diagonal                                                                   | -                                                                              | [161]                                                                                        |
| Para                                                                           | -                                                                              | -                                                                              | ÔŒpÑq                                                                          | Z Diagonal                                                                     | -                                                                              | [121]                                                                                        |
| Para                                                                           | -                                                                              | Atrous                                                                         | ÔpÑq                                                                           | Z                                                                              |                                                                                |                                                                                              |
| ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                     | ÔpÓq Z - [162] Sequential Multi-Router Scanning Strategies                                   |
| Seq                                                                            | -                                                                              | -                                                                              | ÔpÑq ŒpÑq                                                                      | Z Z                                                                            | -                                                                              | [163]                                                                                        |
| Seq                                                                            | -                                                                              | -                                                                              | ÔÖpÑÓq ÕŒpÑÓq                                                                  | Z S                                                                            | -                                                                              | [164]                                                                                        |
| Seq                                                                            | -                                                                              | Window -                                                                       | ÔpÑq ÔpÑq                                                                      | S S                                                                            | -                                                                              | [120]                                                                                        |
| Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                 | Novel Preprocessing Strategies                                                               |
| Para                                                                           | - WB: SaT-1                                                                    | - -                                                                            | ÔŒpÑq ÐÑpÑq                                                                    | S S                                                                            | -                                                                              | [165]                                                                                        |
| Para                                                                           | WB: MSD 1                                                                      | -                                                                              | ÔpÑÓq                                                                          | S                                                                              |                                                                                |                                                                                              |
|                                                                                | WB: MSD 2 4                                                                    | -                                                                              | ŒpÑÓq                                                                          | S                                                                              | -                                                                              | [36]                                                                                         |
| -                                                                              | WB: MSD                                                                        | - -                                                                            | ÔpÑq ÔŒpÑq                                                                     | S Z                                                                            | - -                                                                            | [166]                                                                                        |
| - Seq WB:                                                                      | WB: DTA WB: CG SaT-1+CG                                                        | - -                                                                            | ÔpÑq ÐpÑq                                                                      | S S Nested                                                                     | -                                                                              | [113] [167]                                                                                  |
| -                                                                              | BB: SsM                                                                        | -                                                                              | ÔŒpÑÓq                                                                         |                                                                                | -                                                                              | [168]                                                                                        |
| -                                                                              | BB: WSM                                                                        | -                                                                              | ÔpÑq                                                                           | S-Shape AR-Shape                                                               | -                                                                              | [151]                                                                                        |

TABLE I: This table summarizes a total of 44 different scan strategies utilized in the field of remote sensing. All acronyms are clearly illustrated in Fig. 4-10. The terms WB and BB denote 'Within Block' and 'Between Block', respectively, and are discussed in detail in Section IV-B1b. Arrows in the direction column indicate the starting token and initial scan directions, which are further explained in Section IV-B3.

within individual windows, as depicted in Fig. 6.D.

Fig. 6: The four scam sampling for scan strategy.

<!-- image -->

3) Scan Directions : Scan directions determine the starting token (typically a corner token or the center token) and the initial scanning trajectory (e.g., vertical, horizontal, clockwise, or counterclockwise) across all sub-feature maps. Since transforming 2D feature maps into 1D sequences inevitably disrupts spatial relationships, employing multiple scan directions helps improve the sequence model's spatial understanding [22], [23].

Fig. 7: The 11 scam directions for scan strategy.

<!-- image -->

A scanning strategy can utilize d different scan directions, resulting in d times the number of original sub-feature maps. While scan sampling segments a feature map into multiple sub-feature maps without additional computational cost, increasing scan directions effectively replicates each sub-feature map d times, leading to O d p ˆ N q computational complexity.

As illustrated in Fig. 7.A and summarized in Table I, diagonal arrows indicate the starting corner token, such as Ô representing the top-left corner. Horizontal ( Ñ ) and vertical ( Ó ) arrows denote the initial scanning directions. For instance, Vision Mamba (Vim) [22] introduces a bi-directional scan, starting from the top-left and bottom-right tokens vertically, which is recorded as ÔŒpÑq . Bi-directional scan results in twice of the original feature maps. Similarly, VMamba [23] proposes the VSS method, initiating scanning from both the top-left and bottom-right tokens in both vertical and horizontal directions, recorded as ÔŒ pÑÓq . VSS method results in four times the original feature maps. A special case involves scanning from the center token in a clockwise direction [152], specifically designed to align with a designated helical scan

pattern (discussed in Section IV-B4). Our analysis of 2D feature maps reveals a total of 11 scanning directions.

This concept can be extended to 3D data, as illustrated in Fig. 7.B. In 3D feature maps, eight regular corner tokens are available as initial points for scanning, with three primary scanning directions: vertical (along the H dimension), horizontal (along the W dimension), and spectral (along the spectral dimension), yielding 3 ˆ 8 fundamental scanning paths. Incorporating additional starting points, such as the central token, and further directions like clockwise and counterclockwise scanning significantly expands the potential number of 3D scan directions. Within remote sensing contexts, SSUMamba [153] is currently the sole Mamba-based approach treating hyperspectral data in 3D, introducing the first 3D data scan strategy method, which employs 12 of these scanning directions, as illustrated in Fig. 7.B.

Similarly, certain preprocessing methods, such as SaT, can directly transform 2D feature maps into 1D sequences, bypassing some subsequent scan steps. For 1D sequences, scanning options include two starting tokens, namely the first token ( Ð ) and the last token ( Ñ ). In addition, the initial direction is from left to right if starting token is the first, and is from right to left if starting token is the last, which is recorded as Ñ .

- 4) Scan Patterns : After completing previous steps, the subsequent step involves employing suitable scan patterns to systematically convert the 2D feature map into 1D sequences. Various scan patterns provide unique benefits tailored to specific applications. In remote sensing, 10 primary scan patterns are commonly used, which are visually illustrated in Fig. 8.

Fig. 8: The ten scam pattern for scan strategy.

<!-- image -->

a) Vanilla Scan Pattern (Z-Shape): The Z-shape pattern [22], [23] is the most straightforward transformation, achieved by directly flattening the feature map into a 1D sequence, as shown in Fig. 8.A. Although straightforward and commonly employed, this pattern compromises spatial relationships in the original 2D feature map. In particular, transitions between the last token of each row and the first token of the subsequent row lack spatial continuity, possibly resulting in diminished semantic context.

b) Continuous Pattern (S-Shape): Introduced by PlainMamba [54], the S-shape pattern, depicted in Fig. 8.B, preserves spatial continuity by ensuring that scanning progresses smoothly across rows. This effectively resolves the spatial discontinuity issue characteristic of the Z-shape pattern, making it especially advantageous for high-resolution remote sensing data, where preserving spatial consistency significantly enhances performance.

- c) Diagonal Pattern: The diagonal scanning pattern, first adopted by VMambaIR [173], enhances spatial connectivity by following a diagonal trajectory, as illustrated in Fig. 8.C. This approach is particularly effective for capturing long-range dependencies in high-resolution remote sensing imagery.

d) Local Pattern: Local Mamba [52] employs a local pattern, first processing tokens within individual windows before moving to adjacent windows, as shown in Fig. 8.D. This method is advantageous for capturing fine-grained local features. However, a drawback is that it disrupts the continuity between different windows, which may limit its ability to model global structures.

- e) Hilbert Pattern: Hilbert scanning [108] utilizes the Hilbert curve, a space-filling trajectory that recursively subdivides the image while maintaining spatial coherence, as illustrated in Fig. 8.E. This pattern effectively preserves local adjacency relationships, enhancing the spatial consistency of the generated 1D sequence.
- f) Helical Pattern: The helical pattern [152] initiates scanning from the center token and proceeds outward in a clockwise manner, as depicted in Fig. 8.F. This method is particularly useful in remote sensing tasks where classification is based on the central pixel of a high-resolution patch. By prioritizing the central region, the helical pattern ensures more precise feature representation.
- g) Random Pattern: The random pattern [109], [157][159] disrupts sequential order by randomly rearranging tokens to enhance positional invariance, as shown in Fig. 8.G. This approach is often used in conjunction with other scan patterns to improve robustness against spatial transformations.
- h) Tree Pattern: The tree-based pattern [84] constructs a minimum spanning tree (MST) by eliminating edges with low cosine similarity, as demonstrated in Fig. 8.H. The 1D sequence is then generated through a Breadth-First Search (BFS) traversal of the tree. This method enhances global information extraction and generalization across different input structures.
- i) Nested S-Shape Pattern: The nested S-shape pattern, proposed by MaIR [168], integrates the principles of the Sshape and local patterns. It partitions the feature map into non-overlapping stripes and applies S-shape scanning within each stripe, as depicted in Fig. 8.I. Additionally, the shift-strip mechanism (discussed in Section IV-B1b) further refines this approach to enhance feature extraction.
- j) Attention-based Reshuffling Pattern (AR-Shape): The AR-Shape pattern, introduced by CD-Lamba [151], dynamically prioritizes scan order based on an attention mechanism, as shown in Fig. 8.J. A 4 ˆ 4 average pooling operation generates attention scores, which determine the order in which windows are merged and scanned. This approach preserves spatial continuity in semantically critical regions, such as roads or buildings within remote sensing imagery, thereby making it especially suitable for change detection (CD) applications [174].

5) Feature Post-Processing : Feature post-processing involves operations conducted after acquiring the 1D sequence.

In some cases, post-processing can function similarly to preprocessing, depending on the chosen feature preparation methods, scan sampling approaches, scan directions, and scanning patterns. Nevertheless, it is crucial to note that further operations may still be applied after generating the 1D sequences to enrich the diversity of scan strategies. For single-modality images, only one post-processing technique, Pyramid Concatenation (PC), introduced by MLMamba [118] and LDMNet [108], which concatenates multi-scale feature representations before feeding them into SSMs. Compared with MSD methods, PC inherently facilitates one SSM block in capturing long-range dependencies across features of various scales, thus improving the overall effectiveness of the model. Methods tailored for multimodal images will be detailed in Section IV-C3.

Fig. 9: One feature post-processing method for scan strategy.

<!-- image -->

C. Multi-Modal and Bi-Temporal Feature Interaction via Mamba

Numerous studies [24], [25], [35], [36], [80]-[82], [109], [111], [117], [123], [126], [151] have leveraged multimodal and bi-temporal data, such as conventional RGB images, LiDAR point clouds and infrared images, to enhance performance. The bi-temporal data refers two images captured at different time in the same location, normally for the Change Detection (CD) task.

A crucial aspect of multimodal and bi-temporal learning is ensuring effective feature interaction between multiple modalities and bitemporality. Traditional approaches employ various techniques, such as addition [25], [28], [32], [35], [103], [109], [121], [124], [136], [147], subtraction [87], [89], [135], [175], direct concatenation along the C dimension [27], [79], [80], [126], [147], [159], [175], CNN-based attention mechanisms [79], [80], [84], [88], [97], [127], [147], [156], [159], and cross-attention mechanism [89], [130], [135] to facilitate multimodal and bi-temporal feature interactions.

Some studies have explored the potential of Mamba for multimodal and bi-temporal feature interaction. In this paper, we provide a comprehensive summary of all methods employed for multimodal and bi-temporal feature interaction in the remote sensing domain. We categorize these methods into four main groups: (1) SSM formula-based methods [35], [36], [80], [82], [112], (2) Scan strategy-based feature preprocessing methods [109], [176], (3) Scan strategy-based feature postprocessing methods [24], [117], [123], [126], [151], and (4) Mamba's gated mechanism-based methods [111], [176].

- 1) SSM Formula-based Methods : Standard SSMs are originally designed for single-modal and single-temporal features. Several studies have attempted to integrate multimodal and bi-temporal feature interactions within SSM formula, as discussed in Section IV-A2 and illustrated in Fig. 2.b.
- 2) Scan Strategy-based Feature Preprocessing Methods : Following the pre-processing methods of scan strategy framework introduced in Section IV-B1, the Channel Interaction Alignment Module (CIAM) proposed in CMS2I-Mamba [109] can be categorized as a scan strategy's feature preprocessing method, as depicted in Fig. 10.1. CIAM splits each modality features into two separate pathways, employing Former-Last and Odd-Even concatenation strategies. This enables Mamba blocks to model features encompassing both modalities. Similarly, Pan-Mamba [176] only adopts the Former-Last strategy of CIAM to preprocess multimodal and bi-temporal features.
- 3) Scan Strategy-based Feature Post-Processing Methods : Similarly, following post-processing methods of the scan strategy framework in Section IV-B5, a total of four methods [24], [117], [123], [126], [151] fall under the category of scan strategy's feature post-processing-based methods. Unlike preprocessing-based methods, these approaches are applied after obtaining 1D sequences.

The first method, Concatenation along the N dimension ( ConN ) [24], [117], [123], directly concatenates multimodal and bi-temporal features along the N dimension, as illustrated in Fig. 10.2a. ConN allows hidden states from the first modality to propagate into the computation of the second modality, thereby enabling SSMs to model cross-modal dependencies. The second method, Cross Concatenation along the N dimension ( CConN ) [123], [151], employs an interleaved concatenation strategy (Fig. 10.2b). Unlike ConN, which facilitates unidirectional interaction (from the first modality to the second), CConN enables bidirectional interaction, allowing both modalities to influence each other simultaneously. The third method, Concatenation along the C dimension ( ConC ) [111], [123], merges multimodal and bi-temporal features along the C dimension (Fig. 10.2c). This approach allows SSMs to capture long-range dependencies across modalities in parallel rather than sequentially. The fourth method, Exchange 1D Sequences ( E1DS ), proposed in Mamba-Diffusion [126], enables an SSM to process identical orientations from both modalities simultaneously (Fig. 10.2d). E1DS ensures that the projection layer's weights are optimized for both modalities concurrently during training.

4) Mamba's Gated Mechanism-based Methods : Beyond the methods discussed above, the fourth category explores Mamba's gating mechanism to facilitate multimodal and bitemporal feature interaction. RSCaMa [111] presents the Spatial Difference-aware SSM (SD-SSM), which replaces the traditional single-modality input for computing the activation value Z with the difference between features from two modalities. This design enables the gating mechanism to explicitly model cross-modal interactions. Pan-Mamba [176] introduces Cross Modality SSM (CM-SSM), where multimodal and bitemporal features are first projected into a unified representation space. CM-SSM then applies gated operations to enhance the learning of complementary signals and mitigate

Fig. 10: Overview of multimodal and bi-temporal feature interaction methods. This figure illustrates the primary categories of multimodal and bi-temporal feature interaction approaches via Mamba, including (1) scan strategy's preprocessing-based methods, (2) scan strategy's post-processing-based methods, and (3) Mamba's gated mechanism-based methods.

<!-- image -->

redundancy, leading to more effective multimodal and bitemporal interaction.

## V. MACRO-ARCHITECTURE ADVANCEMENT

Building upon the micro-architecture advancements discussed previously, this section examines higher-level architectural designs. We systematically analyze 4 key architectural advancement: (1) Hybrid Architectures with CNNs/Transformers (Section V-A), exploring combinations of Mamba with CNNs and Transformers; (2) Substitution in Existing Frameworks (Section V-B), where Mamba blocks are placed in existing frameworks; (3) Learning Paradigms (Section V-C), investigating unsupervised, self-supervised and prompt learning paradigms; and (4) Frequency Domain Operations (Section V-D), examining Fast Fourier Transformer, 2D Discrete Cosine Transform and Wavelet Transform. Tab. II presents the outline and the representative work.

capability of Mamba with local feature extraction capability of CNN. Various approaches [24]-[26], [28], [29], [34], [79][82], [84], [87]-[89], [91], [93], [95], [97], [98], [100]-[104], [106]-[108], [116], [119]-[122], [124], [125], [127]-[130], [133], [134], [136]-[139], [142], [143], [145]-[150], [152][156], [158], [159], [161], [162], [175], [177], [178] have been proposed to achieve this hybridization, typically falling into sequential or parallel configurations-both within each stackable building block and across the overall architectural framework.

## A. Hybrid Architectures with CNNs/Transformer

1) Hybrid Architecture with CNNs : The integration of CNNs with Mamba aims to complement the global modeling a) Sequential or Parallel Integration within Basic Units: One common approach to integrate CNNs with Mamba is in a sequential manner, wherein CNN layers may be potentially combined with normalization layers and activation functions [34], [88], [95], [98], [124], [129], [130], [142], [143], [159], [177]. Alternatively, CNNs can be arranged in parallel with Mamba within a basic unit, facilitating complementary feature extraction [87], [127], [137], [138], [142], [147], [154].

Beyond simple CNN-Mamba combinations, several studies incorporate multi-scale CNNs to enhance local feature extrac-

| CNN                                                  | Unit                                                 | Seq Para                                             | [124], [34], [129], [98], [159], [143] [88], [130], [142], [95], [177], [119], [103] [120], [155], [138], [132], [145], [119], [120] [155], [103], [159], [116], [122], [100], [154], [97]               |
|------------------------------------------------------|------------------------------------------------------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CNN                                                  | Unit                                                 | Seq Para                                             | [154], [87], [127], [142], [147], [137], [138]                                                                                                                                                           |
| CNN                                                  | Unit                                                 | Seq Para                                             | [28], [87], [127], [142], [147], [137], [138] [128], [93], [146], [148], [29], [81] [91], [178], [152], [128], [93], [146], [148]                                                                        |
| CNN                                                  | Unit                                                 | Seq Para                                             | [98], [175], [133], [104], [106] [139], [161], [28], [149]                                                                                                                                               |
| Trans- former                                        | Unit                                                 | Seq                                                  | [33], [34], [94], [179], [135]                                                                                                                                                                           |
| Trans- former                                        | Unit                                                 | Seq                                                  | [127], [130], [163]                                                                                                                                                                                      |
|                                                      | Arch                                                 | Seq Para                                             | [92], [93], [125], [148] [111], [140]                                                                                                                                                                    |
|                                                      | Arch                                                 | Seq Para                                             | [114]                                                                                                                                                                                                    |
| Mamba as Module                                      | Mamba as Module                                      | Mamba as Module                                      | [89], [24], [25], [82], [80], [151] [26], [134], [102]                                                                                                                                                   |
| Section V-B: Substitution                            | Section V-B: Substitution                            | Section V-B: Substitution                            | in Existing Framework                                                                                                                                                                                    |
| U-Net                                                |                                                      |                                                      | [28], [31], [123], [124], [33], [125] [34], [154], [127], [160], [128], [121] [142], [147], [143], [155], [180], [145], [163] [146], [153], [131], [149], [132], [181], [137] [138], [139], [161], [158] |
| YOLO                                                 | YOLO                                                 | YOLO                                                 | [26], [102], [115], [162]                                                                                                                                                                                |
| Diffusion                                            | Diffusion                                            | Diffusion                                            | [87], [148]                                                                                                                                                                                              |
| Section V-C: Learning Paradigm Unsupervised Learning | Section V-C: Learning Paradigm Unsupervised Learning | Section V-C: Learning Paradigm Unsupervised Learning | [146]                                                                                                                                                                                                    |
| Self-Learning                                        | Self-Learning                                        | Self-Learning                                        | [166], [141]                                                                                                                                                                                             |
| Prompt Learning                                      | Prompt Learning                                      | Prompt Learning                                      | [133]                                                                                                                                                                                                    |
|                                                      |                                                      |                                                      | Domain Operation                                                                                                                                                                                         |
| Section V-D: Frequency                               | Section V-D: Frequency                               | Section V-D: Frequency                               | [32], [130], [163], [103], [131], [150]                                                                                                                                                                  |
| Fourier Transform                                    | Fourier Transform                                    | Fourier Transform                                    |                                                                                                                                                                                                          |
| Wavelet Transform 2D Discrete                        | Wavelet Transform 2D Discrete                        | Wavelet Transform 2D Discrete                        | [121]                                                                                                                                                                                                    |
| Cosine Transform                                     | Cosine Transform                                     | Cosine Transform                                     |                                                                                                                                                                                                          |

TABLE II: Summary of macro-architecture advancements in remote sensing leveraging Mamba-based models, categorized into hybrid architectures with CNNs/Transformers, substitutions of established frameworks (U-Net [42], YOLO [43] and Diffusion Models [44]), learning paradigms, and frequency domain operations, along with corresponding representative studies.

tion in a sequential manner [103], [119], [120], [132], [138], [155]. Similarly, CNN-based attention mechanisms have been employed to refine feature representations when combined sequentially with Mamba [103], [116], [119], [120], [122], [155], [159].

In addition to these designs, other approaches incorporate specialized CNN variations to enhance performance. GraphMamba [100] integrates Graph CNNs (GCN) with Mamba sequentially, while UV-Mamba [154] utilizes Deformable CNNv4 (DCNv4) [182], [183] sequentially. SSRFN [97] employs 3D CNNs in parallel with Mamba to preserve spatial information. EGCM-UNet [145] enhances edge feature extrac- tion by incorporating CNNs with 3 ˆ 1 and 1 ˆ 3 kernels.

- b) Sequential or Parallel Integration at the Architectural Level: Beyond modifications at the unit level, hybrid architectures combining CNNs and Mamba have also been explored at the architecture level. For segmentation tasks, some architectures adopt a Mamba-based encoder paired with a CNN-based decoder [28], [87], [127], [137], [138], [142], [147], while others employ a CNN-based encoder with a Mamba-based decoder [93], [128], [146], [148]. For the non-encoder-decoder architectures, hybrid approaches often distribute CNNs and Mamba across different processing stages. Some models apply CNNs in the first several stages, followed by Mamba in later stages [29], [81], [91], [152], [178]. Conversely, others adopt the opposite configuration, employing Mamba in the early stages and CNNs in the later stages [93], [128], [146], [148].

Apart from such sequential structures, hybrid architectures utilizing parallel structures have been proposed to simultaneously capture both local and global features. Some models establish two separate branches-one Mamba-based and the other CNN-based-before fusing the extracted global and local features [98], [104], [106], [133], [139], [161], [175]. RS3Mamba [28] and ColorMamba [149] introduce Mambabased auxiliary encoders to provide global information to CNN-based main encoders, facilitating feature fusion at each processing stage.

- 2) Hybrid Architecture with Transformers : Efforts to enhance Mamba's sequential global modeling with Transformer's parallel global modeling have led to various TransformerMamba hybrid architectures. Similar to the CNN-Mamba hybrid approaches, these Transformer-Mamba architectures can be categorized into sequential or parallel structures within each stackable basic unit and at the architectural level.
- a) Sequential or Parallel Integration within Basic Units: Some studies simply integrate vanilla Transformer blocks with Mamba sequentially [33], [34], [94], [135], [179]. In addition to these sequential integration methods, SDMSPan [135] employs cross-attention to facilitate multimodal and bitemporal feature interaction at the beginning of each basic unit.

Furthermore, MFMamba [127] and MambaFormerSR [130] integrate vanilla Transformer blocks alongside Mamba in a parallel configuration. TransMamba [163] improves spectraldomain representation by incorporating a Transformer block augmented with Fourier transform operations.

- b) Sequential or Parallel Integration at the Architectural Level: At the architectural level, MamTrans [114] is the only work that integrates Transformer and Mamba in parallel, whereas other studies adopt a sequential design. MSTFNet [92] places Mamba blocks in the early stages and SwinTransformer [169] in the later stages. PyramidMamba [93], UNetMamba [125], and MaDiNet [148] utilize vanilla Transformer, efficient Transformer [184], and agent Transformer [185] encoders, respectively, paired with Mamba-based decoders. RSCaMa [111] and Mamba-MDRNet [140] initially extract features using a Mamba-based backbone, which are then fed into a Transformer-based language model to produce text captions and predict classification outputs.

3) Mamba as a Module : Beyond serving as a backbone, Mamba has also been employed as a module to achieve specific objectives. Many studies [24], [25], [80], [82], [89], [151] utilize Mamba-based modules to facilitate multi-modal, bitemporal, multi-scale, and multi-stage feature interactions and fusion. In particular, HSDet-Mamba [26] and Mamba-UAVSegNet [134] incorporate Mamba-based modules to enhance global multi-scale feature representation. Additionally, ES-HSFPN [102] employs Mamba to aggregate global features with local object features, further improving feature integration.

## B. Substitution in Existing Frameworks

1) Substitution in U-Net : U-Net [42] is a symmetric encoder-decoder architecture extensively utilized in image segmentation. It is distinguished by its skip connections, which integrate shallow spatial details with deep semantic features across hierarchical scales, thereby facilitating precise localization and multi-scale contextual modeling. Due to the high efficacy of the U-Net framework, several studies [31], [33], [34], [125], [131], [132], [137], [139], [142], [143], [146], [153]-[155], [160], [161], [180], [181] have replaced the conventional CNN-based or Transformer-based blocks with Mamba-based blocks to enhance feature representation and computational efficiency. Furthermore, some approaches [123], [124], [147], [163] have introduced a Siamese encoder structure to effectively process multi-modal and bi-temporal images.

Building upon the original U-Net framework, some studies [28], [127], [145], [149] have designed auxiliary branches to augment feature extraction capabilities. For instance, RS3Mamba [28] incorporates a Mamba-based auxiliary encoder to supplement global information extracted by a CNNbased encoder, thereby enhancing local feature representation. EGCM-UNet [145] introduces an auxiliary branch that extracts edge features, which are subsequently fused at each stage of the encoder-decoder pipeline. ColorMamba [149] integrates a Mamba-based HSV color prediction sub-network, leveraging HSV color space priors to provide multi-scale guidance for RGB reconstruction. Similarly, MFMamba [127] utilizes a Mamba-based auxiliary encoder to extract global features from digital surface model (DSM) images, which are then fused with local features from RGB images, leading to improved overall performance.

Additionally, several studies have sought to refine feature representation within skip connections. Traditional approaches include leveraging multi-scale CNNs and attention mechanisms [128], introducing frequency domain terms [121], and incorporating self-attention mechanisms [158]. Beyond these methods, Mamba-based blocks have been employed to enhance skip connections. LCCDMamba [138] first extracts local features using multi-scale CNNs and subsequently applies a Mamba-based block for long-range feature modeling.

- 2) Substitution in YOLO : To enhance object detection performance, several studies have explored the integration of Mamba-based modules into the YOLO [43] framework. These modifications primarily target the backbone and feature fusion components of YOLOv8 [186] to improve global feature

extraction while maintaining computational efficiency. HSDetMamba [26] integrates Mamba into the YOLO architecture by replacing conventional feature extraction modules with a spatial feature enhancement module (SFEM). This module fuses CNN-based feature extraction with Mamba to better capture both spatial and spectral dependencies, leading to improved detection accuracy in hyperspectral imagery. ES-HSFPN [102] replaces the Spatial Pyramid Pooling-Fast (SPPF) module of YOLOv8 with an SPPF-Mamba module, which enhances global and local feature fusion, thereby improving object-context representation. In YOLO-Mamba [115], the C2f module in YOLOv8 is replaced with a C2f-Mamba module, aiming at leveraging a Mamba-based attention mechanism to capture long-range dependencies across feature and spatial dimensions, reducing redundant information and improving the detection of small or occluded objects in infrared aerial imagery. The resulting YOLO-Mamba model achieves higher detection accuracy while maintaining minimal computational overhead. HRMamba-YOLO [162] integrates Mamba into the YOLO architecture, which enhances feature extraction and multi-scale feature fusion by capturing long-range dependencies and improving contextual information representation. The incorporation of Mamba-based modules within the highresolution feature pyramid network further strengthens crossscale feature interactions, leading to improved small object detection performance in UAV imagery.

3) Substitution in Diffusion Model : Several studies have explored the integration of Mamba-based modules into diffusion models [44] to enhance feature extraction and representation learning in remote sensing tasks. In particular, MaDiNet [148] replaces conventional CNN-based feature extraction in diffusion-based SAR target detection with the MambaSAR module, which captures rich spatial structural information and improves target differentiation from complex backgrounds. This integration allows MaDiNet to enhance global contextual understanding while leveraging the denoising process of the diffusion model to refine target localization. IMDCD [87] incorporates Mamba into the diffusion model through the SwinMamba-Encoder (SME) and the Variable State Space Change Detection (VSS-CD) module. SME enhances long-range dependency modeling, while VSS-CD extracts transformationaware features, which are iteratively refined within the diffusion process to generate high-precision change detection maps. The diffusion model provides a generative framework for iterative refinement, ensuring robustness against noise and improving detection accuracy.

## C. Learning Paradigm

This subsection provides an overview of various learning paradigms applied in remote sensing, with emphasis on unsupervised, self-supervised and prompt learning paradigms. Except for these paradigms, the self-supervised learning is used by the rest of the work.

1) Unsupervised Learning : In the reviewed study, only RFCC [146] employs an unsupervised learning approach for remote sensing image change detection. The method integrates Mamba-based differentiable feature clustering to perform automatic segmentation, ensuring that spatially contiguous pixels

with similar spectral and spatial features are grouped together. To further enhance classification accuracy, the framework incorporates fuzzy C-means clustering, which decomposes mixed pixels into multiple signal classes, and a contextsensitive Bayesian network (CSBN) [187], which refines posterior probability estimations by incorporating spatial information. This combination reduces the cumulative clustering error and eliminates the reliance on manual annotations.

2) Self-Supervised Learning : In the reviewed studies, two innovative frameworks utilizing Mamba exhibit distinct selfsupervised strategies. In particular, SatMamba [141] integrates masked autoencoders (MAEs) [188] with Mamba for foundation model pretraining. Inspired by MAE [188] and SatMAE [189], the framework randomly masks 75% of image patches during pretraining on the fMoW dataset [190] and reconstructs normalized pixel values of masked regions through a Mambabased encoder-decoder structure. This approach eliminates dependency on labeled data while capturing spatial-spectral dependencies through linear-complexity SSM blocks. Notably, positional encodings are experimentally ablated, revealing that Mamba's inherent sequential processing effectively preserves spatial order without explicit positional guidance. HTDMamba [166] introduces spectrally contrastive learning for hyperspectral target detection. It employs a spatial-encoded spectral augmentation technique to generate augmented views by aggregating contextual pixels within patches, weighted by spectral similarity. These views form positive/negative pairs for contrastive loss optimization, enabling discrimination between target and background spectra without manual annotations.

- 3) Prompt Learning : Prompt-Mamba [133] introduces an interactive prompt-based segmentation framework for urban flood detection, leveraging four distinct types of prompts (i.e., points, boxes, curves, and masks), to guide the model in refining segmentation results. The method employs a convolutional prompt encoder that transforms prompt inputs into structured feature representations, enabling efficient integration with image embeddings. By incorporating expert knowledge through interactive prompts, the approach effectively reduces annotation costs while maintaining segmentation accuracy.

## D. Frequency Domain Operation

Several studies incorporate frequency-domain operations, namely Fast Fourier Transform (FFT), wavelet transform and 2D discrete cosine transform, to enrich spatial-spectral features with complementary frequency information. In total, seven papers [32], [103], [130], [131], [150], [163] integrate FFT, while one paper [99] employs wavelet transformation and one paper [121] uses 2D discrete cosine transform.

1) Fast Fourier Transform : FreMamba [32] is the first to merge Mamba with Fourier transformation within its basic unit, adding an FFT-based branch sequentially to both the Mamba-based and CNN-based blocks. This design facilitates the exploration of spatial and frequency correlations. Similarly, FMambaIR [131] applies an attention mechanism separately to the amplitude and phase obtained via FFT, alongside an additional Mamba branch, thereby enhancing the extraction of global degradation features and overall global information perception. MambaFormerSR [130] introduces an FFT-based attention mechanism into the conventional FeedForward Network (FFN). By multiplying the FFT-enhanced branch (processed through CNNs and FFT) with a branch solely processed by CNNs, the network is guided to emphasize degradation-sensitive components during image restoration. In TransMamba [163], a dual-branch architecture comprised Transformer and Mamba modules is proposed. Within the Transformer branch, a self-attention module applies FFT to reallocate features into distinct frequency bands. Learnable attention weights then adaptively suppress low-frequency rain streaks while amplifying high-frequency textures, and a Spectral Enhanced Feed-Forward (SEFF) module further refines features with frequency-specific filters and dilated convolutions.

In addition to integration at the basic unit level, two other studies incorporate FFT in different network components. CSMN [103] proposes a Cross-Domain Mamba Module (CDMM) that fuses spatial, spectral, and frequency data by applying a Fourier transform after convolutional feature fusion. This branch captures global frequency patterns, contributing to improved detail preservation and robust spatial-spectral fusion in pan-sharpening tasks. Furthermore, VmambaSCI [150] integrates FFT within multi-stage interactions to reinforce feature fusion across different stages.

- 2) 2D Discrete Cosine Transform : CVMH-UNet [121] employs a Multi-Frequency Multi-Scale Feature Fusion Block (MFMSBlock) that uses a 2D Discrete Cosine Transform (DCT) to compute channel attention. This allows the model to better capture both low-frequency structural information and high-frequency edge details, mitigating information loss in skip connections and thereby enhancing segmentation accuracy. This approach not only enhances feature fusion across multiple scales but also addresses the issue of information inconsistency in conventional U-Net skip connections.
- 3) Wavelet Transform : WaveMamba [99] integrates wavelet transform with a Mamba-based spatial-spectral network by employing the classical Haar wavelet to decompose hyperspectral data into multiple subbands. This approach separates input spatial and spectral features into low- and highfrequency components, effectively capturing both fine-grained details and global structures, and subsequently feeds the multiresolution subband features into a SSM to boost classification performance.

## VI. DOWNSTREAM APPLICATIONS

In this section, we introduce six benchmarks involving models based on CNN, Transformer, and Mamba architectures, across various downstream tasks. To provide a more comprehensive comparison and demonstration, we have selected tasks that include both high-level vision tasks (i.e., image classification (Tab. III), segmentation (Tab. IV), change detection (Tab. V, object detection (Tab. VI) and low-level vision tasks (i.e., super-resolution (Tab. VII), image restoration (Tab. VIII)).

Results on Image Classification Task: Tab. III shows the classification performance among different models on the

| Method                    | Rate                      | OA                        | AA                        | Kappa                     |
|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         |
| 1D-CNN [191]              | 5.00%                     | 85.82                     | 83.25                     | 81.17                     |
| 2D-CNN [192]              | 5.00%                     | 93.30                     | 89.49                     | 91.07                     |
| 3D-CNN [193]              | 5.00%                     | 93.52                     | 91.22                     | 91.37                     |
| 1D-CNN [194]              | 9.90%                     | 75.50                     | 86.26                     | 69.48                     |
| 2D-CNN [195]              | 9.90%                     | 86.05                     | 88.99                     | 81.87                     |
| Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods |
| HIS-BERT [196]            | 5.00%                     | 85.45                     | 71.41                     | 83.36                     |
| GSC-ViT [197]             | 5.00%                     | 98.28                     | 94.42                     | 98.04                     |
| CASST [198]               | 5.00%                     | 96.65                     | 92.25                     | 96.18                     |
| ViT [199]                 | 9.90%                     | 76.99                     | 80.22                     | 70.10                     |
| SpectralFormer [200]      | 9.90%                     | 91.07                     | 90.20                     | 88.05                     |
| SSFTT [201]               | 9.90%                     | 92.61                     | 93.37                     | 90.29                     |
| Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       |
| LS2SM [177]               | 0.10%                     | 98.83                     | 98.18                     | 98.45                     |
| SDMamba [106]             | 0.21%                     | 98.50                     | 95.89                     | 98.01                     |
| STMamba [152]             | 0.50%                     | 97.03                     | 94.27                     | 96.05                     |
| MambaHSI [105]            | 0.63%                     | 95.74                     | 95.86                     | 95.00                     |
| SSUM [144]                | 1.00%                     | 96.15                     | 95.42                     | 94.91                     |
| LE-Mamba [119]            | 2.00%                     | 99.63                     | 99.43                     | 99.51                     |
| IGroupSS-Mamba [167]      | 5.00%                     | 99.75                     | 99.46                     | 99.66                     |
| DBMamba [178]             | 5.00%                     | 99.40                     | 98.99                     | 99.21                     |
| 3DSS-Mamb [165]           | 5.00%                     | 98.48                     | 97.56                     | 97.98                     |
| MiM [156]                 | 8.90%                     | 91.58                     | 92.76                     | 89.83                     |
| MamTrans [114]            | 8.90%                     | 96.30                     | 95.83                     | 95.02                     |
| MambaLG [120]             | 8.92%                     | 95.66                     | 95.91                     | 94.19                     |
| S2Mamba [112]             | 8.93%                     | 97.81                     | 97.14                     | 97.05                     |
| HSIRMamba [104]           | 9.90%                     | 99.20                     | 99.21                     | 98.95                     |
| HSIMamba [116]            | 9.90%                     | 98.08                     | 97.87                     | 97.41                     |
| MorpMamba [94]            | 20.00%                    | 97.67                     | 96.93                     | 96.91                     |
| WaveMamba [99]            | 25.00%                    | 98.63                     | 97.70                     | 98.19                     |
| MHSSMamba [179]           | 25.00%                    | 96.41                     | 97.62                     | 96.85                     |
| SS-Mamba [96]             | 49.89%                    | 96.40                     | 98.43                     | 95.31                     |
| DTAM [113]                | 50.00%                    | 84.03                     | -                         | 81.62                     |

TABLE III: The classification benchmark for the Pavia University Dataset [202]. Rate represents the percentage of the training data to the total data; OA represents overall classification accuracy; AA represents average classification accuracy; Kappa represents the kappa coefficient.

Pavia University Dataset [202]. As we can see, although the Rate parameters set by each method vary, when the same Rate (5.00%) is adopted, the IGroupSS-Mamba [167] achieves optimal performance in parameters such as OA, AA, Kappa, and surpasses all other CNN-based and Transformer-based models.

Results on Image Segmentation Task: Tab. IV presents the comparison performance among different models on the ISPRS Vaihingen dataset [213]. Overall, Transformer-based methods tend to outperform CNN-based methods, but Mambabased methods generally surpass Transformer-based methods in most cases. Specifically, PPMamba [142] achieves the best performance on the mF1 and mIOU metrics, while [128] excels in the OA metric among all methods reporting this metric.

Results on Image Change Detection Task: Tab. V shows the performance comparisons among different models on the WHU-CD dataset [224]. Similarly, Mamba-based models achieve better performance than CNN-based and Transformerbased models. Specifically, DC-Mamba [124] delivers the best performance in the F1 metric, TTMGNet [84] excels

TABLE IV: The segmentation benchmark for the ISPRS Vaihingen dataset [213]. mF1 represents mean F1 score; mIOU represents mean intersection over union; OA represents overall accuracy.

| Methods                   | mF1                       | mIOU                      | OA                        |
|---------------------------|---------------------------|---------------------------|---------------------------|
| CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         |
| DANet [203]               | 79.60                     | 69.40                     | 88.20                     |
| ABCNet [204]              | 89.50                     | 81.30                     | 90.70                     |
| CMTFNet [205]             | 87.37                     | 78.06                     | -                         |
| BANet [206]               | 90.32                     | 82.45                     | 91.92                     |
| MANet [207]               | 90.68                     | 83.06                     | 92.28                     |
| Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods |
| HST-UNet [208]            | 86.62                     | 78.67                     | -                         |
| FTUNetformer [209]        | 91.30                     | 84.10                     | 91.60                     |
| DC-Swin [210]             | 90.71                     | 83.08                     | 92.30                     |
| UNetFormer [211]          | 90.59                     | 82.93                     | 92.21                     |
| TransUNet [212]           | 92.86                     | 87.15                     | 91.56                     |
| Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       |
| RS3Mamba [28]             | 90.34                     | 82.78                     | -                         |
| RTMamba [29]              | 91.08                     | 83.92                     | 91.30                     |
| Samba [90]                | 84.23                     | 73.56                     | -                         |
| UNetMamba [125]           | 90.95                     | 83.47                     | 92.51                     |
| MFMamba [127]             | 90.52                     | 83.13                     | 91.81                     |
| CM-Unet [128]             | 92.01                     | 85.48                     | 93.81                     |
| CVMH-UNet [121]           | 85.98                     | 75.97                     | 85.82                     |
| PPMamba [142]             | 91.32                     | 84.37                     | -                         |
| PyramidMamba [93]         | -                         | 83.10                     | -                         |
| PPMamba [155]             | 88.34                     | 79.60                     | -                         |
| UrbanSSF [158]            | 91.70                     | 85.00                     | 93.60                     |

TABLE V: The change detection benchmark for WHU-CD dataset [224].

| Methods                                                                                                                                                              | Precision                                                                                                                                                            | Recall                                                                                                                                                               | F1                                                                                                                                                                   | IoU                                                                                                                                                                  | OA                                                                                                                                                                   |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CNN-based Methods                                                                                                                                                    | CNN-based Methods                                                                                                                                                    | CNN-based Methods                                                                                                                                                    | CNN-based Methods                                                                                                                                                    | CNN-based Methods                                                                                                                                                    | CNN-based Methods                                                                                                                                                    |
| FC-EF [214]                                                                                                                                                          | 92.10                                                                                                                                                                | 90.64                                                                                                                                                                | 91.36                                                                                                                                                                | 84.10 81.37                                                                                                                                                          | 99.32 99.20                                                                                                                                                          |
| IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods | IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods | IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods | IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods | IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods | IFNet [215] 91.51 88.01 89.73 SNUNet [216] 84.70 89.73 87.14 77.22 DSIFN [217] 97.46 83.45 89.91 81.67 CGNet [218] 94.47 90.79 92.59 86.21 Transformer-based Methods |
| SwinUNet [219] BIT [220]                                                                                                                                             | 92.44                                                                                                                                                                | 87.56 91.95                                                                                                                                                          | 89.93 91.90                                                                                                                                                          | 81.71                                                                                                                                                                | 99.22 99.35                                                                                                                                                          |
| ChangeFormer [221]                                                                                                                                                   | 91.84 93.73 93.47                                                                                                                                                    | 87.11 89.16                                                                                                                                                          | 90.30 91.27                                                                                                                                                          | 85.01 82.32 83.94                                                                                                                                                    | 99.26                                                                                                                                                                |
| RS-Mamba [27]                                                                                                                                                        | 93.37                                                                                                                                                                | 90.42                                                                                                                                                                | 91.87 93.76                                                                                                                                                          | 84.96                                                                                                                                                                | -                                                                                                                                                                    |
| Mamba-based Methods                                                                                                                                                  | Mamba-based Methods                                                                                                                                                  | Mamba-based Methods                                                                                                                                                  | Mamba-based Methods                                                                                                                                                  | Mamba-based Methods                                                                                                                                                  | Mamba-based Methods                                                                                                                                                  |
| CDMamba [89]                                                                                                                                                         | 95.58                                                                                                                                                                | 92.01                                                                                                                                                                |                                                                                                                                                                      | 88.26                                                                                                                                                                |                                                                                                                                                                      |
| IMDCD [87]                                                                                                                                                           | 93.85                                                                                                                                                                | 93.27 92.23                                                                                                                                                          | 93.56                                                                                                                                                                | 88.39                                                                                                                                                                | 99.51 99.51                                                                                                                                                          |
| ChangeMamba [123] DC-Mamba [124]                                                                                                                                     | 96.18 -                                                                                                                                                              | 94.33                                                                                                                                                                | 94.19 95.22                                                                                                                                                          | 89.02 90.87                                                                                                                                                          | 99.58 99.48                                                                                                                                                          |
| TTMGNet [84]                                                                                                                                                         |                                                                                                                                                                      | 89.74                                                                                                                                                                | 90.94                                                                                                                                                                | 91.25                                                                                                                                                                | 99.15                                                                                                                                                                |
| LCCDMamba                                                                                                                                                            | 92.18                                                                                                                                                                |                                                                                                                                                                      |                                                                                                                                                                      |                                                                                                                                                                      |                                                                                                                                                                      |
|                                                                                                                                                                      |                                                                                                                                                                      | 94.96                                                                                                                                                                | 94.18                                                                                                                                                                | 89.00                                                                                                                                                                | 99.49                                                                                                                                                                |
| [138] CD-Lamba [151]                                                                                                                                                 | 93.41                                                                                                                                                                |                                                                                                                                                                      |                                                                                                                                                                      |                                                                                                                                                                      |                                                                                                                                                                      |
|                                                                                                                                                                      | 93.45                                                                                                                                                                | 91.59                                                                                                                                                                |                                                                                                                                                                      | 86.07                                                                                                                                                                |                                                                                                                                                                      |
|                                                                                                                                                                      |                                                                                                                                                                      |                                                                                                                                                                      | 92.51                                                                                                                                                                |                                                                                                                                                                      | 99.32                                                                                                                                                                |

in the IoU metric, and ChangeFormer [221] achieves the best performance in the OA metric. For more comprehensive comparison, it is recommended that the author refer to relevant survey paper [174].

Results on Image Object Detection Task: Tab. VI compares the performance of object detection tasks on the DroneVehicle [225] and VisDrone [226] datasets. Specifically, the Mamba-based model, ES-HS-FPN [102], outperforms both

TABLE VI: The object detection benchmark for DroneVehicle [225] and VisDrone [226] datasets. mAP represents mean average precision; mAP 0 5 . represents the mAP at an IoU threshold of 50%. M and Y represent Mamba and YOLO for short. : represents use of multimodal data, RGB and thermal infrared images.

| DroneVehicle [225]            | mAP 0 . 5                     | mAP                           | VisDrone [226]                | mAP                           |
|-------------------------------|-------------------------------|-------------------------------|-------------------------------|-------------------------------|
| Transformer/CNN-Based Methods | Transformer/CNN-Based Methods | Transformer/CNN-Based Methods | Transformer/CNN-Based Methods | Transformer/CNN-Based Methods |
| MKD [227]                     | -                             | 69.0                          | YOLOv10 [228]                 | 41.3                          |
| GHOST [229]                   | 81.5                          | 59.3                          | Gold-YOLO [230]               | 41.0                          |
| GM-DETR [231]                 | 80.8                          | 55.9                          | DREN [232]                    | 30.3                          |
| CMADet [233]                  | 82.0                          | 59.5                          | GLSAN [234]                   | 32.5                          |
| TSFADet : [235]               | -                             | 73.9                          | TPH-YOLOv5 [236]              | 38.9                          |
| C2Former : [237]              | -                             | 74.2                          | FFCA-YOLO [238]               | 41.2                          |
| Mamba-Based Methods           | Mamba-Based Methods           | Mamba-Based Methods           | Mamba-Based Methods           | Mamba-Based Methods           |
| DMM(R-CNN) [24]               | 77.2                          | -                             | ES-HS-FPN [102]               | 43.5                          |
| DMM(S2ANet) [24]              | 79.4                          | -                             | HRMamba- Y [162]              | 38.9                          |
| COMO( Y v5s) [80]             | 85.3                          | 63.4                          | Mamba- Y [162]                | 41.9                          |
| COMO( Y v8s) [80]             | 86.1                          | 65.5                          |                               |                               |
| RemoteDet- M : [25]           | -                             | 81.8                          |                               |                               |
| MGMF : [136]                  | 80.3                          | -                             |                               |                               |

TABLE VII: The super-resolution benchmark for Chikusei [239] and AID [240] datasets (scale factor is 4 ˆ ). PSNR represents peak signal-to-noise ratio and SSIM represents structural similarity index. The data on Chikusei [239] are preprocessed differently so that the performances of HSRMamba [122] and MambaIR [249] are better.

| Chikusei [239]            | PSNR                      | SSIM                      | AID [240]                 | PSNR                      | SSIM                      |
|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
| CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         |
| SSR-NET [241]             | 25.37                     | -                         | FENet [242]               | 29.16                     | 0.7812                    |
| Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods |
| MSST-Net [243]            | 22.66                     | -                         | HAT-L [244]               | 30.81                     | 0.8124                    |
| UHNTC [245]               | 25.76                     | -                         | RGT [246]                 | 30.91                     | 0.8159                    |
| MIMO-SST [247]            | 28.78                     | -                         | TransENet [248]           | 30.80                     | 0.8109                    |
| Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       |
| UVMSR [143]               | 28.12                     | 0.9642                    | FreMamba [32]             | 31.07                     | 0.8185                    |
| SSRFN [97]                | 29.86                     | -                         | MambaFormerSR [130]       | 29.35                     | 0.7870                    |
| FusionMamba [28]          | 27.61                     | -                         | MambaIR [249]             | 30.85                     | 0.8130                    |
| HSRMamba [122]            | 40.28                     | 0.9441                    |                           |                           |                           |
| MambaIR [249]             | 39.48                     | 0.9353                    |                           |                           |                           |

CNN-based and Transformer-based models, achieving the highest mAP value.

Results on Image Super-Resolution Task: Table VII presents a comparison of image super-resolution task performance across the Chikusei [239] and AID [240] datasets. In particular, FreMamba [32] records the highest values in both PSNR and SSIM metrics on the AID dataset.

Results on Image Restoration Task: Tab. VIII presents a performance comparison of image restoration tasks on the SateHaze1k [258] and UAV-Rain [259] datasets. Notably, RSDehamba [31] sets a new state-of-the-art benchmark for the tasks on the SateHaze1k dataset in both PSNR and SSIM metrics.

## VII. CHALLENGES AND FUTURE DIRECTIONS

This section examines the principal challenges encountered when applying Mamba to remote sensing and outlines several promising directions for future research to enhance its performance in this domain.

## A. Causality

The S6 block in Mamba operates as a causal system, wherein predictions for each token are generated based solely

TABLE VIII: The image restoration benchmark for SateHaze1k [258] and UAV-Rain [259].

| Methods                   | SateHaze1k                | SateHaze1k                | Methods                   | UAV-Rain                  | UAV-Rain                  |
|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|---------------------------|
|                           | PSNR                      | SSIM                      | Methods                   | PSNR                      | SSIM                      |
| CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         | CNN-based Methods         |
| FFA-Net [250]             | 22.87                     | 0.8965                    | RCDNet [251]              | 22.48                     | 0.8753                    |
| M2SCN [252]               | 24.22                     | 0.8960                    | SPDNet [253]              | 24.78                     | 0.9054                    |
| Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods | Transformer-based Methods |
| Restormer [254]           | 24.34                     | 0.9021                    | IDT [255]                 | 22.47                     | 0.9054                    |
| RSDformer [256]           | 24.30                     | 0.9071                    | DRSformer [257]           | 24.93                     | 0.9155                    |
| Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       | Mamba-based Methods       |
| RSDehamba [31]            | 25.91                     | 0.9157                    | LightMamba [160]          | 25.56                     | 0.9042                    |
| LightMamba [160]          | 25.80                     | 0.9148                    | Weamba [161]              | 25.25                     | 0.9080                    |
| FMambaIR [131]            | 24.35                     | 0.9003                    |                           |                           |                           |
| MambaIR [249]             | 24.50                     | 0.9093                    |                           |                           |                           |
| Weamba [161]              | 25.55                     | 0.9151                    |                           |                           |                           |

on the current input and a hidden state summarizing preceding information. This architecture is inherently optimized for one-dimensional sequential data rather than two-dimensional image. Although patch embedding techniques and flattening allow to transform 2D data into 1D sequences, this process inevitably results in the loss of spatial information due to the causal nature of the system, thereby compromising performance.

One strategy to alleviate this issue involves employing more effective scanning strategies. As detailed in Section IV-B, several scan methods have been proposed to partially overcome causal constraints and reduce spatial information loss. Nevertheless, these methods do not entirely resolve the fundamental limitations. An alternative approach is to modify the core formulation of the SSM itself. For instance, VSSD [72] incorporates a backbone architecture specifically designed for non-causal data processing, whereas TTMGNet [84] adjusts its hidden state calculations to match the employed scanning strategy, unintentionally shifting towards a noncausal framework. Developing an SSM explicitly for handling non-causal data remains an attractive and relatively unexplored area of research.

## B. Novel SSM Formulations

As discussed in Section IV-A, several studies have proposed modifications to the SSM formulation to achieve specific objectives. Nevertheless, this research area remains in its early stages, with considerable scope for further improvement. Developing an SSM formulation that is optimally suited to the characteristics of remote sensing imagery is both critical and promising for future studies.

## C. Multi-Modal Interaction via Mamba

Employing multi-modal and bi-temporal data in remote sensing is prevalent. While certain studies discussed in Section IV-C have explored multimodal and bi-temporal data interaction using Mamba, further efforts are necessary to develop more efficient and interpretable interaction architectures based on Mamba. Advancements in this direction could significantly enhance the performance of multimodal and bi-temporal remote sensing tasks.

## D. 3D Scan Data Processing

For data rich in spectral information, such as hyperspectral image, an intriguing approach is to consider these data as 3D data. Within the Mamba framework, only SSUMamba [153] currently leverages a 3D scan strategy to process such data. Further exploration of 3D scanning techniques to capture spatial-spectral relationships represents an unexplored but promising direction in remote sensing research.

## E. Mamba-based Foundation Models in Remote Sensing

The recent success of foundation models [260] across various domains has spurred interest in their application to remote sensing. However, to date, only SatMAE [189] has provided a preliminary validation of Mamba-based foundation models in this field. Given Mamba's theoretical computational efficiency and linear complexity, Mamba-based models may offer significant advantages over transformer-based models, particularly when processing very high-resolution images.

Nonetheless, scaling Mamba-based models to larger sizes may introduce stability issues [40]. Although several backbone networks [21], [64], [75], [76] have proposed methods to enhance stability in large-scale implementations, these solutions have not yet been fully validated for foundation models in remote sensing. Addressing the stability challenges associated with large Mamba-based models is therefore a crucial area for future research.

## F. Computational Efficiency

Despite the advantages of linear computational complexity and a low computation burden, Mamba's recurrent computation paradigm leads to relatively low computational efficiency. The Mamba-1 [20] incorporates a hardware-aware algorithm for acceleration, and Mamba-2 [21] further improves efficiency by introducing an optimized SSD algorithm, achieving a 2-8 ˆ speedup compared to the vanilla. However, many remote sensing applications utilizing Mamba-1 still require enhancements in computational efficiency. Potential approaches include: (1) adopting Mamba-2 for remote sensing tasks, (2) developing more efficient hardware-aware algorithms, and (3) modifying the SSM formulation to circumvent the inherent limitations of recurrent computation, thereby enabling more effective utilization of current GPUs.

## G. Adaptations to Downstream Tasks

While Mamba-based models have demonstrated promise in several remote sensing applications, their full potential remains underexplored, particularly in unconventional or emerging downstream tasks. The architecture's advantages, such as linear scaling and efficient long-range dependency modeling, could effectively address unique challenges in less-studied tasks, such as some application in agriculture [261], in forest [262] and in ecological restoration [263] and in less-explored data, such as hyperspectral time-series analysis [264] that is characterized by both high spectral resolution and temporal continuity. Moreover, the potential of vision-language (CV +

NLP), for remote sensing remains underexplored. A Mambabased multimodal framework could enable novel applications such as content-guided image retrieval [265] in remote sensing data , automated visual question answering (VQA) [266] for non-expert users and image captioning [267] of remote sensing imagery. In addition, given the recent success of Large Language Models (LLMs), combining Vision Mamba with LLMs represents a promising direction, as it leverages the strong reasoning capabilities of LLMs. Beyond the tasks discussed in Section VI, further investigation into applications traditionally addressed by CNN and Transformer models represents a significant and promising research direction.

## VIII. CONCLUSION

Mamba architectures have rapidly emerged as a promising alternative to conventional CNN-based and Transformer-based models in remote sensing applications, primarily owing to their linear computational complexity, dynamic feature selection via input-dependent parameterization, and efficient long-range dependency modeling capabilities. This survey systematically summarized the evolution of Mamba-based methods in remote sensing, beginning with a concise overview of vision Mamba backbone networks. Then, we conclude a systematic analysis of both micro-architectural advancement, including enhanced SSM formula, scan strategies and multi-modal and bi-temporal feature interaction, and macro-architectural developments encompassing hybrid CNN/Transformer integrations, framework substitutions in existing frameworks, learning paradigms, and frequency-domain operations. Some structured taxonomies were established to systematically organize these technological advancements, providing researchers with clear pathways for methodology comparison and selection. Furthermore, we identified critical challenges and proposed promising research directions, which holds substantial potential to advance Mamba's capabilities in remote sensing. These insights aim to catalyze future investigations and foster the development of nextgeneration remote sensing systems powered by Mamba-based models.

## ACKNOWLEDGMENTS

this is acknowledgments
<|endofpaper|>