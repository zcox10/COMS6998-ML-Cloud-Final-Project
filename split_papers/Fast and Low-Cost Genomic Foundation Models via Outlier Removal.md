<|startofpaper|>
## Fast and Low-Cost Genomic Foundation Models via Outlier Removal

Haozheng Luo * 1 Chenghao Qiu * 2 Maojiang Su 1 Zhihan Zhou 1 Zoe Mehta 3 Guo Ye 1 Jerry Yao-Chieh Hu 1 Han Liu 1

## Abstract

## 1 Introduction

To address the challenge of scarce computational resources in genomic modeling, we introduce GERM , a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T , a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings. Code is available at https://github.com/MAGICS-LAB/GERM.

* Equal contribution 1 Northwestern University 2 Tianjin University 3 Vernon Hills High School. Correspondence to: Haozheng Luo &lt;hluo@u.northwestern.edu&gt;, Chenghao Qiu &lt;q1320460765@tju.edu.cn&gt;, Maojiang Su &lt;maojiangsu2030@u.northwestern.edu&gt;, Zhihan Zhou &lt;zhihanzhou2020@u.northwestern.edu&gt;, Guo Ye &lt;guoye2018@u.northwestern.edu&gt;, Zoe Mehta &lt;zoe.mehta@vhhscougars.org&gt;, Jerry Yao-Chieh Hu &lt;jhu@u.northwestern.edu&gt;, Han Liu &lt;hanliu@northwestern.edu&gt;.

Proceedings of the 42 nd International Conference on Machine Learning , Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).

We introduce a novel model named GERM by utilizing outlier-free Hopfield layer (Hu et al., 2024a) to replace traditional attention layer (Vaswani et al., 2017). GERM offers a quantization-friendly and rapidly adaptable DNA genomic foundation model (GFM), making it ideal for deployment and fine-tuning on resource-constrained devices.

Existing GFMs, such as DNABERT2 (Zhou et al., 2024) and GenomeOcean (Zhou et al., 2025b), achieve state-of-the-art performance on various genomics tasks. However, many GFM users include not only professional computational researchers but also researchers from traditional biomedical labs, who often operate on resource-constrained platforms such as mobile phones, edge devices, and IoT systems. The large size and high computational cost of these models make them challenging to use in such devices. Also, if researchers require the model to adapt to new tasks, it should be able to be fine-tuned on those tasks without demanding substantial computational resources. Efficient methods, such as lowrank adaptation fine-tuning (e.g., LoRA (Hu et al., 2022)) and post-training quantization (e.g., SmoothQuant (Xiao et al., 2023)) help reduce training and inference costs for GFMs. However, directly applying these techniques to original models without modification leads to huge performance drops. This results from outlier values in the model's attention mechanisms, inherited from pretrained models (Clark et al., 2019; Kovaleva et al., 2019b). Prior studies (Hu et al., 2024a; Bondarenko et al., 2024; Clark et al., 2019) show that transformer-based models often direct attention toward less useful tokens, referred to as outliers. These outliers cause inefficiencies that reduce the overall model performance. Additional studies (Wu et al., 2024c; Huang et al., 2024; Hu et al., 2025) reveal that low-rank adaptation worsens the outlier issue. Outliers from both pretrained models and low-rank adaptation fine-tuning processes distort outputs and lower accuracy.

To address inefficiencies caused by outliers in transformerbased genomic foundation models, GERM draws inspiration from associative memory models (Hu et al., 2024a;b; 2023; Xu et al., 2024; Wu et al., 2024a;b; Ramsauer et al., 2021). We replace the standard transformer attention mechanisms with an outlier-free attention layer proposed by Hu et al. (2024a), which detects and removes outliers occurring dur-

Figure 1: Structural Comparison of DNABERT-2 and GERM Models. This diagram illustrates the differences in processing pipelines between DNABERT-2 and GERM. Both DNABERT-2 and GERM use the SciencePiece tokenizer with BPE for tokenization. Following that, both models employ ALiBi for positional encoding in the embedding layer. However, as shown in (a), DNABERT-2's transformer architecture outputs the outliers. We propose replacing the vanilla Softmaxwith an outlier-free layer. In (b), the output of the attention mechanism removes outliers from the original output.

<!-- image -->

ing pretraining and low-rank adaptation (LoRA).

This outlier mitigation in GERM results in a 'triple win' for genomic foundation models: faster low-rank adaptation, reduced computational demands, and more reliable post-training quantization. On resource-constrained devices, incorporating fine-tuning techniques such as QLoRA (Dettmers et al., 2024a) and quantization methods like OmniQuant (Shao et al., 2024) enables efficient fine-tuning and inference with minimal performance degradation. This significantly enhances its accessibility and usability, promoting broader deployment without specialized hardware.

to enhance the model's ability to handle and mitigate outliers during pretraining and fine-tuning. Additionally, we introduce a continual learning strategy as a compromise version to avoid retraining the model from scratch. This strategy ensures suboptimal performance in terms of model quantization robustness and low-rank adaptation.

Additionally, addressing the limitations of (Hu et al., 2024a), particularly its resource-heavy training from scratch, we introduce GERM-T. GERM-T adds an outlier-free layer to existing GFM and uses small-step continual training to efficiently achieve near-optimal performance.

Contributions. We propose GERM , an outlier-free GFM with enhanced quantization robustness and rapid low-rank adaptation. Our contributions are as follows:

- · We propose an outlier-free model structure to address and mitigate outliers introduced by pretrained models and low-rank adaptation. This approach enables rapid low-rank adaptation and robust post-training quantization, significantly enhancing the overall performance of the quantized model and model finetuning. Notably, our model fine-tunes DNABERT in just 5 minutes on a single NVIDIA GeForce RTX 2080 Ti GPU.
- · Methodologically, we replace the standard transformer attention mechanism in the GFM with an outlier-free layer
- · Experimentally, We evaluate the performance and efficiency of our method using the existing DNABERT-2 model (Zhou et al., 2024) structure. Additionally, we benchmark it against the state-of-the-art low-rank adaptation methods and post-training quantization techniques. Compared to the standard framework, the proposed framework achieves average performance improvements of 37.98% in finetuning and 64.34% in quantization, respectively. Additionally, GERM shows a reduction of 92.14% in the average kurtosis and 82.77% in the maximum infinity norm on average.

## Related Work

Quantization. Considering the quantized object, exiting foundation models (FMs) quantization can be classified into two fields: weight-only quantization and weight-activation quantization. For weight-only quantization , prior studies focus on converting weights to low-bit values. For instance, GPTQ (Frantar et al., 2023) uses block-wise reconstruction for 3/4-bit quantization. SpQR (Dettmers et al., 2024b), OWQ (Lee et al., 2024), and AWQ (Lin et al., 2024) emphasize the significance of weights tied to higher-magnitude activations. Therefore, SpQR and OWQ employ mixed-precision quantization to safeguard vital weights, while AWQ opts for channel-wise scaling to avoid

mixed-precision's hardware inefficiency. QLoRA (Dettmers et al., 2024a), LoftQ (Li et al., 2023) and QUIP (Chee et al., 2023) restore the capabilities of the quantized model through parameter-efficient fine-tuning. For weightactivation quantization , prior studies compress both weights and activations. SmoothQuant (Xiao et al., 2023), LLM.int8() (Dettmers et al., 2022), and Outlier Suppression (Wei et al., 2022) achieve W8A8 quantization by managing activation outliers. LLM.int8() uses mixed-precision decomposition, while the other two employ channel-wise scaling. Furthermore, Outlier Suppression+ (Wei et al., 2023) adds channel-wise shifting to drive W6A6 quantization. In comparison to other quantization approaches, including prior works (Wei et al., 2023; Xiao et al., 2023) that address the outlier issue during quantization, the outlierfree layer in GERM is more effective at managing outliers within the model's attention mechanism. It provides GERM with a unique advantage in terms of quantization robustness.

Outlier Values in Quantization. Numerous studies (Hu et al., 2024a; Ma et al., 2024; Heo et al., 2024; Puccetti et al., 2022; Kovaleva et al., 2021; Bondarenko et al., 2021a; Luo et al., 2021) observe outlier values in the transformer-based language models such as BERT (Devlin et al., 2019) and early GPT (Radford et al., 2019) models. Since the advent of FMs (Zhou et al., 2024; 2025a; Zhang et al., 2022; Brown et al., 2020) root in the GPT and BERT, recent studies by Xiao et al. (2023); Ahmadian et al. (2023); Dettmers et al. (2022) tackle the existence of outlier values in FMs. According to them, these outliers exhibit a large magnitude of values at the shared dimensions of hidden states across tokens. More recently, Bondarenko et al. (2024); Sun et al. (2024); Hu et al. (2024a) explain that the outliers attribute to the vertical pattern in the attention mechanism (Xiao et al., 2024; Kovaleva et al., 2019a), influencing the performance of FMs. In particular, Sun et al. (2024) claim a different type of outlier existing in the hidden states of specific tokens. However, most of these studies concentrate on language and vision models, leaving the impact of outliers on genomic foundation models largely unexplored. Additionally, methods like Hu et al. (2024a) require training from scratch to eliminate outliers, which is computationally expensive.

Genomic Foundation Model. The majority of genomic foundation models (GFMs) use transformers to model sequence dependencies, similar to BERT (Devlin et al., 2019) and GPT (Brown et al., 2020) in NLP. Specifically, DNABERT (Ji et al., 2021) and DNABERT-2 (Zhou et al., 2024) leverage transformers for DNA sequence analysis by employing masked language modeling and fine-tuning for biological tasks. In addition, Nucleotide Transformer (DallaTorre et al., 2024) excels at molecular phenotype prediction and variant prioritization, while HyenaDNA (Nguyen et al., 2023b) is optimized for modeling long-range genomic dependencies. Furthermore, GenomeOcean (Zhou et al., 2025b) provides an efficient 4-billion-parameter genome foundation model for diverse, context-aware DNA sequence generation. However, these models demand significant computational resources and lack robustness to quantization, rendering them unsuitable for deployment on resourceconstrained devices. Specifically, GenomeOcean utilizes 64 NVIDIA A100 80G GPUs over a span of 14 days for training. This limits accessibility for research labs with limited computational capacity. More recently, Evo (Nguyen et al., 2024), a generative genomic model, integrating Transformer and Hyena operator to efficiently capture long-range dependencies in genomic sequences, achieving a context window of 131k nucleotides. Furthermore, Evo uniquely bridges bridges the DNA-RNA-protein central dogma via cross-modal inference without task-specific supervision.

## 2 GERM

This section introduces the proposed method, which comprises the outlier-free architecture, small-step continual learning, and the DNA genomic foundation model (GFM). The outlier-free architecture is designed to mitigate challenges posed by outliers during the model fine-tuning process. Meanwhile, the small-step continual learning technique extends the training process using smaller learning steps after the initial training, aiming to address and mitigate the outliers present in the original model checkpoints.

In our study, we develop the GFM framework to train DNA sequence-based genomic foundation models that employ Transformer-based architectures such as DNABERT (Ji et al., 2021) and Nucleotide Transformer (Dalla-Torre et al., 2024). These models use DNA tokenization and Transformer attention mechanisms, making them well-suited for integrating techniques like LoRA and the outlier-free mechanisms proposed in our approach. Alternatively, models like HyenaDNA (Nguyen et al., 2023b) and Caduceus (Schiff et al., 2024) utilize different architectures, such as convolutional layers or the Mamba architecture. While these models introduce novel features, they are not currently the most widely adopted in genomic modeling and require further research. Therefore, we adopt DNABERT-2 as the baseline in this paper, as it best represents the Transformer-based DNA GFMs central to our study. The proposed outlier-free architecture of GERM is illustrated in Figure 1.

## Outliers Challenge in Transformer Architecture. Clark

et al. (2019); Kovaleva et al. (2019b) reveal that BERT assigns disproportionately high attention weights to certain tokens, such as delimiters and end-of-sentence (eos) markers. Consequently, these tokens dominate the attention mechanism, overshadowing more informative tokens. Additionally, Kobayashi et al. (2020) show that tokens with

smaller value vectors often receive significantly larger attention weights. These phenomena indicate that transformerbased models may focus on less relevant information, leading to inefficient processing. Studies by Hu et al. (2024a); Bondarenko et al. (2024) highlight the underlying cause of the outlier challenge in transformer-based models, proposing that transformers do not require updates when the attention inputs are sufficiently informative. However, the Softmax function causes low-value tokens to receive disproportionately high attention weights. This leads to a wide range of attention scores and the presence of outliers, which adversely affect the model's performance. Additionally, this issue increases computational and memory demands during training and results in significant performance degradation after model quantization. Consequently, implementing a strategy to address outliers during both the pretraining and fine-tuning stages is crucial. Numerous studies address the outlier problem across different model stages, including pre-training (Hu et al., 2024a), fine-tuning (Hu et al., 2025), and inference (Bondarenko et al., 2024; Xiao et al., 2023). In our study, we extend the work of Hu et al. (2024a) by utilizing the memory-associated retrieval dynamics function Softmax 1 . This function is defined as

$$\text{Softmax} _ { 1 } ( S ) \coloneqq \frac { \exp ( S ) } { 1 + \sum _ { i = 1 } ^ { L } \exp ( S _ { i } ) },$$

where S is the input to the activation function. This approach addresses outlier problems in GFMs. Additionally, we provide a theoretical analysis of the expressive guarantee of low-rank adaptation for transformer-based GFMs with Softmax 1 in Appendix A.

Small-step Continual Learning. The outlier removal technique introduced in OutEffHop (Hu et al., 2024a) is highly effective in reducing the impact of outliers during model pretraining. However, a major limitation of this approach is the need to retrain the model from scratch, which is a significant challenge for large-scale models like GFMs due to the extensive time and computational resources required. To address this issue, we suggest a small-step continual learning approach as a compromise to the existing GERM structure, called GERM-T. It involves resuming training with an outlier-free model structure after the initial training phase to address and mitigate outliers in the original model checkpoints. This approach aims to lower the computational cost and time needed for retraining while still optimizing performance. Although this small-step continual learning technique may not be as effective as full retraining, it offers a more efficient and cost-effective solution for outlier removal in GFMs. For users with limited computational resources who cannot train a model from scratch and rely on 8-bit or 6-bit quantization during inference, GERM-T offers a viable compromise strategy.

DNA Genomic Foundation Model. We implement a simple yet effective design for the DNA genomic foundation model (GFM) following DNABERT-2 (Zhou et al., 2024). Initially, we employ SentencePiece (Kudo &amp; Richardson, 2018) with Byte Pair Encoding (BPE) (Sennrich et al., 2016), a subword tokenization method, to process DNA sequences. SentencePiece is particularly effective for handling the large number of unique tokens present in DNA without assuming any pre-tokenization, such as k-mer segmentation (Chor et al., 2009). SentencePiece with BPE is used in natural language processing for word segmentation and learns a fixed-sized vocabulary of variable-length tokens based on character co-occurrence frequencies. Due to the significant difference between natural language and DNA sequences, the vocabulary sizes used in the NLP domain (Zhang et al., 2022; Radford et al., 2019; Kenton &amp; Toutanova, 2019) are not suitable for DNA sequences. In our study, we set vocabulary size to 4096, as it best balances model performance with computational efficiency among candidates.

We then adopt the BERT architecture (Kenton &amp; Toutanova, 2019) to train our GFM on DNA sequences, with several modifications to better accommodate the unique characteristics of the DNA data. Standard positional encoding methods, such as Rotary Positional Encoding (Su et al., 2024) and Sinusoidal Positional Encoding (Vaswani et al., 2017), face limitations when applied to sequences longer than those encountered during training due to their inherent input length restrictions. To address these limitations, we employ the Attention with Linear Biases (ALiBi) method (Press et al., 2022), as it is more robust to variations in sequence length and can handle longer sequences compared to traditional positional encoding methods. Instead of adding positional embeddings to the input, ALiBi introduces linear biases into the attention mechanism, allowing the model to learn positional information inherently from the input sequence. Specifically, let q i ∈ R d represent the query vector for the i -th token in a sequence of length L , and K ∈ R L × d denote the key matrix for all tokens. The attention score for query q i is computed as: Softmax( q K i ⊤ + m × -[ ( i -1) , . . . , -1 0 , , -1 , . . . , -( L -1 -i )]) , where m is a fixed scalar. ALiBi uses a geometric sequence of different m values for each attention head, allowing model to learn positional information from the input sequence itself. By replacing learned positional embeddings with ALiBi, GERM can process arbitrarily long sequences during fine-tuning and inference, despite being pre-trained on relatively shorter sequences.

## 3 Experimental Studies

In this section, we perform a series of experiments to demonstrate the effectiveness of our proposed method. In particular, we compare the performance of our method with DNABERT-2 detailed in (Zhou et al., 2024).

Table 1: Comparing GERM and GERM-T with DNABERT-2 in a Post-Training Quantisation (PTQ) setting. We perform experiments on GERM with baseline models using four quantization methods (Traditional W8A8, SmoothQuant, Outlier Suppression, OmniQuant) across three quantization configurations (Weight-8bit-Activation-8bit (W8A8), Weight-6bit-Activation-6bit (W6A6), and Weight-4bit-Activation-4bit (W4A4)). The evaluation metrics include the Matthews Correlation Coefficient (MCC), the difference in MCC (Delta MCC) compared to the official DNABERT-2 checkpoint, the average kurtosis , and the maximum infinity norm ∥ x ∥ ∞ for outlier values at FP16. The best results are highlighted in bold, while the second-best results are underlined. In most configurations, GERM demonstrates superior fine-tuning performance compared to DNABERT-2.

| Model     | #Bits   | Quantization Method   | MCC ( ↑ )    | Delta MCC ( ↓ )   | Avg Performance Drop ( ↓ )   | Avg. Kurtosis ( ↓ )   |   Max inf. norm ( ↓ ) |
|-----------|---------|-----------------------|--------------|-------------------|------------------------------|-----------------------|-----------------------|
| Official  | 16W/16A | -                     | 66.11        | -                 | -                            | 39.68                 |                 53.61 |
| DNABERT-2 | 16W/16A | -                     | 59.11        | 7.00              | - 43.81%                     | 270.90                |                 61.64 |
| DNABERT-2 | 8W/8A   | -                     | 33.60 ± 0.41 | 32.51             |                              | 270.90                |                 61.64 |
| DNABERT-2 | 8W/8A   | -                     | 36.51 ± 0.02 | 45.37             | 38.63%                       | 270.90                |                 61.64 |
| DNABERT-2 | 6W/6A   | SmoothQuant           | 20.74 ± 0.04 | 45.37             | 66.18%                       | 270.90                |                 61.64 |
| DNABERT-2 | 4W/4A   | -                     | -1.03 ± 0.06 | 67.06             | 101.24%                      | 270.90                |                 61.64 |
| DNABERT-2 | 8W/8A   | Outlier               | 25.26 ± 0.02 | 40.85             | 57.60%                       | 270.90                |                 61.64 |
| DNABERT-2 | 6W/6A   | Outlier               | 27.84 ± 0.28 | 38.27             | 52.71%                       | 270.90                |                 61.64 |
| DNABERT-2 | 8W/8A   | OmniQuant             | 49.92 ± 0.05 | 16.19             | 15.76%                       | 270.90                |                 61.64 |
| DNABERT-2 | 6W/6A   | OmniQuant             | 48.47 ± 0.14 | 17.64             | 18.61%                       | 270.90                |                 61.64 |
| DNABERT-2 | 4W/4A   | OmniQuant             | 2.94 ± 0.19  | 63.17             | 94.78%                       | 270.90                |                 61.64 |
| GERM      | 16W/16A | -                     | 59.73        | 6.38              | -                            | 251.40                |                 10.62 |
| GERM      | 8W/8A   | -                     | 57.30 ± 0.08 | 8.81              | 3.77%                        | 251.40                |                 10.62 |
| GERM      | 8W/8A   | SmoothQuant           | 56.65 ± 0.15 | 9.46              | 4.82%                        | 251.40                |                 10.62 |
| GERM      | 6W/6A   | SmoothQuant           | 56.48 ± 0.07 | 9.63              | 5.45%                        | 251.40                |                 10.62 |
| GERM      | 4W/4A   | SmoothQuant           | 20.05 ± 0.00 | 46.06             | 69.44%                       | 251.40                |                 10.62 |
| GERM      | 8W/8A   | Outlier               | 45.87 ± 0.08 | 20.24             | 25.23%                       | 251.40                |                 10.62 |
| GERM      | 6W/6A   |                       | 40.57 ± 0.56 | 25.54             | 36.27%                       | 251.40                |                 10.62 |
| GERM      | 8W/8A   | OmniQuant             | 55.99 ± 0.09 | 10.12             | 5.95%                        | 251.40                |                 10.62 |
| GERM      | 6W/6A   | OmniQuant             | 55.70 ± 0.03 | 10.41             | 6.41%                        | 251.40                |                 10.62 |
| GERM      | 4W/4A   | OmniQuant             | 49.42 ± 0.00 | 16.69             | 17.17%                       | 251.40                |                 10.62 |
| GERM-T    | 16W/16A | -                     | 59.30        | 6.81              | -                            |                       |                 28.49 |
| GERM-T    | 8W/8A   | -                     | 38.38 ± 0.15 | 27.73             | 35.27%                       |                       |                 28.49 |
| GERM-T    | 8W/8A   | -                     | 57.52 ± 0.00 | 8.59              | 3.01%                        |                       |                 28.49 |
| GERM-T    | 6W/6A   | SmoothQuant           | 30.34 ± 0.04 | 35.77             | 48.83%                       |                       |                 28.49 |
| GERM-T    | 4W/4A   | SmoothQuant           | 0.22 ± 0.00  | 65.89             | 99.63%                       |                       |                 28.49 |
| GERM-T    | 8W/8A   | Outlier               | 42.57 ± 0.05 | 23.54             | 28.31%                       |                       |                 28.49 |
| GERM-T    | 6W/6A   | Outlier               | 46.02 ± 0.06 | 20.06             | 22.34%                       |                       |                 28.49 |
| GERM-T    | 8W/8A   | OmniQuant             | 56.80 ± 0.12 | 9.31              | 4.21%                        |                       |                 28.49 |
| GERM-T    | 6W/6A   | OmniQuant             | 55.41 ± 0.00 | 10.71             | 6.57%                        |                       |                 28.49 |
| GERM-T    | 4W/4A   | OmniQuant             | 3.86 ± 0.00  | 62.25             | 93.49%                       |                       |                 28.49 |

Models. Following Zhou et al. (2024), we validate our strategy with DNABERT-2 model. we adopt the DNABERT2 model of size 117 million parameters . We pretrain this 1 model with the masked language modeling (MLM) technique, following the original DNABERT-2 (Zhou et al., 2024). Each model trained from scratch undergoes a total of 200K training steps. For models utilizing small-step continual learning, we initially train the model from scratch using the DNABERT-2 architecture, followed by continual learning with the outlier-free structure for the remaining steps. In our experiment, we use the 40K continual learning steps model as the representative example of GERM-T to compare against DNABERT-2 and GERM.

1 https://huggingface.co/zhihan1996

Datasets. We utilize 27 datasets spanning 7 tasks and 4 species, as outlined in (Zhou et al., 2024). As shown in Appendix B.3, most downstream tasks in GFMs are classification tasks. Consequently, the datasets are designed

Table 2: Comparing GERM and GERM-T with DNABERT-2 in a Low-Rank Adaptation Setting. We perform experiments on GERM with baseline models across three Low-Rank Adaptation methods (LoRA, QLoRA, LoftQ). The evaluation metrics include the Matthews Correlation Coefficient (MCC), the Delta MCC performance difference relative to the official DNABERT-2 checkpoint, the average kurtosis , and the maximum infinity norm ∥ x ∥ ∞ for outlier values. Additionally, we measure the average performance drop after low-rank adaptation to evaluate the efficiency of GERM in this setting. The best results are highlighted in bold, while the second-best results are underlined. In most configurations, GERM demonstrates superior fine-tuning performance compared to DNABERT-2. The GERM demonstrates an average performance improvement of 37.98% compared to the DNABERT-2 model.

| Models     | Low-Rank Adaptation Method   | MCC ( ↑ )          | Delta MCC different ( ↓ )   | Avg Performance Drop ( ↓ )   | Avg. kurtosis( ↓ )   |   Max inf. norm( ↓ ) |
|------------|------------------------------|--------------------|-----------------------------|------------------------------|----------------------|----------------------|
| DNA BERT-2 | Full                         | 59.11 50.91 ± 1.67 | 7.00 15.2                   | - 13.87%                     | 270.90 -             |                61.41 |
| DNA BERT-2 | LoRA                         |                    |                             |                              |                      |               219.2  |
| DNA BERT-2 | QLoRA                        | 50.65 ± 0.13       | 15.46                       | 14.31%                       | 292.85               |                53.91 |
| DNA BERT-2 | LoftQ                        | 50.76 ± 0.06       | 15.31                       | 14.05%                       | 299.18               |                54.18 |
| GERM       | Full                         | 59.73              | 6.38                        | -                            | 21.29                |                10.62 |
| GERM       | LoRA                         | 57.27 ± 0.70       | 8.84                        | 4.12%                        | -                    |                19.41 |
| GERM       | QLoRA                        | 53.16 ± 0.21       | 12.95                       | 10.99%                       | 34.29                |                27.27 |
| GERM       | LoftQ                        | 53.11 ± 0.08       | 13.00                       | 11.08%                       | 33.02                |                27.41 |
| GERM-T     | Full                         | 59.30              | 6.81                        | -                            | 251.40               |                28.49 |
| GERM-T     | LoRA                         | 55.60 ± 0.28       | 10.51                       | 6.23%                        | -                    |               140.86 |
| GERM-T     | QLoRA                        | 51.05 ± 0.07       | 15.06                       | 13.90%                       | 287.95               |                53.92 |
| GERM-T     | LoftQ                        | 51.20 ± 0.13       | 14.91                       | 13.65%                       | 286.16               |                53.35 |

for genome sequence classification problems, with input lengths ranging from 70 to 1000.

conducted three times with different random seeds, and we report the average and standard deviation for each metric.

Evaluation Metrics. To evaluate the performance of outliers in our strategy, we report the maximum infinity norm ∥ x ∥ ∞ of the activation tensors x across all transformer layers as a metric for detecting outliers. Additionally, we present the average kurtosis of x , calculated only from the output tensors from the Feed-Forward Network (FFN) layer and Layer Normalization. These two components are known to contain outliers, as confirmed by our experiments and prior studies (Hu et al., 2024a; Bondarenko et al., 2024; 2021b). Both metrics have demonstrated a strong correlation with model quantizability (i.e., robustness to outliers) (Bondarenko et al., 2021b; Shkolnik et al., 2020). For pre-quantization performance, we also report the FP16 (16bit floating-point) Matthews correlation coefficient (MCC) score to assess model's downstream classification ability.

## 3.1 Post-Training Quantization (PTQ)

To assess the efficiency of our method for Post-Training Quantization (PTQ), we replace the standard attention layer in DNABERT-2 (Zhou et al., 2024) with the Softmax 1 activation function. We utilize the pre-trained checkpoints of these three models and fine-tune them at full rank following the procedure described in (Zhou et al., 2024). In this experiment, we evaluate the models on the test datasets using FP16 precision and apply PTQ to measure performance degradation due to quantization. Each evaluation is

Baselines. To evaluate the performance of our method against the official DNABERT-2 model, we also full finetune the official pretrained DNABERT-2 model 1 as a baseline on a downstream classification task to demonstrate the absolute performance of those three models. We further compare the performance of these three models on the same downstream classification task using FP16 precision and four PTQ methods: Traditional W8A8 (Weights-8bit, Activations-8bit) as outlined in Bondarenko et al. (2024), SmoothQuant (Xiao et al., 2023), Outlier Suppression (Wei et al., 2022), and OmniQuant (Shao et al., 2024). With the exception of the W8A8 method, we evaluate and compare the quantization performance of SmoothQuant, Outlier Suppression, and OmniQuant at W8A8 (Weights-8bit, Activations-8bit) and W6A6 precision levels. Additionally, we present the quantization performance for W4A4 using OmniQuant and SmoothQuant. We use the same hyperparameters specified in their respective studies. This approach guarantees that our evaluations are conducted under standardized conditions, enabling precise comparisons and assessments of each quantization method.

Results. Referring to Table 1, it is clear that the GERM surpasses the DNABERT-2 in scenarios involving W4A4, W6A6 and W8A8 post-training quantization using state-ofthe-art PTQ methods. Specifically, when both weights and activations are quantized to 8 bits (W8A8), GERM exhibits

a minimal average performance decline of only 4.82% on SmoothQuant. Additionally, the proposed strategy remains effective as the quantization bit size decreases. For example, when models are quantized to W4A4, GERM exhibits a minimal average performance decline of only 17.17% on OmniQuant, compared to an 94.78% drop in the DNABERT2. This demonstrates that GERM outperforms the vanilla structure by enhancing the robustness of model quantization and improving performance across outlier-free methods like SmoothQuant and Outlier Suppression. Additionally, GERM-T exhibits strong quantization performance at both 8-bit and 6-bit levels, with a minimal performance drop, even smaller than that of GERM. The only exception is with W4A4 quantization, where GERM-T experiences a notable performance drop due to larger outliers in GERM-T compared to the GERM. The outlier metrics indicate the improvement in average kurtosis is minimal, though a substantial reduction in the maximum infinity norm compared to the DNABERT-2. Outlier metrics show that GERM reduces the average kurtosis by ∼ 92.14% and the maximum infinity norm by ∼ 82.77% across 27 datasets. Additionally, GERM-T achieves a reduction of approximately 7.20% in average kurtosis and 53.78% in the maximum infinity norm across the same datasets.

## 3.2 Low-Rank Adaptation

Fine-tuning models for downstream tasks is often computationally expensive. To enhance the efficiency of finetuning with fewer parameters, various parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), are commonly used. To assess the efficiency of our method for fine-tuning tasks with LoRA, we evaluate our framework on LoRA methods. We use the pretrained checkpoints of the three models and fine-tune them using multiple LoRA approaches following a similar procedure as described in Section 3.1. In this experiment, we evaluate the models on the test datasets using full-rank fine-tuning to compare the performance drop across various LoRA approaches. Each evaluation is conducted three times with different random seeds, and we report the average and standard deviation for each metric.

LoRA Methods. We compare our method with the vanilla version across three different LoRA methods: LoRA (Hu et al., 2022), QLoRA (Dettmers et al., 2024a), and LoftQ (Li et al., 2023). For the Full Fine-Tuning method, we fine-tune the model at full rank using mixed-precision FP16 training. For the LoRA method, following (Hu et al., 2022), we finetune the model with low-rank adaptations using a rank of 128 and an alpha value of 256. For the QLoRA and LoftQ methods, we fine-tune the model with quantized low-rank adaptations, maintaining the same rank and alpha values as in LoRA. Both QLoRA and LoftQ utilize 4-bit quantization methods as described in (Dettmers et al., 2024a).

Results. In Table 2, our results highlight the effectiveness of GERM in low-rank adaptation. In most configurations, GERM significantly enhances fine-tuning performance. Specifically, GERM achieves an average performance improvement of 37.98% in low-rank adaptation compared to DNABERT-2 model. Similarly, GERM-T shows an average performance improvement of 20.01% over the same baseline. These results demonstrate that both GERM and GERM-T can greatly enhance low-rank adaptation for the model. When considering outlier metrics, we observe that LoRA exhibits significantly larger outlier values compared to full fine-tuning. Also, in most configurations, the outlier values in LoRA are much higher than those in QLoRA and LoftQ. One potential reason is that QLoRA and LoftQ utilize quantization to stabilize parameter updates and compress model representations, which helps minimize the amplification of extreme outlier values. Furthermore, GERM demonstrates a substantial reduction in outlier values compared to DNABERT-2, and GERM-T also experiences a significant decrease in the maximum infinity norm , though the reduction in average kurtosis remains limited.

Table 3: Quantization Robustness Performance Comparison with Different Continual Learning Steps. We evaluate the quantization robustness of different models, using SmoothQuant at 16-bit, 8-bit, and 6-bit quantization levels. The evaluation metric is the Matthews Correlation Coefficient (MCC) with the average performance drop following quantization also noted. The best results are highlighted in bold, and the second-best results are underlined.

| Method    | #Bits   |   MCC ( ↑ ) | Avg Performance Drop ( ↓ )   |
|-----------|---------|-------------|------------------------------|
| DNABERT-2 | 16W/16A |       59.11 | -                            |
| GERM      | 16W/16A |       59.73 | -                            |
| Out20k    | 16W/16A |       59.21 | -                            |
| GERM-T    | 16W/16A |       59.3  | -                            |
| Out100k   | 16W/16A |       60.56 | -                            |
| DNABERT-2 | 8W/8A   |       36.51 | 38.23%                       |
| GERM      | 8W/8A   |       56.78 | 4.93%                        |
| Out20k    | 8W/8A   |       54.75 | 7.53%                        |
| GERM-T    | 8W/8A   |       57.52 | 3.00%                        |
| Out100k   | 8W/8A   |       58.77 | 2.96%                        |
| DNABERT-2 | 6W/6A   |       20.74 | 64.91%                       |
| GERM      | 6W/6A   |       56.48 | 5.44%                        |
| Out20k    | 6W/6A   |       27.61 | 53.36%                       |
| GERM-T    | 6W/6A   |       28.32 | 52.24%                       |
| Out100k   | 6W/6A   |       30.44 | 49.74%                       |

## 3.3 Additional Experiments

In this section, we conduct additional experiments to evaluate the effectiveness of our method in various scenarios.

Figure 2: Comparison of Performance in Resource-Constrained Computing Environments. Comparison of three models on the quantization and fine-tuning task. All models were trained on the same computing infrastructure (Nvidia GeForce RTX 2080 TI 11GB) for fair comparison. The training time represents the average time per epoch, with OmniQuant used as quantization example in this figure.

<!-- image -->

We conduct ablation studies to investigate the impact of different continual learning steps, the influence of adaptor rank. Also, we conduct a case study on model deployment and fine-tuning using a single 2080 Ti to evaluate model latency and fine-tuning speed per epoch.

larger performance drop observed in the model using 100K steps of continual learning, compared to GERM-T. It is attributed to the superior performance achieved through full fine-tuning. These results indicate that employing 40K continual learning steps in GERM-T is optimal for enhancing model performance in our approach.

Impact of Different Continual Learning Steps. To evaluate our proposed method's performance across various continual learning steps, we conduct experiments on three different step sizes: 20K, 40K, and 100K. We train the models for a total of 200K steps, employing a combination of vanilla and outlier-free pretraining as outlined in (Zhou et al., 2024) and (Hu et al., 2024a). To assess performance degradation after quantization using SmoothQuant, we perform full-rank fine-tuning of the models using the training datasets. Additionally, we assess the performance decline across models using three iterations of LoRA-based fine-tuning technology. We use the "Out xxk " prefix to indicate the number of continual learning steps applied to the outlier-free structure. For instance, Out100k represents a model trained with 100K continual learning steps using the outlier-free structure. This notation helps illustrate how different levels of continual learning impact the model's performance.In most configurations, GERM outperforms DNABERT-2 in both quantization robustness and low-rank adaptation. The only exception is in the 8-bit quantization scenario, where the Out100k and GERM-T models exhibit better performance and a smaller average performance drop than GERM. Overall, GERM-T achieves better near-optimal performance across all continual learning model settings. The results, as shown in Tables 3 and 4, demonstrate that our method outperforms the vanilla approach across all test sets. Also, we observe that GERM-T exhibits the most optimal performance drop during quantization and low-rank adaptation compared to other continual learning steps. The

Performance of GERM on Alternative Transformer-based Models. To assess the performance of our proposed method on alternative transformer-based GFMs, we conducted experiments on Nucleotide Transformer (NT) (Dalla-Torre et al., 2024), a significant genomic foundation model in this domain. The results, as shown in Tables 19 and 20, demonstrate that our method outperforms the vanilla approach across all test sets. GERM achieves an average performance improvement of 52.01% in low-rank adaptation and 67.69% in PTQ experiments. We also conduct experiments on the NT-2.5B model to demonstrate the scalability of GERM on larger-scale GFMs, as shown in Appendix D.7.

## Performance of GERM with Alternative Outlier Removal

Methods. To evaluate the performance of our proposed method against existing outlier removal techniques, we compare GERM with approaches such as clipped softmax and gated attention (Bondarenko et al., 2024). As shown in Appendix D.8, GERM achieves a performance improvement of 2.59% in 4-bit and 1.99% in 8-bit quantization.

## Performance of GERM with Different Adaptor Rank.

In our experiments, we evaluate the performance of our proposed method under the influence of adapter rank. All results are presented in Appendix D.1.

Table 4: Low-Rank Adaptation Performance Comparison with Different Continual Learning Steps. We evaluate the lowrank adaptation performance (LoRA, QLoRA, LoftQ) for various models with different continual learning steps. The evaluation metric is the Matthews Correlation Coefficient (MCC), and the average performance drop after adaptation is also shown. The best results are highlighted in bold, and the second-best results are underlined.

| Method    | Fine-Tuning Method   |   MCC ( ↑ ) | Avg Performance Drop ( ↓ )   |
|-----------|----------------------|-------------|------------------------------|
| DNABERT-2 | Full                 |       59.11 | -                            |
| GERM      | Full                 |       59.73 | -                            |
| Out20k    | Full                 |       59.21 | -                            |
| GERM-T    | Full                 |       59.3  | -                            |
| Out100k   | Full                 |       60.56 | -                            |
| DNABERT-2 | LoRA                 |       50.91 | 13.87%                       |
| GERM      | LoRA                 |       56.78 | 4.94%                        |
| Out20k    | LoRA                 |       54.75 | 7.53%                        |
| GERM-T    | LoRA                 |       55.6  | 6.24%                        |
| Out100k   | LoRA                 |       56.61 | 6.52%                        |
| DNABERT-2 | QLoRA                |       50.65 | 14.31%                       |
| GERM      | QLoRA                |       53.16 | 11.00%                       |
| Out20k    | QLoRA                |       50.61 | 14.52%                       |
| GERM-T    | QLoRA                |       51.05 | 13.91%                       |
| Out100k   | QLoRA                |       51.24 | 15.39%                       |
| DNABERT-2 | LoftQ                |       50.76 | 14.13%                       |
| GERM      | LoftQ                |       53.11 | 11.08%                       |
| Out20k    | LoftQ                |       50.94 | 13.97%                       |
| GERM-T    | LoftQ                |       51.2  | 13.66%                       |
| Out100k   | LoftQ                |       50.77 | 16.17%                       |

## 3.4 Case Study: Performance in Resource-Constrained Computing Environments.

Case Study 1: Performance in Single 2080-Ti GPU Computing Environments. To demonstrate GERM's capability in resource-constrained environments, we conduct performance tests on a single NVIDIA GeForce RTX 2080 Ti 11GB GPU, where GERM requires only 5 minutes to finetune, demonstrating its practicality and efficiency. We compare GERM's performance with the DNABERT-2 model on the same downstream classification task using OmniQuant. Consistent hyperparameters from their respective studies are applied across all models. Additionally, we provide the per-epoch training time and inference time for the LoRA, QLoRA, and LoftQ fine-tuning methods. The results, as shown in Figure 2, show that both GERM and GERM-T achieve shorter full-rank fine-tuning times per epoch compared to DNABERT-2. Additionally, the model quantization latency for both GERM and GERM-T is lower than that of DNABERT-2, while delivering superior quantization performance. These observations indicate that GERM offers faster adaptation and requires fewer computational resources for users during both the inference and fine-tuning processes.

Case Study 2: Performance in CPU-Only Computing Environments. To demonstrate GERM's capability in CPUonly computing environments, we perform performance tests on CPU-only devices. We compare GERM's per-epoch training and inference times for the LoRA and QLoRA finetuning methods. The results, presented in Appendix D.4, indicate that both GERM and GERM-T achieve shorter finetuning times per epoch compared to DNABERT-2, with the only exception being QLoRA when deployed, where the time is slightly longer.

## 4 Discussion and Conclusion

We introduce GERM, a versatile and accessible GFM designed to function on limited computational resources. By replacing the vanilla attention layer with an outlier-free layer, we eliminate outliers during both model pretraining and fine-tuning. This approach ensures robust quantization and enables effective low-rank adaptation. In addition to presenting a novel architecture that enhances DNABERT by mitigating outliers, GERM incorporates a compromise strategy with continual learning, eliminating the need for extensive retraining. Empirically, GERM achieves an average reduction of average kurtosis by ∼ 92.14% and the maximum infinity norm by ∼ 82.77% across 27 datasets. Additionally, GERM enhances model quantization robustness by decreasing the average quantization performance drop by 64.34% and the average low-rank adaptation performance drop by 37.98%. For the compromise model GERM-T, quantization robustness is improved by reducing the average quantization performance drop by 31.42% and the average low-rank adaptation performance drop by 20.01%.

Limitations and Future Work. While successful in many settings, our proposed GERM-T still faces challenges in eliminating outliers in GFM, leading to significant performance drops during 4-bit quantization and low-rank adaptation methods such as QLoRA and LoftQ. In future work, we aim to develop strategies that efficiently remove outliers without necessitating retraining from scratch. We provide a more detailed discussion of the limitations of GERM-T in Appendix E.

## Impact Statement

We believe this methodology presents an opportunity to strengthen the core of foundation models, including large language models, by improving robustness through quantization and enabling faster low-rank adaptation. However, this approach may also amplify biases in the training data, potentially leading to unfair or discriminatory outcomes for underrepresented groups.

## Acknowledgments

The authors would like to thank Yegna Jambunath for enlightening discussions on related topics, and Jiayi Wang for facilitating experimental deployments. The authors would like to thank the anonymous reviewers and program chairs for constructive comments.

Han Liu is partially supported by NIH R01LM1372201, NSF AST-2421845, Simons Foundation MPS-AI-00010513, AbbVie and Dolby. Haozheng Luo is partially supported by the OpenAI Researcher Access Program. This research was supported in part through the computational resources and staff contributions provided for the Quest high performance computing facility at Northwestern University which is jointly supported by the Office of the Provost, the Office for Research, and Northwestern University Information Technology. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.
<|endofpaper|>