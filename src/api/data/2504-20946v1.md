<|startofpaper|>
## Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models

## Tyler McDonald

## Ali Emami

Department of Computer Science Brock University St. Catharines, Ontario tmcdonald3@brocku.ca

## Abstract

As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-ofThought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of highquality computational linguistics applications. Our code, featuring data analytics and testing scripts, is provided here.

## 1 Introduction

Prompt engineering has emerged as a paramount field of study within natural language processing (NLP), with various, highly performant approaches being published in rapid succession (Wei et al., 2023; Wang et al., 2023; Yao et al., 2023; Kojima

Department of Computer Science Brock University St. Catharines, Ontario aemami@brocku.ca

## et al., 2023; Huang et al., 2022a; Zou et al., 2023).

As these approaches become more lauded within research communities, sheer performance has become a dominant benchmark on myriad datasets. However, these approaches popularize reasoning as an 'all-or-nothing' process, with no insight into these steps until a post-hoc evaluation can be performed. A lack of transparent reasoning means common, easily resolved errors can propagate through the problem-solving process, degrading performance and potentially encouraging greedy solutions, while giving no extra opportunities to refine reasoning during problem-solving (Saparov and He, 2023).

Additionally, with the rise of various small-scale language models fine-tuned for specific tasks, it has become markedly easier for users to deploy models for problem solving on their own consumer hardware (Touvron et al., 2023; Xu et al., 2023; Tunstall et al., 2023; Jiang et al., 2023, inter alia ). This control over a suite of local models brings a paradigm shift towards the age of accessibility via open-source language modeling. While easily deployed and accessible, these language models can struggle on common tasks such as textual and multi-modal arithmetic reasoning, demanding an improvement to their critical problem-solving skills (Hendrycks et al., 2021b; Shao et al., 2024; Lu et al., 2024).

By recognizing the potential for hybrid deployments, and the strengths of employing both large- and small-scale models in a multi-modal environment, we can work to counteract the deficits of each approach. While large, closedsource language models are computationally intensive, we can distill the critical reasoning strengths from these models to imbue the same strengths in small-scale models (Shridhar et al., 2023; Chen et al., 2023a). By harnessing this distillation, we can offset the computational load onto commercially available hardware as

Figure 1: Comparison of standard Zero-Shot Chain-of-Thought prompting with our Trace-of-Thought prompting strategy on a GSM8K problem instance.

<!-- image -->

opposed to using closed-source APIs, allowing for both flexibility in the models deployed alongside reliable service quality through direct access to the underlying machinery.

are executed. This allows for more informed reasoning not only about the question at hand, but also regarding the strengths of the model performing the delegations without the need to wait for a post-hoc analysis.

Given these challenges and opportunities, we propose Trace-of-Thought , a novel prompting strategy that segments complex problem-solving into observable reasoning steps, as a direct response to the need for more transparent, efficient, and accessible problem-solving mechanisms within the field of NLP. Figure 1 illustrates a comparative example on a GSM8K (Cobbe et al., 2021) problem instance, highlighting the streamlined and concise reasoning pathways that make Trace-of-Thought particularly suited for smaller models that typically struggle with more complex reasoning chains.

Our contributions with introducing Trace-ofThought are threefold:

- 1. Transparency . Trace-of-Thought provides an observable framework in which steps to be executed by a question answering model are known by the human supervisor before they
- 2. Open-Source Problem-Solving . Through bolstering open-source models with instructions from larger, closed-source models, we observe sizable gains in performance that showcase the utility of smaller models as competitive problem solvers. By utilizing opensource models as solution models, we observe performance from these models increasing by over 125% compared to other popular approaches. Additionally, with models such as WizardMath-7B exceeding GPT-3.5-Turbo Chain-of-Thought by a substantial margin of over 10%, employing Trace-of-Thought is a viable alternative when the correct models are established. This encouraging performance suggests small, open-source language models may continue to develop as appropriate alternatives to closed-source models.
- 3. In-Context Knowledge Distillation . Using high quality instructions generated by large language models such as GPT-4, we are able to harness the concept of knowledge distillation and apply it in an in-context manner without the need for prior fine-tuning (Hinton et al., 2015; Huang et al., 2022b; OpenAI et al., 2023). Through simple delegation and answering prompts, we exploit the continued success with GPT-4 as a high-level reasoner, and diffuse this reasoning to models less endowed through typical human interaction (Espejel et al., 2023). This creates a reliance on large, closed-source models not for blanket solutions, but for instruction-based reasoning and task identification.

## 2 Related Work

Open-source language modeling . The democratization of NLP has been greatly advanced by the advent of open-source language models. These models, such as Zephyr-7B, Llama 2-7B Chat, and WizardMath-7B, serve as testament to the field's progress, offering domain-specific capabilities that challenge the hegemony of large, proprietary models (Touvron et al., 2023; Olausson et al., 2023; Li et al., 2023b). Our work builds on this foundation, demonstrating through empirical evaluation how these models can be further empowered by Trace-of-Thought prompting to approach the efficacy of their larger counterparts at a fraction of the computational cost. This not only showcases their potential as standalone solutions but also underscores the importance of open-source models in fostering a competitive, diverse NLP ecosystem, especially through harnessing suites of these opensource alternatives (Chen et al., 2023b).

Intermediate reasoning . Tasks such as those in the domain of arithmetic reasoning demand careful and planned steps to create strong outputs, and a failure to identify sub-problems correctly can create elementary mistakes (Stolfo et al., 2023). Approaches such as Zero- and Few-Shot Chainof-Thought Prompting aid with this deliberate reasoning; however, these reasoning paths are hidden ahead of runtime and do not lend any insight into potentially incorrect reasoning (Li et al., 2023a). Similarly, even with strong Chain-of-Thought reasoning, subsequent outputs may prove to be unfaithful to the reasoning employed (Lyu et al., 2023; Lanham et al., 2023). Approaches like Least-to-

Most prompting do allow for some visibility of intermediate reasoning, and encourage faithful performance, but may suffer from very lengthy input prompts that trade efficiency and readability for performance (Zhou et al., 2023b). In implementing Trace-of-Thought, we wish to tightly constrain problem solving to steps generated and assigned to a model, while preserving strong performance, all in a compact prompt.

Question decomposition. Prior work has explored question decomposition via the use of either recursive prompting or as a method of fine-tuning to learn intermediate step patterns (Qi et al., 2023; Yao et al., 2023; Shridhar et al., 2023). However, these methods have focused on question decomposition as a function of training data, trade problematic time complexity for performance gains, or require additional human involvement to succeed (Radhakrishnan et al., 2023; Patel et al., 2022; V et al., 2023). Trace-of-Thought differs from these works as an efficient, in-place prompting methodology, as full conversations take no more than two prompts by default. Additionally, Trace-ofThought assumes no requirement that the model be trained on training data that utilizes question decomposition strategies. By extracting trends and strengths from existing training corpora - demonstrated here across a wide range of models - a user can simply attach prompt prefixes and suffixes to employ Trace-of-Thought in their own work.

## 3 Model Hybridity

The utilization of LLMs like GPT-4 has been transformative across numerous domains, showcasing an unparalleled ability to handle complex tasks with nuanced reasoning (OpenAI et al., 2023). Despite their prowess, reliance on such closed-source models introduces limitations in accessibility, customization, and potential concerns over long-term availability (Bubeck et al., 2023). These restrictions underscore the need for more democratic solutions within computational linguistics.

In response to the proprietary nature of industrystandard models, the NLP community has seen a surge in the development and adoption of opensource alternatives. These models, such as WizardMath and DeepSeekMath, have not only achieved parity with GPT-3.5 on arithmetic reasoning benchmarks but are also closing the gap with GPT-4 (Shao et al., 2024; Hendrycks et al., 2021a,b; Liu et al., 2023). This trend towards open-source so-

Figure 2: Trace-of-Thought Prompting workflow compared to the workflow of two other approaches, Chain-ofThought Prompting and Standard Prompting.

<!-- image -->

lutions is reshaping the landscape, offering viable, cost-effective alternatives that democratize access to high-quality NLP tools.

The concept which we hereby coin model hybridity emerges as a natural progression in this evolving environment. This approach is not merely about leveraging the individual strengths of various models for improved performance; it's about fostering a synergistic relationship between them. Model hybridity combines the critical reasoning capabilities of LLMs with the agility and accessibility of smaller, open-source models, aiming for a composite that is greater than the sum of its parts (Madaan et al., 2023; Chen et al., 2023b).

By viewing models as collaborators, we capitalize on their unique strengths-be it the deep, nuanced understanding of LLMs or the specialized knowledge and efficiency of smaller models. This collaborative ensemble not only enhances performance but also ensures resilience and adaptability in rapidly changing application domains. Furthermore, by dismantling the idea of development as a performance race, we encourage future work to focus on accessible and powerful language modelling while stressing the benefits of collaboration from a community of researchers and developers.

With Trace-of-Thought Prompting, we seek to embody the principles of model hybridity, presenting a prompting methodology that breaks from traditional linear and recursive structures. This method is designed to maximize the utility of opensource models, directing them to operate in concert with the reasoning capabilities distilled from their larger counterparts. It is also a step towards an ensemble-oriented future, where the collective intelligence of diverse models paves the way for innovative problem-solving strategies in computational linguistics.

## 4 Trace-of-Thought Prompting

Consider a scenario where a complex problem is tackled not by a single entity but by a collaborative effort, drawing on the specialized capabilities of various participants. This is the essence of Traceof-Thought Prompting-an approach designed to split the problem-solving process into two transparent, manageable stages, leveraging the strengths of different language models.

Let us define a language model D that receives a divisible question q , which is a query that can be deconstructed into a series of simpler, intermediate subproblems. This model D produces a sequence of reasoning steps s 1 ...n within a single context:

$$D ( q ) \rightarrow s _ { 1 }, s _ { 2 }, \dots, s _ { n } \text{ \quad \ \ } ( 1 )$$

The key here is that s 1 ...n are not isolated outputs but are generated sequentially, offering a clear, stepwise path to the solution.

Next, a solver S utilizes these intermediate steps to compute the final answer a :

$$S ( q | s _ { 1 }, s _ { 2 }, \dots, s _ { n } ) \to a \text{ \quad \ \ } ( 2 )$$

Importantly, the solver S can be a different model or instance from D , providing flexibility in the problem-solving process.

The Trace-of-Thought approach comprises two distinct stages:

- 1. Delegation ( D q ( ) ) : This stage involves generating a set of concise, step-by-step instructions for tackling the problem. Users can verify and refine these steps, either manually or using a

Table 1: Prompting templates used in experimental evaluation.

| Prompt Type                   | Template                                                                                                                      |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| Standard                      | '<question>.'                                                                                                                 |
| Chain-of-Thought              | '<question>. Think step-by-step. '                                                                                            |
| Trace-of-Thought - Delegation | ' Create very short step-by-step prompts for the following problem: <question>. Format as a list. Do not solve the problem. ' |
| Trace-of-Thought - Solution   | ' We are given the following problem: <question>. Use the following steps to solve the problem: <steps>.'                     |

verifier model, before proceeding to the answering stage. This explicit segmentation ensures that any inaccuracies can be corrected early in the process (Ling et al., 2023).

- 2. Answering ( S q s , s ( | 1 2 , . . . , s n ) ) : In the second stage, a solver model uses the delegated steps to resolve the original question. The model focuses solely on the steps provided, without being prompted to engage in additional reasoning, thus sticking closely to the laid-out plan. Figure 2 contrasts this streamlined process with the potentially more convoluted pathways of previous methods.

Trace-of-Thought Prompting not only enhances the visibility of the reasoning process but also empowers users to harness the specialized abilities of a suite of models, particularly in areas like arithmetic reasoning. This promotes a more effective utilization of resources and paves the way for models to act as focused problem-solvers or adept delegators, depending on their strengths (Zhang et al., 2023).

By adhering to the principles of model hybridity, Trace-of-Thought Prompting advocates for a diverse, cooperative problem-solving landscape. It heralds a step towards the realization of an ensemble-oriented approach, where the convergence of different models' strengths leads to robust, adaptable, and highly capable NLP systems.

## 5 Experimental Setup

## 5.1 Benchmarks

For the purposes of this paper, we consider two arithmetic reasoning tasks:

- 1. GSM8K : A dataset of 8.5k grade school mathematics word problems (Cobbe et al., 2021). GSM8K was chosen for its combination of questions demanding skills at large and small scale computations and commonsense reasoning, while still remaining an entry-level benchmark.
- 2. Mathematics Aptitude Test of Heuristics (MATH) : A dataset of 12.5k challenging competition math problems (Hendrycks et al., 2021b). MATH is a dataset with historically low performance on models that do not employ code solutions, and was chosen to evaluate longer-form reasoning and reasoning on tasks often outside of the scope of some models such as WizardMath-7B (Zhou et al., 2023a).

From each of these datasets, we select the top 200 questions, either from the main body of the dataset or the test split if it is available.

## 5.2 Models

We consider a wide variety of closed source and open-source models for comparison:

GPT-4 (undisclosed size, closed-source): Closed-source model created by OpenAI. Used at the 06/13 snapshot.

GPT-3.5-Turbo (undisclosed size, closedsource): Part of the predecessor series to GPT-4, created by OpenAI. Used at the 06/13 snapshot.

WizardMath-7B (7B, open-source): An opensource suite of models produced by WizardLM, trained on Mistral, and fine-tuned on the GSM8K dataset. Trained using a novel extension of Reinforcement Learning from Human Feedback (RLHF), Reinforced Evol-Instruct (RLEIF).

Llama 2-7B Chat (7B, open-source): Part of a suite of open-source models produced by Meta and fine-tuned for dialogue purposes.

Zephyr-7B (7B, open-source): An open-source model developed by HuggingFace's H4 team, trained on synthetic data using Direct Preference Optimization (Rafailov et al., 2023).

## 5.3 Prompting Approaches

We compare three primary prompting approaches on all datasets, all in a zero-shot environment - a prompting environment chosen for emulation of

Figure 3: Accuracy of various models on GSM8K using three prompting approaches.

<!-- image -->

the typical human-model interaction in the fairest possible form:

Standard Prompting : Models are prompted with the exact content of the dataset, unaltered.

Chain-of-Thought Prompting : Models are given a problem and instructed to 'think step-bystep,' with no prior contextual examples.

Trace-of-Thought Prompting : A Delegation model and Solution model are chosen, and we execute Trace-of-Thought's delegation &amp; answer workflow. Again, no prior contextual examples are given. For delegation, we employ GPT-4 in every instance of Trace-of-Thought. 1

To illustrate the operational differences between these prompting approaches, Figure 2 provides a comparative workflow diagram, which can be referenced for a visual understanding of each method's structure. For a detailed comparison of the prompting templates used see Table 1.

## 5.4 Evaluation

Answers are evaluated for exact match accuracy against dataset labels; 'close 'answers are not considered unless rounded by the model. Results, including questions, model and dataset answers, and for Trace-of-Thought, steps and answers, are recorded in a CSV file. Each response is marked as true or false by an in-house annotator.

If models arrived at the correct answer, but added further information such as needless additional results or follow-up questions, we indicate the an-

1 Our use of GPT-4 as the delegation model is based on its proven reasoning abilities and convenience for our study, not a requirement for the model to be large or closed-source. Future work is encouraged to investigate open-source alternatives for Trace-of-Thought to enhance cost-efficiency in language modeling.

Figure 4: Accuracy of various models on MATH using three prompting approaches.

<!-- image -->

swer as correct but note the occurrence for further analysis. Additionally, in ambiguous cases where models correctly reasoned the answer, but potentially returned a different answer during the answer formatting process, we indicate the question as correct with an indication for further analysis. For example, if a model arrives at a result of 5 during reasoning, but wrongfully says 'The answer is: 6, 'we credit the reasoning for 5 and not the formatted answer of 6.

## 6 Results

## 6.1 GSM8K

Figure 3 depicts each model's performance on GSM8K ( n = 200 ) using all three approaches.

As model size decreases, we observe larger, statistically significant relative gains in performance through the employment of Trace-of-Thought Prompting. Although GPT-4 sees an overall minimal loss, GPT-3.5-Turbo sees accuracy gains of 17%, Llama 2-7B Chat sees gains of over 125%, while other models see accuracy gains ranging from 14% to 50%. Similarly, in Tables 2 and 3, we see tighter accuracy ranges, a greater mean and highly consistent accuracy gains, indicating an encouraging trend in general improvements to accuracy across all models. When employing WizardMath with Trace-of-Thought, we observe gains approaching that of GPT-4, a strong example of leveraging GPT-4's reasoning capacities with WizardMath's arithmetic fine-tuning.

Tabular results and statistical significance tests for GSM8K can be found in Appendix Tables 5 and 7, respectively.

Table 2: Accuracy gain comparison across tested models on GSM8K and MATH ( n = 200 ).

| Model           |   Highest Alternative |   Trace-of-Thought | %Change   | Model           |   Highest Alternative |   Trace-of-Thought | %Change   |
|-----------------|-----------------------|--------------------|-----------|-----------------|-----------------------|--------------------|-----------|
| GPT-4           |                  94.5 |               93.5 | -1.06%    | GPT-4           |                  66   |               70.5 | + 6.82 %  |
| GPT-3.5-Turbo   |                  75.5 |               86   | + 13.91%  | GPT-3.5-Turbo   |                  75.5 |               86   | + 12.5%   |
| WizardMath-7B   |                  73.5 |               84.5 | + 14.97%  | WizardMath-7B   |                  44.5 |               40.5 | -8.99%    |
| Llama 2-7B Chat |                  23.5 |               53   | + 125.53% | Llama 2-7B Chat |                   7.5 |                8.5 | + 13.33%  |
| Zephyr-7B       |                  12   |               18   | + 50%     | Zephyr-7B       |                  26   |               49.5 | + 90.38%  |

Table 3: Measures of central tendency for the accuracies for each prompting approach, using GSM8K ( n = 200 ).

| Approach           |   Min |   Max |   Mean |
|--------------------|-------|-------|--------|
| Standard Prompting |  22   |  94.5 |   57.4 |
| Chain-of-Thought   |  23.5 |  93.5 |   57.5 |
| Trace-of-Thought   |  49.5 |  93.5 |   73.3 |

| Approach           |   Min |   Max |   Mean |
|--------------------|-------|-------|--------|
| Standard Prompting |   6.5 |  57.5 |   32.4 |
| Chain-of-Thought   |   7.5 |  66   |   34.2 |
| Trace-of-Thought   |   8.5 |  70.5 |   39.2 |

## 6.2 MATH

Figure 4 depicts each model's performance on MATH ( n = 200 ) using all three approaches.

While gains on closed-source models such as GPT-4 and GPT-3.5 lack statistical significance, their improvement-especially that of GPT4, which benefits from being prompted by itselfsignifies a strong trend towards the accuracy benefits of employing Trace-of-Thought as a general solution, even on large, closed-source models. Additionally, while Trace-of-Thought fails to draw equivalence to other approaches on WizardMath7B, we continue to see gains on historically poorperforming models Llama 2 Chat and Zephyr. The usage of the MATH dataset upholds a similar trend of tighter accuracy ranges and performance gains, as observed in Tables 2 and 4. The extent to which this initial iteration of Trace-of-Thought can be applied remains to be seen, but these results confidently support the notion of, at minimum, partially enhanced arithmetic problem solving on a dataset of difficult questions.

Tabular results and statistical significance tests for MATH can be found in Appendix Tables 6 and 8, respectively.

## 7 Extended Analysis

## 7.1 Reasoning Enhancements

In using Trace-of-Thought with small-scale language models, we observed two trends of interest pertaining to a small-scale model's enhancement of provided steps with its own reasoning.

Firstly, as seen in Figure 6, open-source language models identified and subsequently ignored redundant steps, retaining transparency while also preserving reasoning and reducing the chance of hallucinations. By identifying unnecessary pro-

Table 4: Measures of central tendency for the accuracies for each prompting approach, using MATH ( n = 200 ).

cesses or processes with a net-zero gain, the model exemplifies problem solving that is more involved than just instruction following, but rather problem solving that uses the steps as a supervisory framework.

Additionally, when a model encounters a step that is further divisible given the requisite information-whether from a prior step or from observing the question context-it may choose to enhance the provided steps with further reasoning chains, similar to that of a recursive reasoning chain, as seen in Figure 5 (Lee and Kim, 2023). By further introducing local reasoning, the model fills in the blanks, or adds what it deems to be necessary question context.

While prior work has investigated a similar evaluation of steps as a post-hoc method, this phenomenon leverages the same context where answer generation takes place; by analyzing and correcting steps in place, there is no need for additional verification post-hoc, and instead the model can course correct as necessary (Miao et al., 2023).

## 7.2 Reflexive Trace-of-Thought

When employing GPT-4 on the MATH dataset as both a delegation model and solution model, we observed significant gains in accuracy despite no other models being used for that series of tests (Figure 4). This reflexive use of GPT-4, while not statistically significant, articulates the benefits of deliberate reasoning regardless of the model employed for the task.

This approach, which we call reflexive Traceof-Thought , emphasizes less the abilities of model ensembles and instead harnesses the power of deliberate problem solving previously applied in approaches such as Tree of Thoughts (Yao et al.,

## Prompting Step

## Solution

Figure 5: Zephyr-7B using step enhancement to execute local reasoning chains - i.e, while solving an equation.

<!-- image -->

## Step Prompting

## Solution

Figure 6: Zephyr-7B using redundant step identification to ignore useless work - i.e, while simplifying a result.

<!-- image -->

2023). The implications raised by incremental success in using reflexive Trace-of-Thought opens further consideration for using larger, open-source models for both the tasks of delegation and question answering. This shift away from closed-source modelling solutions is a critical paradigm in computational linguistics, empowering research firms to develop models not just for problem solving, but for deliberate reasoning and instruction generation.

the solution model to adhere to the structure and desired steps, even if they can be altered.

## 7.3 Transparent Reasoning

While existing approaches such as Chain-ofThought prompting encourage step-by-step reasoning, it can be difficult to properly segment these steps into distinct parts of the whole solution. We find that Trace-of-Thought prompting encourages models to adhere to the defined step structure, including aligning their solutions with the step number for better observability, even if the step is redundant or missing.

Cases such as Figure 7 (Appendix) depict the differences in transparency between both approaches when utilizing GPT-3.5-Turbo. Crucially, Traceof-Thought encourages observance of the provided steps guide for reasoning. While Chain-of-Thought develops this reasoning chain as the answer is rendered, Trace-of-Thought sets clear expectations on

This clear window into reasoning allows not only judgement of a solution model's efficacy at certain targeted tasks, but can also lend some insight into the necessity of some steps generated by the delegation model. Combined with a solution model's ability to sometimes omit these redundant steps altogether, this is a strong trend that encourages careful and fine-grained tuning of the delegation process.

## 8 Conclusion

We introduced Trace-of-Thought Prompting, a novel prompting approach that seeks to decompose problems into a delegation and answering step, and we analyzed its performance when applied to smallscale langauge models using delegation from GPT4. Trace-of-Thought Prompting seeks to encourage a paradigm shift towards accessible and opensource language modeling, and suggests that large language models like GPT-4 may have future utility as delegators and not problem-solvers. Our findings implore the community to consider the advantages of accessible, inclusive, and democratized research, and to develop future approaches with opensource models at the forefront of consideration.

## Limitations

Prompt sensitivity. There may be the potential for degraded performance when omitting certain parts of either the delegation or answer prompt that remains unexplored. We encourage a thorough saliency study of the structure of these prompts to prevent this degradation.

Abstract reasoning. Datasets such as ARC and ACRE explore abstract reasoning tasks, where the aim is to generalize and apply a common pattern across very little data (Xu et al., 2024; Gendron et al., 2024). Due to their abstract nature, and the lack of observable divisions of the original problem, it remains to be seen whether Trace-of-Thought is a valid and useful framework for conducting tasks such as general pattern recognition or commonsense reasoning.

Financial impact. Due to the extra prompt necessary for generating steps for the solution model, and the lengthier input prompts necessary to achieve this effect, applying Trace-of-Thought in a primarily closed-source setting may lead to additional costs and heightened usage of restricted resources such as API credits toward rate limits.

Solution diffusion from delegation model. Similar to prompt sensitivity, users should be mindful that prompting their delegation model too strongly towards encouraging a solution may lead to the generation of steps as a result of prior work, not as a direction for the solution model, leading to contaminated results downstream.

Model dependence and scalability. The effectiveness of Trace-of-Thought heavily relies on the selected models' capabilities, potentially limiting its application in areas where models lack specific knowledge or reasoning skills. Additionally, the method's computational and resource demands may pose scalability challenges in resourceconstrained settings.
<|endofpaper|>