<|startofpaper|>
## ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification

Ziqing Fan 1,2, ∗ , Cheng Liang 2, ∗ , Chaoyi Wu 1,2 ,

Ya Zhang 1,2 , Yanfeng Wang 1,2 and Weidi Xie 1,2

1 Shanghai Jiao Tong University, Shanghai, China

2

Shanghai Artificial Intelligence Laboratory, Shanghai, China ∗ Equal contributions

Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner , a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR , a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore , a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.

## 1 Introduction

In the recent literature, the reasoning-enhanced large language models (LLMs), such as OpenAI-o3 [1] and DeepSeek-R1 [2], have achieved tremendous success in tasks like mathematics and programming [3, 4, 5]. Similarly, advancements in multimodal large language models (MLLMs) have been demonstrated in reasoning tasks [6, 7]. Inspired by the success of chain-of-thoughts (CoTs), these models aim to generate intermediate reasoning steps before presenting final outcomes, enhancing interpretability and performance [8, 9].

In medical applications, developing artificial intelligence (AI) models with reasoning abilities is particularly well-suited, as the complexity of medical data and the analytical rigor for accurate diagnosis demand reasoning. In particular, radiologists often follow a systematic, step-by-step approach to image analysis, adhering to clinical guidelines [10, 11, 12]. As shown in Fig. 1(a), this involves identifying low-level anomalies, integrating patient-specific information ( e.g. , chief complaints), and inferring potential diagnosis. These structured reasoning workflows are meticulously documented in clinical reports, progressing logically from findings to impressions as dictated by clinical standards. Despite this, existing AI research in medical reasoning studies [13, 14] fails to leverage this structured prior knowledge, and often rely on outcome-based RL methods, neglecting the rich reasoning supervision embedded in clinical reports.

In this paper, we aim to develop a radiology diagnosis model with reasoning capabilities, by mining and utilizing process supervision from clinical reports.

For dataset construction , we highlight that daily radiology reports are an invaluable resource for constructing high-quality reasoning chains, as they typically include doctors' findings, and impressions. To this end, as illustrated in Fig. 1(b) and Fig. 2(a), we propose to construct diverse question-answer pairs based on impressions, and extracts clinically relevant observations in findings as the reasoning processes. We leverage tools like GPT-4o [15] to automate the labor-intensive tasks, for example, generating question-answer pairs, extracting clinical observations, and refining them into complete, clinically coherent reasoning chains. Compared to the

## a. Reasoning Workflow in Radiology Clinical Report

<!-- image -->

## b. RadRBench-CXR Mined fro m Reports

## n c. ChestX-Reasoner through Process Supervisio

<!-- image -->

## d. Evaluation Results

LLaVA-Next-8B

<!-- image -->

RadFM-RSuper-7B

Figure 1 | Mining Reasoning from Clinical Reports for Enhancing MLLMs. a. The reasoning workflow in radiologists' clinical report writing from low-level patterns of clinical observations to high-level disease identification. b. Mined reasoning samples from clinical reports in conjunction with GPT-4o. c. A two-stage training pipeline that incorporates process supervision to develop our reasoning-enhanced MLLM ChestX-Reasoner. d. An overview of evaluation results on RadRBench-CXR in terms of both reasoning ability and outcome accuracy, compared to extensive baseline models.

collecting reasoning processes from large visual-language models or relying on manual annotation, extracting reasoning chains directly from textual reports is both scalable and highly reliable.

For model development , we present a generative radiology model with reasoning abilities, termed as ChestX-Reasoner . As illustrated in Fig. 1(c), we adopt a two-stage training framework that is designed to harness the medical reasoning process supervision [16]. It starts with supervised fine-tuning (SFT) as a cold start, leveraging both answer-only and reasoning-augmented data for auto-regressive training. It then employs reinforcement learning (RL) with additionally designed process supervision via process reward, that evaluates the correctness of the generated clinical observations compared to the ground-truth report sentences, ensuring a more transparent and reliable training process.

For model evaluation , we curate a reliable reasoning benchmark on Chest X-rays, termed as RadRBenchCXR , that features detailed diagnostic reasoning processes. It includes 59K visual question answering (VQA) samples from three public datasets, namely, MIMIC-CXR [17], ChexPert [18], and MS-CXR-T [19]. The questions span diverse task types that require complex reasoning chains. Each sample is augmented with reasoning processes mined from its corresponding clinical report, comprising 301k clinically validated reasoning steps. Additionally, we present an evaluation metric, RadRScore , that assesses a model's reasoning capabilities across three key dimensions: factuality (the correctness of generated reasoning), completeness (the thoroughness in covering clinical findings), and effectiveness (the necessity and relevance of diagnostic processes).

As a result, our proposed ChestX-Reasoner is a multimodal large language model with medical reasoning for Chest X-ray diagnosis. We compare it against three medical foundation models and five general-domain foundation models in terms of outcome accuracy and reasoning ability, using RadRScore on both RadRBenchCXR and the public CheXbench [20]. As demonstrated in Fig. 1(d), ChestX-Reasoner achieves significant improvements compared to all baselines. Specifically, it achieves 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical baseline, the best general baseline, and its base model (Qwen2VL-7B [21]), respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All code, datasets, and models are open-sourced at ChestX-Reasoner, and we hope this work inspires further research into developing medical reasoning MLLMs.

## 2 Benchmarks

We first present the details for constructing the benchmark, namely, RadRBench-CXR , with ground-truth reasoning steps mined from clinical reports. In addition to the final outcome accuracy that measures the correctness of generated answer compared to the ground truth answer, we also provide an evaluation metric to measure the reasoning ability, termed as RadRScore .

Collecting visual question answering (VQA) samples. As illustrated in Fig. 3 (a), we collect VQA samples based on the question design framework of CheXbench [20], which defines comprehensive clinical tasks and generation methods. Specifically, we focus on five diagnostic task types:

- ◦ Binary disease diagnosis: yes/no questions about disease presence.
- ◦ Single disease diagnosis: multi-choice questions with four options, each representing a disease.
- ◦ Multiple disease diagnosis: multi-choice questions with four options, each listing disease combinations, requiring the most clinically relevant answer.
- ◦ Anomaly detection: open-ended tasks where models must identify all abnormal findings without predefined options.
- ◦ Temporal comparison analysis: disease progression questions using current and prior images, requiring models to determine if a symptom is stable, worsening, or improving.

The samples are drawn from three datasets-MIMIC-CXR [17], CheXpert [18], and MS-CXR-T [19]-all paired with clinical reports. To evaluate the performance of our reasoning-enhanced model across multiple clinical centers, we additionally utilize two datasets, RSNA [22] and SIIM [23], that do not provide associated clinical reports.

Mining reasoning chains from reports. As illustrated in Fig. 1(b), and Fig. 2(a), for a subset of VQA samples, we propose to mine the reasoning chains for diagnosis from clinical reports with three key steps:

## a. Procedures of Mining Reasoning

<!-- image -->

## b. Calculation of RadRScore

Figure 2 | Reasoning Mining Procedures and RadRScore Calculation a. Procedures of mining reasoning. We first construct reasoning plans by prompting GPT-4o based on given QA pairs and clinical reports, then extract relative clinical observations from the report as answers for each plan to derive the reasoning steps, and finally, refine the logic to form coherent and complete reasoning chains. b. RadRScore calculation targeting on factuality (the correctness of generated reasoning), completeness (the thoroughness in covering clinical findings), and effectiveness (the necessity and relevance of diagnostic processes).

- ◦ Constructing reasoning plans: Given a question-answer pair and the corresponding clinical report, we prompt GPT-4o to generate structured reasoning plans that systematically outline the diagnostic steps leading to the conclusion, for example, the specific symptom or clinical observation.
- ◦ Extracting diagnostic evidences: For each reasoning plan, we extract diagnostic evidence from the medical report to construct a single diagnostic reasoning chain. If the report lacks explicit mention of a finding, we infer the response as 'normal' or 'no disease', given the assumption that physicians do not miss abnormal findings in their reports.
- ◦ Optimizing logical chains: Finally, we integrate all reasoning plans and their extracted answers, refining the logical flow to form a coherent diagnostic reasoning chain.

To streamline the process, we utilize GPT-4o [15] to automate the labor-intensive tasks, including constructing reasoning plans, extracting answers from medical reports, and refining diagnostic narratives.

Evaluating reasoning with RadRScore. To quantitatively measure the quality of model's intermediate rationales, we deign an automatic evaluation pipeline and a metric, termed as RadRScore . As illustrated in Fig. 2(b), we first employ large language models (GPT-4o) to extract entities, that belong to clinical observations or findings. For example, from reasoning sentences: 'Based on the given images, there are no visible signs of lung tissue collapse or an enlarged heart. Besides, there are no signs of pneumothorax...', we can extract entity set { no lung tissue collapse, no enlarged heart, no pneumothorax } .

We use obs model , obs gt , and obs report to respectively denote the extracted entity set from the model's output, ground-truth reasoning, and corresponding report. Then, we calculate the values of following three dimensions, namely factuality, completeness, and effectiveness based on obs model , obs gt , and obs report :

- ◦ Factuality ( R f ): It is calculated as the proportion of elements in obs model that are semantically matching with report via GPT-4o. This dimension assesses the factual accuracy of the model's reasoning,

ensuring no hallucinated or medically invalid content is included. It can be formulated as:

$$R _ { f } = \frac { \L o b s _ { \text{model} } \cap \L o b s _ { \text{report} } } { | \L o b s _ { \text{model} } | }.$$

- ◦ Completeness ( R c ): It is calculated as the proportion of elements in obs gt that are semantically matching with obs model , evaluating how comprehensive the model's reasoning is. It can be formulated as:

$$R _ { c } = \frac { \L o b s _ { g t } \cap \L o b s _ { \text{model} } } { | \L o b s _ { g t } | }.$$

- ◦ Effectiveness ( R e ): It is calculated as the proportion of reasoning steps that are semantically included in the ground truth reasoning. Since ground truth reasoning is concise and comprehensive for reaching the final answer, a predicted step is considered effective if it matches one of these steps. It evaluates whether each reasoning step advances the solution in a meaningful way. It can be formulated as:

$$R _ { e } = \frac { \L o b s _ { \text{model} } \cap \L o b s _ { \text{gt} } } { | \L o b s _ { \text{model} } | }.$$

Notably, when calculating R f , if an element in obs model is not found in obs report , and its meaning contains either 'normal' or 'no disease', we consider it to be correct, since we assume the physicians may overlook normal observations but typically do not miss abnormal findings in their reports. Finally, RadRScore is then calculated as the mean value of the three dimensions:

$$\text{RadRScore} = ( R _ { f } + R _ { c } + R _ { e } ) / 3$$

RadRBench-CXR compilation. Leveraging the generated VQA samples, mined reasoning chains, and evaluation metrics, we construct the first visual-language reasoning benchmark, RadRBench-CXR .

To create a balanced and unbiased dataset, we incorporate a diverse range of diseases, organs, and findings while ensuring variety across samples. For the balancing strategy, we analyze the disease distribution in the original data and adjust the quantities, such that the most frequent disease does not exceed twice the least frequent ones, promoting fairness in the dataset.

Specifically, the benchmark comprises 1.2M answer-only VQA samples for training and 7.3K samples for testing. For a subset of these, we generate 59K reasoning-augmented VQA samples for training and 1K reasoning-augmented samples for testing. We apply strict quality control to the VQA samples with reasoning chains. Samples with a factuality score R f lower than 1 are filtered out, ensuring that only high-quality, factual reasoning chains are retained in our benchmark. Notably, prior to factuality filtering, the generated reasoning processes achieve a significantly higher factuality score ( R f ) of 0.82, compared to GPT-4o (0.60) and Qwen2VL-72B (0.59), demonstrating the effectiveness of our method to produce high quality reasoning samples. The detailed cases of instruction design, and mined reasoning processes are shown in Supplementary 6.3.

## 3 Results: Reasoning Enhanced Medical VLM (ChestX-Reasoner)

In this section, we compare to various models on the proposed RadRBench-CXR benchmark, emphasizing the superiority of our proposed model over existing multimodal large language models (MLLMs) in terms of enhanced reasoning ability and outcome accuracy.

The comparison includes three medical MLLMs-RadFM-14B [24], CheXagent-3B [20], and MedDr-40B [25], and five general MLLMs: DeepSeek-VL-7B [26], LLaVA-NeXT-8B [27], Qwen2VL-7B, Qwen2VL-72B [21], and GPT-4o [15]. The evaluation encompasses the five tasks explained in Sec. 2, namely, binary disease diagnosis, single disease diagnosis, multiple disease diagnosis, anomaly detection, and temporal comparison analysis.

Due to the lack of clinical reports in SIIM [23] and RSNA [22], no mined reasoning processes or process supervision were applied to samples from these datasets during our training pipeline. We use these two datasets for cross-center validation. Additionally, we provide an ablation study on the impact of different

Figure 3 | Evaluation Results of Reasoning Ability. a. Question-answering pairs visualization of all task types. b. Reasoning abilities on dimensions of factuality, completeness, and effectiveness, as well as the averaged value, RadRScore.

<!-- image -->

training strategies, underscoring the critical role of process supervision in achieving superior outcomes.

## 3.1 Comparison on Reasoning Ability

While evaluating the reasoning ability of ChestX-Reasoner on the five tasks (Fig. 3(a)) sourced from MIMICCXR [17], ChexPert [18], and MS-CXR-T [19], we used chain-of-thoughts (CoT) prompt to elicit reasoning from MLLMs and evaluated each model with RadRScore, measuring factuality, completeness, and effectiveness of the reasoning.

ChestX-Reasoner (7B) consistently outperformed all other MLLMs across tasks, showing an average RadRScore improvement of 18% over its base model, Qwen2VL-7B (from 0.349 to 0.531; see last column of Fig. 3(b)).

Among general domain models, GPT-4o and Qwen2VL-72B achieved RadRScores of 0.472 and 0.407, respectively. The best medical domain baseline, MedDr-40B, reached 0.367. Although CheXagent-3B produced competing outcome accuracy compared to ours, its reasoning scores were near zero across all tasks.

Factuality. As shown in the first column of Fig. 3(b), ChestX-Reasoner achieved the following scores: 0.660 (95%CI = 0.603-0.712) on binary disease diagnosis, 0.741 (95%CI = 0.672-0.802) on single disease diagnosis, 0.719 (95%CI = 0.691-0.746) on multiple disease diagnosis, 0.517 (95%CI = 0.466-0.566) on anomaly detection, and 0.715 (95%CI = 0.626-0.800) on temporal comparison analysis. These results outperform the best general MLLM, GPT-4o with average scores of 0.534, 0.617, 0.554, 0.460, and 0.532 on the five tasks, respectively. Additionally, ChestX-Reasoner also outperforms the best medical MLLM, MedDr-40B, which achieved average scores of 0.510, 0.443, 0.508, 0.348, and 0.589 on the tasks. These findings demonstrate that ChestX-Reasoner exhibits superior clinical factual correctness compared to other baselines.

Completeness. As shown in the second column of Fig. 3(b), ChestX-Reasoner achieved the following scores: 0.248 (95%CI = 0.213-0.281) on binary disease diagnosis, 0.438 (95%CI = 0.372-0.507) on single disease diagnosis, 0.503 (95%CI = 0.476-0.531) on multiple disease diagnosis, 0.298 (95%CI = 0.260-0.339) on anomaly detection, and 0.284 (95%CI = 0.221-0.347) on temporal comparison analysis. Among the baselines, general-domain MLLMs like GPT-4o and Qwen2VL-72B achieved comparable performance, except for the anomaly detection task. Qwen2VL-72B failed in anomaly detection due to the lack of given options. In contrast, medical-domain MLLMs performed significantly worse in this dimension. The best-performing medical MLLM, MedDr-40B, achieved completeness scores of only 0.155 on binary disease diagnosis, 0.171 on single disease diagnosis, 0.291 on multiple disease diagnosis, 0.156 on anomaly detection, and 0.082 on temporal comparison analysis. These results underscore the superior thoroughness of ChestX-Reasoner in covering clinical findings while highlighting the limitations of current medical-domain MLLMs.

Effectiveness. As shown in the third column of Fig. 3(b), ChestX-Reasoner achieved the following scores: 0.619 (95%CI = 0.563-0.674) on binary disease diagnosis, 0.690 (95%CI = 0.614-0.756) on single disease diagnosis, 0.547 (95%CI = 0.510-0.579) on multiple disease diagnosis, 0.399 (95%CI = 0.333-0.472) on anomaly detection, and 0.630 (95%CI = 0.537-0.728) on temporal comparison analysis. In this dimension, the medical-domain MLLM, MedDR-40B, achieves performance comparable to the best general-domain MLLM, GPT-4o, but still falls significantly short of ChestX-Reasoner. Our ChestX-Reasoner outperforms both GPT-4o and MedDR-40B, with improvements of 5.4% and 24.8%, respectively. These results underscore the superior necessity and relevance of the diagnostic processes generated by our model.

## 3.2 Comparison on Outcome Accuracy

Next, we evaluated the ability of ChestX-Reasoner to output correct outcome after generating reasoning processes on the same five tasks (Fig. 4(b)) sourced from MIMIC-CXR [17], ChexPert [18], MS-CXR-T [19], SIIM [23], and RSNA [22] as shown in Fig. 4(c). Since anomaly detection is an open-ended generation task, we adopt RaTEscore [28] for evaluation, while for other close-ended tasks with provided options, we directly verify whether the generated answer is the same with the ground truth label. Results demonstrated that ChestX-Reasoner achieves superior performance compared to all baselines with 3% overall improvements compared to the existing state-of-the-art MLLM (CheXagent-3B), which was trained on significantly more data (8.5M training samples compared to 1.2M samples used in ChestX-Reasoner). Additionally, ChestX-Reasoner outperforms its base model (Qwen2VL-7B) with 27% overall improvements, and achieves 24% averaged improvements comapred to best general domain MLLM GPT-4o.

Disease diagnosis. For all the disease diagnosis tasks, our model achieves comparable performance to existing state-of-the-art model (CheXagent-3B), with accuracy of 0.800 (95%CI = 0.776-0.821) on binary disease diagnosis, 0.688 (95%CI = 0.641-0.732) on single disease diagnosis, and 0.785 (95%CI = 0.735-0.831) on multiple disease diagnosis. Among general domain MLLM, GPT-4o achieves the best performance with accuracy of 0.620 on binary disease diagnosis, 0.498 on single disease diagnosis, and 0.492 on multiple disease diagnosis, comparable to MedDr-40B, but much worse than our ChestX-Reasoner. This demonstrate the superior performance of our model on solving the disease diagnosis tasks.

Anomaly detection. Here, our model greatly improves CheXagent-3B, with RaTEscore of 0.621 (95% CI = 0.608 - 0.633) compared to 0.529, achieving 17% improvements. General domain MLLMs like GPT-4o and

Figure 4 | Evaluation Results of Outcome Accuracy and Data Statistics. a. Outcome accuracy evaluated on five tasks.

<!-- image -->

b. Train and test data distribution from different data sources. c. Task distribution from different data sources.

Qwen2VL-72B achieves RaTEscore of 0.396 and 0.372, which is even worse than the LLaVA-NeXT-8B of 0.419. This comparison shows that ChestX-Reasoner also performs superior on open-ended generation task compared to all other baselines.

Temporal comparison analysis. Our model achieves comparable performance compared to CheXagent-3B, with an accuracy of 0.661 (95%CI = 0.537-0.778). Among general MLLMs and other medical MLLMs,

Figure 5 | Ablation studies on both reasoning ability and outcome accuracy of our trained models including ChestX-ReasonerSFT(RO), ChestX-Reasoner-SFT(Both), ChestX-Reasoner-RL(O), ChestX-Reasoner-SFT(RO)+RL(O), ChestX-Reasoner ( w.o. Progress Reward), and the final ChestX-Reasoner.

<!-- image -->

DeepSeek-VL-7B and MedDR-40B perform best, with an accuracy of 0.484 and 0.387, which remains inferior to our model.

## 3.3 Training Strategy Ablation

Recent reasoning-enhanced training strategies, prevalent in mathematics and programming, include supervised fine-tuning (SFT) on reasoning samples, outcome-based reinforcement learning (RL) on answer-only samples with chain-of-thoughts (CoT) prompt, and DeepSeek-R1's two-stage approach (SFT followed by RL). However, these methods are largely unexplored for medical especially radiology MLLMs.

To address this gap, we conducted an ablation study (Fig. 5) comparing various strategies, resulting in models:

- ◦ ChestX-Reasoner-SFT(RO) : Model trained on only reasoning-augmented samples with SFT.
- ◦ ChestX-Reasoner-SFT(Both) : Model trained on both reasoning-augmented samples and answer-only samples with SFT.
- ◦ ChestX-Reasoner-RL(O) : Model trained on answer-only samples with outcome-based RL.
- ◦ ChestX-Reasoner-SFT(RO)+RL(O) : Model trained on reasoning-augmented samples with SFT, followed by outcome-based RL on answer-only samples.
- ◦ ChestX-Reasoner ( w.o. PR) : Model trained on both reasoning-augmented and answer-only samples with SFT, followed by outcome-based RL on answer-only samples.
- ◦ ChestX-Reasoner : Our final model, trained on both reasoning-augmented and answer-only samples with SFT followed by RL on both types of samples with additional process reward. See details in Sec. 5.

Notably, reasoning-augmented samples are data with clinical report and mined reasoning sentences, while answer-only samples are data without mined reasoning and clinical reports.

SFT and RL are both necessary. As shown in Fig. 5, models trained solely with outcome-based RL (ChestX-Reasoner-RL(O), averaged RadRScore: 0.343, 95% CI: 0.347-0.362; outcome accuracy: 0.471, 95% CI: 0.452-0.491) or with SFT on reasoning-augmented samples alone (ChestX-Reasoner-SFT(RO), averaged RadRScore: 0.423, 95% CI: 0.412-0.434; outcome accuracy: 0.566, 95% CI: 0.545-0.586) both perform poorly. SFT alone is limited by its reliance on exact ground-truth replication, requiring extensive reasoning processes, while RL alone lacks necessary reasoning knowledge in pre-trained base MLLMs. These findings indicate that combining SFT and RL is crucial for developing effective medical reasoning MLLMs.

Answer supervision is helpful for domain alignment before enhancing reasoning. As shown in Fig. 5,

ChestX-Reasoner-SFT(Both) achieves 5% improvements in reasoning and 19.9% improvements in outcome accuracy compared to ChestX-Reasoner-SFT(RO). Additionally, starting from ChestX-Reasoner-SFT(Both), ChestX-Reasoner ( w.o. process reward) achieves 14.5% improvements in reasoning and 9.5% improvements in outcome accuracy over ChestX-Reasoner-SFT(RO)+RL(O), which begins from ChestX-Reasoner-SFT(RO). These results demonstrate that leveraging extensive answer supervision from answer-only samples during SFT is beneficial for medical domain alignment, thereby producing better reasoning MLLMs in subsequent steps.

Process supervision is essential for reasoning ability. Adding process supervision further enhances reasoning ability. ChestX-Reasoner achieves an averaged RadRScore of 0.525 (95% CI: 0.515-0.536), a 2.5% improvement over ChestX-Reasoner ( w.o. process reward; 0.512, 95% CI: 0.501-0.513). The enhanced reasoning ability also translates into improved outcome accuracy. These results underscore the importance of process supervision in boosting the reasoning performance of medical MLLMs, ultimately leading to better diagnostic accuracy.

## 4 Discussion

This study systematically investigates the development of medical reasoning-enhanced multimodal foundation models. Unlike general computer vision, where visual reasoning is often implicit [7, 6], medical imaging analysis involves explicit, standardized reasoning workflows. Radiologists routinely document detailed reasoning in clinical reports, logically progressing from findings to diagnostic conclusions-offering valuable process supervision signals. Based on this insight, our main contributions are threefold:

A scalable reasoning mining pipeline using clinical reports. Unlike prior approaches that rely solely on chain-of-thoughts distillation [29, 30, 31], our pipeline extracts reasoning cases directly from manually written clinical reports, ensuring higher factual accuracy and verifiability. While demonstrated here with Chest X-ray reports, this method can be readily extended to other radiology modalities, providing a scalable and cost-effective source of validated reasoning samples for clinical AI development.

ChestX-Reasoner, a process-supervised radiology foundation model with reasoning. We introduce ChestX-Reasoner, a multimodal large language model trained with stepwise process supervision via process reward to mirror the structured reasoning of clinical diagnostics. ChestX-Reasoner achieves state-of-theart performance in both reasoning and outcome accuracy on Chest X-ray diagnostic tasks, outperforming general models (DeepSeek-VL-7B [26], LLaVA-NeXT-8B [27], GPT-4o [15], Qwen2VL-7B [21], and Qwen2VL72B [21]) and specialized medical models (RadFM-14B [24], MedDr-40B [25], CheXagent-3B [20]) across all five benchmark tasks.

RadRBench-CXR, a reliable reasoning-focused radiology benchmark. For evaluation, we introduce RadRBench-CXR, derived from real clinical reports and designed to assess both outcomes and reasoning quality. Beyond outcome accuracy, our RadRScore comprehensively evaluates generated reasoning in terms of factuality, completeness, and effectiveness, providing a robust metric for benchmarking medical MLLMs.

Based on our experimental results, we derive the following key findings:

LLMs can transform clinical reports into reasoning samples across various radiology tasks. The development of medical reasoning MLLMs is often constrained by the high cost and limited availability of high-quality, reliable multimodal reasoning datasets. In this work, we demonstrate that existing clinical reports, combined with effective prompt engineering, enable current LLMs to automatically generate diverse, wellstructured reasoning-labeled training instances across a broad range of radiology tasks. Notably, our generated reasoning, based on clinical reports, achieves a significantly higher factuality score (the correctness of generated reasoning defined in RadRScore) of 0.82, substantially outperforming GPT-4o (0.60) and QwenVL-72B (0.59).

Enhancing the initial medical capabilities of base MLLMs via SFT as a cold-start is necessary. General-domain base MLLMs often lack the specialized medical knowledge required for clinical reasoning tasks. Our experiments reveal that directly applying reinforcement learning (RL) to such models, e.g. , ChestX-Reasoner-RL(O), results in poor performance and unstable optimization. This underscores the necessity of initializing general-domain MLLMs with medical knowledge through domain-specific pretraining or task-adaptive supervised fine-tuning before applying RL. Without this initialization, models are prone to convergence failures and cannot reliably develop clinical reasoning skills.

Process rewards are critical for RL in developing medical reasoning MLLMs. While recent advances in general domain reasoning MLLMs have inspired medical MLLMs [13, 14] to adopt outcome-based RL strategies, these rely on self-exploration without explicit reasoning supervision. However, due to the complexity and domain-specificity of medical reasoning, this approach proves unreliable. In this paper, we demonstrate that process rewards are critical in the medical imaging domain, significantly enhancing the quality of the reasoning process. Our model, ChestX-Reasoner, achieves notable improvements in both reasoning ability and outcome accuracy compared to its counterpart without process supervision, as shown in Fig. 5.

These findings can inspire future research in key areas, including the development of robust and generalizable medical reasoning foundation models, scalable generation of clinically validated reasoning data, and the adaptation of process supervision techniques to enhance medical reasoning generation.

From a clinical perspective, our process-supervised medical reasoning model, ChestX-Reasoner, offers significant advantages over prior outcome-only models. By explicitly demonstrating step-by-step imaging interpretation, it produces outputs that are more interpretable to clinicians and better aligned with clinical workflows. This enables transparent decision-support evidence in radiology tasks such as reporting, differential diagnosis, and multidisciplinary case discussions. Additionally, the improved factual consistency and reasoning traceability enhance auditability, reducing challenges in human-AI interaction within clinical practice. As a proof of concept, ChestX-Reasoner highlights a more effective approach to developing reasoning-enhanced medical foundation models, paving the way for integration into real-world clinical systems where explainability, reliability, and safety are essential.

Lastly, we have to emphasize that our study presents several limitations , which highlight the opportunities for future work. First, we focus on chest X-rays to demonstrate our approach due to the availability of public datasets and computational constraints. However, our methods are generalizable to other imaging modalities with detailed reports ( e.g. , CT, MRI, ultrasound), making their extension a key direction for future work. Additionally, our approach builds on the general-domain vision-language model Qwen2VL-7B [21], which serves as a strong baseline. Future research could explore larger models or those with medical domain-specific pretraining. Notably, our framework is model-agnostic and can be easily adapted to other base models. Second, ChestX-Reasoner currently relies on reasoning-augmented samples and rule-based factuality scores for process supervision. As our primary goal is to emphasize the value of process-level rewards, refining the supervision mechanism is left for future work. We aim to explore more robust strategies, such as developing medical-specific agentic reward models [32], to better manage samples without annotated reasoning references.

## 5 Methods

In this section, we first introduce the necessary preliminaries and then outline the training procedure for ChestX-Reasoner, which starts with supervised fine-tuning on both answer-only and reasoning-augmented data, followed by reinforcement learning with additional process rewards. We adopt Qwen2VL-7B [21] as our base model for its strong reasoning performance in the general vision-language domains [33, 34, 35]. Details of the training hyper-parameters are provided in the Supplementary 6.1.

## 5.1 Preliminaries

Problem statement. Assuming dataset D = { X ( i , Q P i , i , C i , A } i ) N i =1 , where N denotes the number of samples, X i represents the input images, Q i denotes the associated text instruction, P i refers to the mined reasoning process, C i is the corresponding clinical report, and A i is the ground-truth answer. Our objective is to develop a multimodal large language model (MLLM) that enhances Chest X-ray interpretation by improving both reasoning capability and answer accuracy on D . Formally, the objective is to generate the desired output sentences given the input images X and the text instruction Q :

$$\mathcal { Y } = \pi _ { \theta } ( \cdot ; \mathcal { X }, \mathcal { Q } ),$$

where π θ is the policy model parameterized by θ , and Y denotes the output sentences. Notably, there are samples without clinical report and mined reasoning processes in D , which means their P = ∅ , and C = ∅ . We distinguish them by dividing D = {D R , D } A with reasoning-augmented data ( D R ), and answer-only

Figure 6 | Illustration of different supervision methods. The top left illustrates the per-token supervision of supervised fine-tuning. The top right shows the outcome supervision on only several tokens related to answer or format. The bottom indicates our process supervision on every several tokens that contributes to a process.

<!-- image -->

## data ( D A ):

- ◦ Reasoning-augmented data ( D R ): set of samples with clinical report and mined reasoning sentences.
- ◦ Answer-only data ( D A ): set of samples without ground-truth reasoning and clinical reports, which means their P = ∅ , and C = ∅ .

To enable the model to function in both reasoning and direct response modes, we design two instruction formats for each sample: one with a chain-of-thoughts (CoT) prompt that elicits reasoning, and one without, which directly requests the answer. Details of these instruction formats are provided in Supplementary 6.2. By leveraging both CoT and non-CoT prompts, our approach allows for disabling intermediate reasoning, thereby substantially reducing inference time when reasoning is unnecessary.

DeepSeek-R0 with outcome rewards and GRPO algorithm. Recent medical studies [13, 14] on enhancing reasoning abilities have adopted the training procedures of DeepSeek-R0 [2], which aims to guide the model for reasoning with reinforcement learning on answer-only samples ( D A ). The optimization algorithm employed is Group Relative Policy Optimization (GRPO) [36] as RL-GRPO illustrated in Fig. 6. Compared to Policy Gradient Optimization (PPO) [37] algorithm, GRPO eliminates the need for a separate critic model and estimates baseline based on group scores, thereby significantly lowering computational overhead. With regard to the reward functions used in the algorithm, they are based on the outcome of model output, namely outcome format reward and outcome accuracy reward. Outcome format reward enables the output to be more structured, thus facilitating further visualization, and performance evaluation. Outcome accuracy reward is the evaluation metric between output answer and the ground truth answer. They evaluate the overall output while overlooking the specific characteristics of individual processes. For comparison, we trained ChestX-Reasoner-RL(O) following this pipeline.

DeepSeek-R1. In the DeepSeek-R1 model [2], an additional supervised fine-tuning step as SFT shown in Fig 6 is utilized as a cold-start initialization before reinforcement learning. This involves auto-regressive training of the base model on samples with reasoning chains ( D R ). The objective maximizes the likelihood of generating both reasoning processes and answers given input X , and Q :

$$\mathcal { L } _ { \text{SFT} } = - \mathbb { E } _ { ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, \mathcal { C }, A ) \sim \mathcal { D } _ { R } } \sum _ { t = 1 } ^ { T } \log \pi _ { \theta } ( y _ { t } \, | \, \mathcal { X }, \mathcal { Q }, y _ { < t } ),$$

where y is the concatenated sequence tokens of mined reasoning processes P and ground truth answer A , t is the index of decoded tokens y , and T is the total number of supervised tokens. For comparison, we trained ChestX-Reasoner-SFT(RO)+RL(O) following this pipeline.

## 5.2 ChestX-Reasoner

After introducing the preliminaries, we now detail the training pipeline for ChestX-Reasoner in the following.

Stage-I: supervised fine-tuning. Different to Deepseek-R1, we adopt auto-regressive training of base model on both D A , and D R . The training maximizes the following objective:

$$\mathcal { L } _ { \text{SFT} } = - \mathbb { E } _ { ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, \mathcal { C }, \mathcal { A } ) \sim \{ \mathcal { D } _ { \mathcal { A } }, \mathcal { D } _ { R } \} } \sum _ { t = 1 } ^ { T } \log \pi _ { \theta } ( y _ { t } \, | \, \mathcal { X }, \mathcal { Q }, y _ { < t } ).$$

We find that tuning the model exclusively on either D A or D R fails to achieve the desired objective. Specifically, solely using the final answer as supervision, without incorporating intermediate reasoning steps, leads to overfitting to the answer patterns and an undesirable loss of reasoning ability. Conversely, tuning the base model on the small-scale reasoning-augmented samples alone fails to leverage the valuable labels available in the full dataset, resulting in suboptimal model performance and the inability to directly output the answer.

Stage-II: reinforcement learning with process reward. With SFT model as an initialization, we then employ GRPO algorithm to perform reinforcement learning with additional process reward as ChestX-Reasoner shown in Fig. 6. Specifically, for each input ( X Q P C A , , , , ) ∈ D , GRPO samples a group of output sentences O{ o , o 1 2 , . . . , o G } from the old model π old , where G is the pre-defined sentence number in a group. It then optimizes the model π θ by maximizing the following objective function:

$$J _ { \text{GRPO} } = \mathbb { E } _ { ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, C, A ) \in \mathcal { D }, \{ \alpha _ { 1 } \} \sim \pi _ { o l d } ( \mathcal { O } | \mathcal { X }, \mathcal { Q } ) } \left [ 1 \sum _ { i = 1 } ^ { G } \left ( \min \left ( \frac { \pi _ { \theta } ( \mathfrak { o } _ { i } | \mathcal { X }, \mathcal { Q } ) } { \pi _ { o l d } ( \mathfrak { o } _ { i } | \mathcal { X }, \mathcal { Q } ) }, 1 - \epsilon, 1 + \epsilon \right ) A _ { i } - \beta \, D _ { \text{KL} } \left ( \pi _ { o l d } \| \ \pi _ { \theta } \right ) \right ) \right ], \ ( 8 )$$

where and are hyperparameters controlling the clipping range and the strength of the KL divergence

ϵ β penalty, respectively. KL penalty constrains the updating calculated as: D KL ( π old ∥ π θ ) = log ( π old ( o i |X Q , ) π θ ( o i |X Q , ) ) . The advantage A i is computed using a group of rewards { r , r 1 2 , . . . , r G } corresponding to the group outputs calculated as A i = r i -mean ( { r 1 ,r 2 ,...,r G } ) std ( { r 1 ,r 2 ,...,r G } ) . Samples from D A use the outcome format reward and the outcome accuracy reward, whereas for samples from D R , an additional process reward is included, which are all defined as follows:

- ◦ Outcome Format Reward R format : A reward of 1 is assigned if the model adheres to a specific output structure, specifically &lt;think&gt; &lt;/think&gt;, &lt;answer&gt; &lt;/answer&gt; of samples in D R and &lt;answer&gt; &lt;/answer&gt; of samples in D A ; otherwise, the reward is 0.
- ◦ Outcome Accuracy Reward R oa : An evaluation metric is used to measure the correctness of the model's output in comparison to the ground truth answer. Specifically, we extract the content enclosed within &lt;answer&gt; and &lt;/answer&gt; tags as the model's predicted answer. For close-ended questions with provided options, the reward is set to 1 if the predicted answer is the same with the ground truth option; otherwise, it is set to 0. For open-ended generation tasks, we employ RaTEScore [28] on the extracted answer as the reward.
- ◦ Process Factuality Reward R f : The reward is calculated as the factuality score defined in RadRScore, which evaluates the proportion of correctness of the generated clinical observations compared to the ground-truth report sentences.

The calculation of reward can be formulated as:

$$r _ { i } = r ( o _ { i } ; ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, \mathcal { C }, \mathcal { A } ) ) = \begin{cases} \ R _ { \text{format} } ( o _ { i } ; \text{format} ) + R _ { o a } ( o _ { i } ; \mathcal { A } ), & ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, \mathcal { C }, \mathcal { A } ) \in \mathcal { D } _ { A }, & ( 9 ) \\ R _ { \text{format} } ( o _ { i } ; \text{format} ) + R _ { o a } ( o _ { i } ; \mathcal { A } ) + R _ { J } ( o _ { i } ; \mathcal { C }, \mathcal { A } ), & ( \mathcal { X }, \mathcal { Q }, \mathcal { P }, \mathcal { C }, \mathcal { A } ) \in \mathcal { D } _ { R }, \end{cases}$$

where C is the clinical report, and A is ground truth answer. The distinction among these rewards lies in their supervision signals. From the equation, we can easily find that, outcome format reward and outcome accuracy

reward are calculated based on only several tokens describing format or final answer compared to a specific format and ground truth answer A . These two rewards are typically used in many prior work [2, 7, 6, 13, 14], as well as in ChestX-Reasoner-RL(O) and ChestX-Reasoner-SFT(RO)+RL(O), lacking supervision of reasoning processes. To incentivize high-quality reasoning, we preserves the robust reasoning supervision embedded in clinical reports via process reward, ensuring a more transparent and reliable reasoning process through the process factuality reward.

## Data availability

All datasets used in this study are publicly accessible. This study utilized datasets that are all publicly accessible. The RadRBench-CXR evaluation benchmark, and the model weights of all stages and variants will be released before publication.

## Code availability

The code used for experiments in this study will be made publicly available before publication. It includes the code to create reasoning processes from reports and the code to train the model ChestX-Reasoner. All the models will be hosted on HuggingFace (https://huggingface.co/) before publication.

## Author Contributions

Ziqing Fan, Chaoyi Wu, and Weidi Xie designed the training pipeline, and Ziqing Fan carried out the model training. Cheng Liang, Ziqing Fan, Chaoyi Wu designed the data collection, and Cheng Liang, and Ziqing Fan carried out creating reasoning processes. Cheng Liang designed and carried out the model evaluation. All authors contributed to the drafting and revision of the manuscript. Weidi Xie, and Yanfeng Wang supervised and guided the research

## 6 Supplementary

Supplementary Table 1 | Training hyper-parameters in training stage 1.

| Parameter name              | Value             |
|-----------------------------|-------------------|
| Global batchsize            | 512               |
| Learning rate               | 2e-6              |
| Optimizer                   | AdamW             |
| β 1 of optimizer            | 0.9               |
| β 2 of optimizer            | 0.95              |
| Warmup ratio                | 0.1               |
| Learning rate scheduler     | Cosine            |
| Gradient clipping threshold | 1.0               |
| Weight decay                | 0.0               |
| Platform                    | 8 Tesla A100 GPUs |
| Training steps              | 3500 steps        |
| Training times              | about 2 days      |
| DeepSpeed                   | zero2             |

Supplementary Table 2 | Training hyper-parameters in training stage 2.

| Parameter name              | Value                                                               |
|-----------------------------|---------------------------------------------------------------------|
| Global batchsize            | 256                                                                 |
| Updating batchsize          | 256                                                                 |
| KL loss coefficient         | 0.0001                                                              |
| Gradient clipping threshold | 1.0                                                                 |
| Entropy loss coefficient    | 0.0001                                                              |
| Group number ( G )          | 8                                                                   |
| Learning rate               | 5e-7                                                                |
| Optimizer                   | AdamW                                                               |
| β 1 of optimizer            | 0.9                                                                 |
| β 2 of optimizer            | 0.95                                                                |
| Platform                    | 8 Tesla A100 GPUs                                                   |
| Training steps              | 400 steps                                                           |
| Training times              | about 3.5 days                                                      |
| Distributed platform        | FSDP without offloading                                             |
| Reward function             | outcome format and outcome accuracy, and process factuality rewards |

## 6.1 Training Details

We utilize Qwen2VL-7B [21] as our base model due to its strong reasoning performance in the domains of mathematics and programming. As shown in Table 1, the training parameters for the first supervised fine-tuning stage are configured as follows: the optimizer is AdamW [38], with β 1 set to 0.9 and β 2 to 0.95. We adopt a cosine learning rate schedule with an initial learning rate of 2e-6, a batch size of 512 samples, a weight decay of 0.0, and a gradient clipping threshold of 1. The experiments are conducted on 8×Tesla A100 GPUs using DeepSpeed-ZeRO2 [39]. In this stage, the model is trained for 3500 steps, taking approximately 2 days to complete. For the second stage, reinforcement learning with process reward, the training parameters are provided in Table 2. The optimizer remains the same as in the first stage, with a learning rate of 5e-7. Both the global batch size and mini updating batch size are set to 256. The coefficient of KL loss is 0.0001, and the group number ( G ) is 8. The reward function combines outcome format reward, outcome correctness reward, and process reward, while entropy loss is 0.0001. The experiments are conducted on 8×Tesla A100 GPUs using PyTorch-FSDP [40] and VeRL engine [41]. In this stage, the model is trained for 400 steps, taking approximately 3.5 days to complete.

## 6.2 Instruction Design

For the model to operate in both reasoning mode and direct response mode, we adopt two instruction formats for each sample, each paired with its corresponding expected output.

## ◦ Instruction without Chain of Thoughts Prompt:

(Input Instruction) System: You are a helpful AI assistant.

(Expected Output Answer) Assistant: &lt;answer&gt; option etc. B or open-ended answer &lt;/answer&gt;.

(Input Instruction) User: &lt;image&gt; ... &lt;image&gt;Question. Please enclose the answer within &lt;answer&gt;&lt;/answer&gt;

## ◦ Instruction with Chain of Thoughts Prompt:

(Input Instruction) System: You are a helpful AI assistant.

(Expected Output Answer) &lt;think&gt; First, assess the presence of consolidation at bilateral lung, ..., Finally, check for Pleural nodule on left side, ..., In conclusion, the findings that align best with the observations are B) Sutures in upper lobe of left lung, Cicatrix on left ribs

(Input Instruction) User: &lt;image&gt; ... &lt;image&gt;Question. Please think step by step, and enclose the answer within &lt;answer&gt;&lt;/answer&gt; and the reasoning processes within &lt;think&gt;&lt;/think&gt;.

- &lt;/think&gt;&lt;answer&gt; option etc. B or open-ended answer &lt;/answer&gt;.

## 6.3 Mined Reasoning Visualization

As shown in Tab. 3, we show the task type, input instruction, and mined reasoning processes of cases in our benchmark RadRBench-CXR.

Supplementary Table 3 | Mined reasoning and corresponding instruction from clinical reports.

| Task Type                      | Instruction                                                                                                                                                                                                                                                           | Mined Reasoning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|--------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Binary Dis- ease Diag- nosis   | <Image> Does this chest X-ray show atelec- tasis?                                                                                                                                                                                                                     | The analysis begins by examining the chest X-ray for signs of atelectasis. Key features to evaluate include reduced lung volumes and any distinctive opacities in the lung. Observations revealed low lung volumes and minimal opacities in the left basilar region, suggesting partial segmental collapse indicative of atelectasis. Normally, atelectasis can cause structures like the trachea or diaphragm to shift towards the affected area, but in this case, no significant displacement was noted, supporting the conclusion that the atelectasis is mild. Given these findings, the chest X-ray does indeed show minimal left basilar atelectasis, confirming the diagnosis of atelectasis.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Single Dis- ease Diag- nosis   | <image><image> <image>Which finding is in this chest X-ray? Options: A) lung opacity B) support devices C) enlarged cardiomediastinum D) atelectasis                                                                                                                  | To determine the finding in this chest X-ray, we begin by assessing for lung opacity, support devices, enlarged cardiomediastinum, and atelectasis. First, examining lung opacity, we observe mild linear opacity in the bilateral lung bases. This is consistent with bibasilar atelectasis, suggesting that the opacity is related to alveolar collapse rather than fluid accumulation. Moving to support devices, the images reveal multiple overlying leads and linear dense objects in the right upper quadrant, indicative of artifacts from electrocardiographic monitoring and likely cholecystectomy clips. These are not pathological findings but rather support devices. Regarding the mediastinal contour, although there is pulmonary vascular crowding, there is no direct evidence of an enlarged cardiomediastinum. The findings suggest possible mild pulmonary vascular congestion but do not conclusively indicate cardiomegaly or significant enlargement. Finally, the low lung volumes and persistent linear opacities at the lung bases, observed across serial images without significant change, confirm the presence of atelectasis. Given these observations, the primary finding in this chest X-ray is atelectasis.                                                                                                                                                                                                                                                                                                                  |
| Multiple Disease Diagnosis     | <image>Which find- ings are in this chest X- ray? Options: A) lung opacity, pneumonia, support devices B) at- electasis, cardiomegaly, support devices C) cardiomegaly, enlarged cardiomediastinum, pneumothorax D) fracture, enlarged cardiomediastinum, atelectasis | First, assess the presence of lung opacity, pneumonia, and support devices. The images reveal a diffuse interstitial pattern and unchanged tracheostomy tube and other support devices. This suggests lung opacities possibly linked to past pneumonia. Next, evaluate the possibility of atelectasis and cardiomegaly along with support devices. There are no visible signs of lung tissue collapse or an enlarged heart. Support devices are noted to be unchanged. Then, consider cardiomegaly, enlarged cardiomediastinum, and pneumothorax. There are no indications of an enlarged heart or mediastinum, nor are there signs of pneumothorax. The status of support devices remains unchanged with previous lung infection sequelae. Finally, check for fracture, an enlarged cardiomediastinum, and atelectasis. No bone fractures or signs of lung collapse are observed. Additionally, there is no evidence of an enlarged mediastinum. In conclusion, the findings that align best with the observations are lung opacity, pneumonia, and support devices.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Anomaly Detection              | <image>Which find- ings are in this chest X-ray?                                                                                                                                                                                                                      | Upon reviewing the chest X-ray images, increased opacification is observed in the left lower lobe. This pattern is indicative of left basilar atelectasis, where areas of increased density suggest partial or complete lung collapse. Comparing with prior images, the stability of this finding suggests a chronic condition rather than an acute collapse. Additionally, the presence of overlying opacity raises the possibility of superimposed pneumonia, indicating potential infectious consolidation. The images reveal multiple fractures in the left-sided ribs and a displaced fracture in the left clavicle. Given their stability over previous imaging, these injuries are currently in a healing or chronic phase, rather than representing new acute fractures. The consistent appearance across different assessments supports this conclusion. Examining the left basilar region, an overlying opacity is evident at the site of atelectasis. This opacity, in combination with stable atelectasis, aligns with the typical presentation of developing pneumonia, signifying possible infectious consolidation. Inspection of the right apical lung area shows a line suggestive of a small pneumothorax. However, in the context of adjacent fractures, this line is more likely due to displaced rib fragments rather than a genuine pneumothorax. This interpretation requires careful consideration of all imaging details. In conclusion, the findings from the chest X-ray indicate the presence of atelectasis, fracture, and pneumonia. |
| Temporal Com- parison Analysis | <image><image>You are given two images: one reference image and one new image. Please identify the progression of consol- idation. Options:A) worsening B) stable C) improving                                                                                        | To assess the progression of consolidation between the two x-ray images, I need to focus on changes in the lung fields, where consolidation manifests as denser areas. Initially, I will observe both images for any variations in opacity across the entirety of the lung regions. Following this, I will pay particular attention to any change in volume or distribution of these opacities, which could indicate progression. Upon carefully comparing the images, I notice that there is an increase in the density and size of the opacities, especially on the left side of the lungs. This indicates that the areas of consolidation have expanded. Given these observations, the conclusion is that there is a worsening of the consolidation, as the increased opacities suggest progression of the condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
<|endofpaper|>