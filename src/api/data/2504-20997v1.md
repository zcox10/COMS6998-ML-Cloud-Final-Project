<|startofpaper|>
## Toward Efficient Exploration by Large Language Model Agents

## Dilip Arumugam 1 Thomas L. Griffiths 1 2

## Abstract

A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of dataefficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statisticallyefficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, dataefficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.

## 1. Introduction

Large language models (LLMs) have rapidly permeated many areas of machine learning, demonstrating proficiency across a broad range of tasks (Bommasani et al., 2021; Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023; Hurst et al., 2024; Jaech et al., 2024). This has inspired recent work studying how LLMs can best be used to solve sequential decision-making problems. These efforts have led to the introduction of new designs for LLM agents that

1 Department of Computer Science, Princeton University. 2 Department of Psychology, Princeton University. Correspondence to: Dilip Arumugam &lt; dilip.a@cs.princeton.edu &gt; .

Figure 1. Abstractly, an RL algorithm is an ordered sequence of steps. Existing approaches for LLM agent design (top) orchestrate some number of LLMs to implicitly induce a RL algorithm. In contrast, this paper advocates for a novel agent design principle (bottom) whereby an existing RL algorithm is explicitly implemented by outsourcing individual steps to distinct LLMs.

<!-- image -->

aim to learn optimal behavior through trial-and-error interaction within natural language environments (Yao et al., 2023; Shinn et al., 2024; Monea et al., 2024; Klissarov et al., 2024). While details vary by approach, broadly speaking these new agent designs involve one or more LLMs that interact to ultimately select actions at each timestep within the environment. However, such agents still reside in the classic RL setting (Sutton &amp; Barto, 1998) and, consequently, must still grapple with the fundamental obstacles to data efficiency (generalization, exploration, &amp; credit assignment) that the RL literature has studied for decades.

While composing LLMs to arrive at new agent designs is the current norm, we propose that an alternative strategy is to re-examine existing RL algorithms and consider how LLMs might implement them in otherwise inaccessible environments. An RL algorithm consists of specifying inputs and detailing a sequence of steps for determining behavior at each time period. Why should the emergence and proliferation of LLMs change the fundamental principles of agent design? Instead, as visualized in Figure 1, perhaps LLMs can be used to create new, potentially-inexact incarnations of existing RL algorithms and the subroutines needed to implement them.

In this work, we focus on data-efficient RL with LLMs and isolate the key challenge of exploration. We demonstrate how modern LLMs afford a contemporary implementation of an existing RL algorithm, Posterior Sampling for Reinforcement Learning (PSRL) (Strens, 2000; Osband et al., 2013), that is both well-studied and whose capacity for good exploration is already known to yield provably-efficient RL in a number of problem classes. We empirically find that our LLM-based implementation of PSRL retains the strong exploration properties that, up to this point, have not only been primarily restricted to tabular domains but also been absent in recent designs for LLM agents. We further observe that the choice of LLM underlying the PSRL implementation matters and, in an environment with stochastic transition dynamics, show that upgrading to a more capable model (GPT-4o to o1-mini) is the difference between incurring linear regret and obtaining cumulative regret on par with classic PSRL. Altogether, our work underscores the importance of addressing exploration in the design of LLM agents, illustrates the considerable value that decades of RL research have to offer data-efficient decision-making with LLMs, and establishes a key distinction between LLMs that implement a RL algorithm versus a RL algorithm that is implemented with LLMs.

## 2. Problem Formulation

For any arbitrary set X , we use ∆( X ) to denote the set of all probability distributions with support on X . For any N ∈ N , we denote the index set as [ N ] = 1 2 { , , . . . , N } .

We formulate a sequential decision-making problem as a finite-horizon, episodic Markov Decision Process (MDP) (Bellman, 1957; Puterman, 1994) defined by M = ⟨S , A R T , , , β, H ⟩ . S is a set of states, A is a set of actions, R : S × A → [0 , 1] is a reward function providing evaluative feedback in the unit interval, T : S × A → ∆( S ) is a transition function prescribing distributions over next states, β ∈ ∆( S ) is an initial state distribution, and H ∈ N is the maximum episode length or horizon. Within each of K ∈ N total episodes, the agent acts for H steps beginning with an initial state s 1 ∼ β ( ) · and, at each timestep h ∈ [ H ] , observes the current state s h ∈ S , selects an action a h ∈ A , enjoys a reward r h = R ( s h , a h ) , and transitions to a next state s h +1 ∼ T ( · | s h , a h ) .

An agent is characterized by its non-stationary, stochastic policy π : S × [ H ] → ∆( A ) , which encodes a pattern of behavior by mapping individual states and the current timestep to a probability distribution over actions. We assess the performance of a policy π in MDP M at timestep h ∈ [ H ] when starting at state s ∈ S and taking action a ∈ A by its associated action-value function Q π M ,h ( s, a ) =

$$\mathbb { E } \left [ \sum _ { h ^ { \prime } = h } ^ { H } \mathcal { R } ( s _ { h ^ { \prime } }, a _ { h ^ { \prime } } ) \, \Big | \, s _ { h } = s, a _ { h } = a \right ]. \, \text{ Taking the value} \quad \text{ to Sec}$$

function as V π M ,h ( s ) = E a ∼ π h ( ·| s ) [ Q π M ,h ( s, a ) ] , we define the optimal policy π ⋆ as achieving supremal value V ⋆ M ,h ( s ) = sup π ∈ Π V π M ,h ( s ) for all s ∈ S , h ∈ [ H ] where Π denotes the class of all non-stationary, stochastic policies. For any episode k ∈ [ K ] , we let τ k = ( s ( k ) 1 , a ( k ) 1 , r ( k ) 1 , . . . , s ( k ) H , a ( k ) H , r ( k ) H , s ( k ) H +1 ) denote the random trajectory experienced by the agent executing its policy in the environment. Meanwhile, H k = { τ 1 , τ 2 , . . . , τ k -1 } ∈ H is the entire random history of agent interaction at the start of the k th episode.

Abstractly, a RL algorithm is a sequence { π ( k ) } k ∈ [ K ] where the policy deployed at each episode π ( k ) is a function of the current history H k . We may evaluate the performance of a RL algorithm on MDP M via its cumulative regret: REGRET ( { π ( k ) } k ∈ [ K ] , M ) =

$$\iota \ w u l { \begin{array} { c } \ h e \, \text{im} \\ \L L M \\ \L L \end{array} } \quad \mathbb { E } \left [ \sum _ { k = 1 } ^ { K } \left ( V _ { \mathcal { M }, 1 } ^ { * } ( s _ { 1 } ) - V _ { \mathcal { M }, 1 } ^ { \pi ^ { ( k ) } } ( s _ { 1 } ) \right ) \right ], \, \text{which measures the to} \\ \L L \end{array} \quad \text{at least some other all but mean on account} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$} \, \text{$a$}$$

tal performance shortfall between an agent's chosen policy and the optimal policy across all episodes. Naturally, an agent designer seeks out a RL algorithm whose cumulative regret is minimal.

## 3. Related Work

While our primary focus in this paper is on efficient exploration for LLM agents, the broader challenge of efficient exploration for RL agents is a long-studied topic. One route to achieving statistically-efficient exploration relies on the use of 'optimism in the face of uncertainty,' where approaches either implicitly or explicitly maintain over-inflated value function estimates for all state-action pairs (Kearns &amp; Singh, 2002; Brafman &amp; Tennenholtz, 2002; Kakade, 2003; Auer et al., 2009; Strehl et al., 2009; Jaksch et al., 2010; Dann &amp;Brunskill, 2015; Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Zanette &amp; Brunskill, 2019; Dong et al., 2022). These optimistic biases are calibrated by an agent designer to incentivize agent visitation of each state-action pair sufficiently many times and eventually result in accurate value estimates that give rise to optimal behavior. Nie et al. (2024) attempt to realize such an optimistic exploration strategy with LLMs (specifically, combining UCB (Auer et al., 2002) with Gemini (Team et al., 2023)) for multi-armed bandit problems and demonstrate the difficulty in coupling statistical machinery like confidence intervals with LLMs outright. While our proposed implementation relies on an equally (if not more) complex statistical object, the Bayesian posterior, our experiments suggest that LLMs in certain cases may maintain an approximation sufficient for guiding exploration. We defer a brief review of prior work on this alternative class of uncertainty-based exploration methods to Section 4.

Existing designs for LLM agents either do not explicitly

engage with the challenge of exploration or do so with complete reliance on in-context learning (ICL) (Brown et al., 2020). One of the most popular LLM agent designs is Reflexion (Shinn et al., 2024) where the policy LLM charged with selecting actions is informed at each episode by a 'selfreflection' generated from another LLM given the previous episode trajectory. While suitable for some tasks, we observe in our experiments that the self-reflection LLM often 'passes the buck' and encourages exploration generically in language without providing a clear strategy for the downstream policy LLM to do so. By relying on LLMs to provide the requisite functions for implementing a prudent choice of existing RL algorithm, we encounter strategic exploration without needing to explicitly instruct any of the involved LLMs to explore.

LLM agents that rely on ICL to enable exploration follow suit with a line of work that examines Transformer-based RL agents in non-natural-language tasks (Laskin et al., 2022; Liu et al., 2023; Lee et al., 2024b; Dai et al., 2024; Yan et al., 2025). These methods often rely on casting ICL as either implicit, approximate Bayesian inference (Xie et al., 2022; Zhang et al., 2023) or within the 'control as inference' framework (Levine, 2018); one key challenge with the former is that such implicit posterior knowledge cannot be flexibly and explicitly leveraged to guide exploration, whereas the latter suffers from not capturing epistemic uncertainty at all (O'Donoghue et al., 2020; Tarbouriech et al., 2023). Very close to the spirit of our work is the in-context policy iteration (ICPI) method of Brooks et al. (2023), who take the classic RL algorithm of policy iteration (PI) (Howard, 1960) and implement it with LLMs and ICL. Unfortunately, the original PI algorithm is oriented towards tabular MDPs that allow for iterating over all state-action pairs simultaneously. While the ICPI algorithm forgoes this in favor of online data collection and resampling via experience replay (Lin, 1992), the authors find it necessary to sample with a dataset balancing scheme to ensure the accuracy of ICL; this presumes that the 'right' data is already present or easily acquired from the environment. In larger environments where data must be judiciously acquired, we find that ICPI is never able to collect the data needed for ICL to exhibit any kind of performant behavior. Monea et al. (2024) study a selective 'dropout' strategy for the ICL demonstrations used by a policy LLM. However, such a strategy mirrors ϵ -greedy exploration (Watkins &amp; Dayan, 1992) without making a concerted effort to strategically guide decision-making, much like how classic dropout in deep RL (Gal &amp; Ghahramani, 2016) is a poor proxy for uncertainty-based exploration (Osband, 2016b). In contrast to ICL, the core idea studied in this work is conceptually similar to meta-prompting (Goodman, 2023), where an agent incrementally accumulates salient environmental knowledge within its system prompt to refine behavior in each episode; while prior work has suggested that meta-prompting is an implicit approximation of posterior sampling (Fr¨ anken et al., 2023), we here are exclusively concerned with the explicit implementation of PSRL.

A related line of approaches examines using classic (deep) RL methods in tandem with LLM reward functions (Klissarov et al., 2024; Kwon et al., 2023; Zheng et al., 2024). These approaches, while interesting, largely focus on nonlinguistic domains whereas our goal is to bring ideas on data-efficient RL to bear on the natural language domains where LLMs stand to have the most impact. The posterior-sampling-based exploration strategy we consider in this work connects more broadly to initial investigations surrounding the information gathering capabilities of LLMs (Ke et al., 2024).

Lastly, we note that the Reinforcement Learning from Human Feedback (RLHF) pipeline (Stiennon et al., 2020; Ouyang et al., 2022) used to explicitly optimize LLMs also faces an underlying sequential decision-making problem (in the original formulation, a contextual dueling bandit (Yue et al., 2012; Dud´ ık et al., 2015)) and, as such, may greatly benefit from mechanisms to facilitate efficient exploration (Dwaracherla et al., 2024). Concretely, at any point in the fine-tuning process either by RLHF or Reinforcement Learning from AI Feedback (RLAIF) (Lee et al., 2024a), there will be preference data that offer very little utility or change in LLM responses and those that stand to dramatically improve response quality. By actively exploring for the latter kind of prompts and responses, one stands to arrive at a more proficient LLM with fewer iterations of RLHF or RLAIF. While such work is nascent, our results may offer a promising new pathway for LLMs to achieve the strategic exploration that could reduce these significant data burdens.

## 4. A LLM-Based Implementation of Posterior Sampling for Reinforcement Learning

One of the major obstacles to data-efficient RL is exploration, where a learner must determine what data to collect from the environment to maximize long-term performance. While much of the early work on addressing exploration in RL discussed in Section 3 adhered to 'optimism in the face of uncertainty,' an alternative is to proceed in a Bayesian fashion.

The Bayesian RL setting (Bellman &amp; Kalaba, 1959; Duff, 2002; Ghavamzadeh et al., 2015) recognizes that the underlying MDP is entirely unknown to the agent and, therefore, a random variable. The agent is thus endowed with a prior distribution P ( M∈· ) to reflect initial uncertainty in the true MDP. While the standard RL objective (Sutton &amp; Barto, 1998) calls for an agent to minimize regret, another performance criterion is the Bayesian regret, which simply integrates out the randomness in M with re-

spect to an agent's prior: BAYESREGRET ( { π ( k ) } k ∈ [ K ] ) = E [ REGRET ( { π ( k ) } k ∈ [ K ] , M ) ] . We make a standard assumption that this prior distribution is well-specified such that the true MDP resides in its support. While theory is not a focus of this work, we simply note in passing that prior misspecification of posterior-sampling methods is a well-studied topic in bandit learning (Russo &amp; Van Roy, 2014; Simchowitz et al., 2021; Liu et al., 2022), where one can provably expect a graceful degradation in performance commensurate with the degree of misspecification; colloquially, similar results are expected for the full RL setting as discussed, for instance, in the introduction of O'Donoghue (2021).

Unfortunately, the canonical Bayes-Adaptive MDP (BAMDP) (Bellman &amp; Kalaba, 1959; Duff, 2002) that encapsulates the full Bayesian RL problem is often computationally-intractable even in the simplest classes of environments, such as tabular MDPs. This is a direct consequence of the intractably-large BAMDP hyperstate space (Duff, 2002; Arumugam &amp; Singh, 2022), in which traditional MDP states are folded in alongside epistemic states (Lu et al., 2023) that contain an agent's beliefs and epistemic uncertainty (Der Kiureghian &amp; Ditlevsen, 2009) about the world. While states and actions of the MDP are considered known, the transition and reward functions are unknown to a RL agent (otherwise, the agent would face a planning, rather than RL, problem). With each step taken in the true environment, the resulting immediate reward and next-state transition provide ground-truth observations with which the agent may obtain posterior beliefs about the underlying MDP M ; even for a simple finite MDP, the BAMDP hyperstate space is exponentially-large in the problem horizon H . While the true MDP state at each timestep is essential to good decision-making, one might hope that the latter epistemic state could be lazily updated while still allowing an agent to strategically explore the world by reducing epistemic uncertainty. This insight is the basis of posterior-sampling methods in RL.

## 4.1. The Classic Approach

The promise of Bayesian RL methods is to facilitate statistically-efficient exploration by reducing an agent's epistemic uncertainty about the world. One strategy for reaping the benefits of uncertainty-based exploration in a computationally-tractable manner is through Posterior Sampling for RL (PSRL) (Strens, 2000), presented as Algorithm 1. Rather than updating the epistemic state at each timestep, PSRL holds it fixed during each episode and only updates posterior beliefs at the end using the full trajectory τ k . To govern action selection within each episode based on current knowledge of the true underlying MDP P ( M ∈ · | H k ) , PSRL employs Thompson sampling (TS) (Thompson, 1933; Russo &amp; Van Roy, 2014; 2016;

Figure 2. The PSRL algorithm with LLM subroutines of posterior sampling, optimal behavior with respect to a sample, and posterior updating shown. Dotted arrows show data flow.

<!-- image -->

Russo et al., 2018), whereby the agent draws one posterior sample as a statistically-plausible hypothesis about the true MDP (Line 3) and proceeds to act optimally with respect to it by executing the sampled MDP optimal policy (Lines 4-5). It has been shown theoretically that, by iteratively employing TS in this manner, PSRL is able to achieve strong exploration and satisfy Bayesian regret upper bounds for statistically-efficient RL in tabular MDPs, where the bestknown bound falls within ˜ ( O √ H ) of the strongest regret lower bound (Jaksch et al., 2010), and beyond (Osband et al., 2013; Osband &amp; Van Roy, 2014; Abbasi-Yadkori &amp; Szepesvari, 2014; Osband &amp; Van Roy, 2016; Agrawal &amp; Jia, 2017; Ouyang et al., 2017; Osband &amp; Van Roy, 2017; Lu &amp; Van Roy, 2019; Arumugam &amp; Van Roy, 2022; Xu et al., 2024). A key contribution of this work is expanding empirical support for PSRL, an algorithm that has largely been a method of theoretical study up to this point.

While PSRL enjoys nice theoretical guarantees, practical implementations extending beyond tabular MDPs (Osband et al., 2013) face significant computational hurdles. Representing and maintaining epistemic uncertainty about the underlying MDP transition and reward functions is an open challenge in high-dimensional environments. While some work has studied using neural networks to address the broader problem of uncertainty estimation for guiding exploration in RL (Osband et al., 2016a; Lu &amp; Van Roy, 2017; Osband et al., 2018; O'Donoghue et al., 2018; Dwaracherla et al., 2020; Osband et al., 2023; Sasso et al., 2023), the overwhelming majority of these efforts have concentrated on a model-free analogue of PSRL that maintains a Bayesian posterior over the optimal action-value function Q ⋆ (Osband et al., 2016b; 2019) in lieu of the underlying MDP M . Meanwhile, the minority of such methods that actually strive to implement PSRL have either been met with mixed results across hard-exploration problems or have been limited to evaluations in smaller-scale domains. Among them is a line of work that leans heavily into the use of Langevin dynamics

Figure 3. Examples of a posterior (top) and posterior sample (bottom) generated by our LLM-based PSRL in Wordle

<!-- image -->

for recovering the strategic exploration of PSRL (Mazumdar et al., 2020; Karbasi et al., 2023; Ishfaq et al., 2024; Jorge et al., 2024); in the context of this work, such technical machinery is incredibly challenging and nontrivial to combine or even emulate with LLM agents.

In parallel, beyond the difficulties of maintaining a PSRL agent's posterior distribution over the true MDP, computing the optimal policy for the posterior sample drawn in each episode constitutes an additional challenge that requires solving a planning problem. While there has been progress and even notable successes in this space for deep modelbased RL agents (Kaiser et al., 2020), it is unclear if those methods are readily applicable to the natural language tasks faced by LLM agents. In our experiments, while we report positive results for our LLM-based PSRL implementation in MDPs with both deterministic and stochastic transition functions, performance in the latter type of environment eventually deteriorates as the size of the state-action space increases and exacerbates poor LLM planning capabilities under stochastic dynamics.

## 4.2. A LLM Implementation

The key contribution of this paper is recognizing that LLMs can be operationalized to provide basic, atomic functions from which PSRL may be implemented. As discussed in Section 3, this stands in stark contrast to existing strides towards efficient decision-making with LLM agents (Nie et al., 2024; Krishnamurthy et al., 2024; Klissarov et al., 2024; Ke et al., 2024) which either leave a LLM to its own devices for strategizing exploration or expect in-context learning (ICL) (Brown et al., 2020) to emulate the exploration of an existing RL or bandit algorithm. While future LLMs may become sufficiently capable to accommodate the former, our experiments today suggest this is not the case for two simple, natural-language tasks where efficient exploration is paramount to success; by the same token, we anticipate that our proposed LLM-based implementation of PSRL will also benefit and gracefully extend to more complex natural language tasks as the constituent LLM models become more capable at performing their requested functions. Indeed, we find this to be the case empirically when applying our approach to MDPs with stochastic transition functions. LLM agents emulating the outputs of classic RL methods are also bound to the same traditional problem classes whereas LLM-based implementations of RL algorithms may broaden the footprint of those classic algorithms to include natural-language domains that would otherwise be entirely infeasible.

As shown in Algorithm 1, our proposed implementation of PSRL relies on LLMs to play three distinct roles: (1) an approximate posterior updater, (2) a posterior sampler, and (3) an optimal policy with respect to a posterior sample. PSRL requires a prior distribution over MDPs as input and, more generally in any episode, needs a current posterior that accurately reflects the agent's current knowledge and uncertainty about the world. For our purposes, such an approximate 'posterior' is a textual description that summarizes both the known and uncertain aspects of the true MDP transition and reward function. More importantly, it also explicitly communicates (in some way) the amount of uncertainty an agent has about these aspects of the world. For ease of exposition, we will refer to this object as a posterior throughout the remainder of the paper, but acknowledge the distinction between it and the true, statistical object that is the Bayesian posterior distribution. As this textual summary amounts to the PSRL agent's epistemic state representation (Lu et al., 2023), an agent designer may exert strong influence over this representation through the presentation and expression of prior knowledge; as a concrete example, specifying the next-state transition distribution of a tabular MDP in our experiments as a Dirichlet distribution (in language) naturally encourages the LLM-based implementation of PSRL to maintain visitation counts. Of course, an advantage is that agent designers may now leverage the full expressivity and fluidity of natural language for communicating prior knowledge without restriction to the few statistical distributions that afford the computational conveniences of conjugate priors.

Given a current posterior reflecting the agent's knowledge and uncertainty about the world, PSRL must be able to draw one posterior sample from these beliefs. We implement this as a first LLM that, given the agent's current textual posterior (initially set to be the agent designer's input prior) is tasked with generating a plausible hypothesis for how transitions and rewards unfold. In some domains, such as tabular MDPs, it may be natural for this to be an exhaustive list of rewards and next-state transitions for each state-action pair. For more practical scenarios of interest, however, it may be beneficial to prompt this posterior sampling LLM so that it can leverage an environment proxy or lossy surrogate MDP (Lu et al., 2023; Arumugam &amp; Van Roy, 2022) that retains only the salient details needed to determine optimal

behavior. As a concrete example, one of our natural language tasks is the game of Wordle (shown in Figure 3) that, as a MDP, has a transition function and reward function defined entirely around an unknown, five-letter target word. Here, the target word serves as an environment proxy that our LLM-based PSRL agent may directly monitor uncertainty over without meticulously maintaining statistics for the rewards and transitions of individual state-action pairs.

With a single posterior sample in hand, a PSRL agent must be able to select actions that would be considered optimal if the sampled MDP truly reflected reality. We implement this as a second LLM tasked with executing actions given the current state that maximize value in a way that is consistent with the natural language hypothesis generated by the posterior sampling LLM. In the simplest case, this optimal sample policy LLM need only be given the posterior sample and input observation and asked directly to generate an action. In more challenging settings, an agent designer may architect the LLM more carefully via chain-of-thought prompting (Wei et al., 2022; Kojima et al., 2022) to increase the chance of selecting optimal actions consistent with provided hypothesis. Even when this policy is only approximately-optimal, classic PSRL still admits a Bayesian regret bound (see Section 5.4 of Osband (2016a)) and one might hope to see an LLM-based implementation of PSRL exhibit similar robustness in practice.

Upon the completion of an episode with the optimal sample policy LLM acting with respect to the hypothesis of the posterior sampling LLM, we task a third and final LLM with updating the PSRL agent's knowledge and residual uncertainty about the world, akin to an (approximate) posterior update. Given a complete trajectory consisting of reward signals and next-state transitions for exactly H stateaction pairs, this posterior LLM must reconcile the agent's prior knowledge at the start of the episode against observed interactions from within the environment. With this last piece of functionality in place, all three LLMs can then be orchestrated to run the PSRL algorithm.

## 5. Experiments

The goal of our experiments is assessing the extent to which our proposed LLM-based PSRL implementation not only retains the desirable exploration properties that PSRL exhibits empirically within simpler problem domains but also expands the range of problems where these benefits can be realized. To this end, we focus our evaluation on tasks which demand prudent exploration to achieve success and where an agent is minimally encumbered by the challenges of generalization and credit assignment. For each task, we present cumulative regret curves (lower, flatter plots indicate better performance) where any shading denotes one standard error. All agents use GPT-4o (Hurst et al., 2024)

Figure 4. Cumulative regret curves for a 5-armed Bernoulli bandit.

<!-- image -->

for their constituent LLMs unless otherwise indicated. We let κ sampling , κ π ⋆ , and κ posterior denote the temperatures of the posterior sampling, optimal sample policy, and posterior update LLMs, respectively. We defer further details of our experiments, including all prompts used in each task to the Appendix.

## 5.1. Multi-Armed Bandits

Following prior work studying the exploratory capabilities of LLMs (Coda-Forno et al., 2023; Binz &amp; Schulz, 2023; Coda-Forno et al., 2024; Krishnamurthy et al., 2024; Nie et al., 2024), we begin the empirical assessment of our LLM-based PSRL with a multi-armed bandit problem (Lai &amp;Robbins, 1985; Bubeck &amp; Cesa-Bianchi, 2012; Lattimore &amp;Szepesv´ ari, 2020). Readers unfamiliar with multi-armed bandits may simply observe them as a special case of a MDP with horizon H = 1 , singleton state space |S| = 1 , and a stochastic (rather than deterministic) reward function. Our evaluation follows that of Krishnamurthy et al. (2024) who chose the simple yet challenging case of a five-armed Bernoulli bandit with independent arms and an action gap of 0 2 . . 1 The version we evaluate has one randomly-selected optimal arm with rewards drawn from a Bernoulli (0 6) . distribution while all other arms use a Bernoulli (0 4) . .

Observe that PSRL specialized to a multi-armed bandit problem mirrors classic TS where, at each timestep, the agent samples one plausible hypothesis for the reward distribution of each arm and then proceeds to select the optimal action believed to achieve highest mean reward under this hypothesis. We compare PSRL implemented with LLMs to classic TS for a Bernoulli bandit with each arm initial-

1 The action gap is defined as the difference in expected reward between the best and second best action. Larger action gaps make it easier to identify the optimal arm with few samples whereas smaller action gaps demand greater exploration.

Figure 5. Cumulative regret curves for the combination lock environment. The vertical axis shows turns to identify the unlock code.

<!-- image -->

ized with a Beta (1 1) , prior. Meanwhile, our LLM-based PSRL agent begins with a prior for each arm specified as a Beta(1,1) in natural language. While we fix temperatures κ π ⋆ = κ posterior = 1 , we find that the posterior sampling temperature has profound impact on the performance of our LLM-based PSRL agent. Figure 4 compares TS (run for 1,000 independent trials) against PSRL with four distinct settings of κ sampling (run for 20 independent trials). We relegate further bandit results to the Appendix.

## 5.2. Natural Language Tasks

While the previous bandit problem is one where classic PSRL (TS) may already find success without the use of LLMs, an LLM-based implementation of PSRL should help realize the benefits of efficient exploration in domains currently untouchable by vanilla PSRL. To illustrate this, we present two natural language tasks where the initial prior uncertainty and time sensitivity due to limited episodes present a formidable exploration challenge. We then move on from these deterministic tasks to one where a highly-stochastic transition function drives the difficulty of exploration.

We compare our LLM-based implementation of PSRL against three baseline LLM agents. In-Context Policy Iteration (ICPI) (Brooks et al., 2023) takes classic policy iteration (Howard, 1960) and offers an implementation via three LLMs, using ICL to elicit a rollout policy; transition function; and reward function respectively. Together, these models allow for policy improvement via greedy action selection π ( k ) ( s h ) = arg max a ∈A Q π ( k -1) M ( s h , a ) , with ties broken randomly. 2 In-Context RL (ICRL) (Monea et al., 2024) aims to explore via the stochasticity in LLM responses from sensitiv-

2 Due to its significantly higher financial cost and lengthy run times, ICPI is limited to only 10 trials in Wordle.

Figure 6. Cumulative regret curves for the Wordle environment. The vertical axis shows turns to identify the target word.

<!-- image -->

ity to the input ICL data. Which episodes are included from a replay buffer for ICL with a LLM policy at each timestep is determined by sampling independent Bernoulli ( p ) random variables; we study three distinct values of the keep probability p ∈ { 1 0 5 , . , 0 1 . } . Finally, Reflexion (Shinn et al., 2024) passes each full trajectory through a self-reflection LLM that generates verbal guidance; the total history of verbal guidance is given at each timestep to the LLM policy, along with the current state, for improving the quality of decision-making.

## 5.2.1. DETERMINISTIC TRANSITION DYNAMICS

The first task is a combination lock environment where an agent must enter H = 3 distinct digits in order to open a lock and receive a reward of +1 . All other rewards are zero and the agent is provided with (verbal) state information indicating whether the most recently guessed digit is either in the correct position for the unlocking code, present in the unlocking code but in some other position, or simply not present in the unlocking code at all. An agent has a total of K = 8 episodes to identify the correct combination and, with each one of 20 independent trials having an unlock code sampled uniformly at random from all 720 possible codes, exploration via uniform random code selection has just under a 0 14% . chance of success.

The second task is the challenging web game known as Wordle (Lokshtanov &amp; Subercaseaux, 2022), where an agent has exactly K = 6 episodes to enter H = 5 distinct letters 3 that form a correct target word and receive a reward of +1 . Across 40 independent trials, the target word is chosen uniformly at random from a corpus of English dictionary words filtered for slang and repeated letters. The agent is provided verbal feedback in each state indicating whether

3 We do not require the letters to form a dictionary word.

Figure 7. Cumulative regret curves for the RiverSwim environment with 3 states. Labels show the choice of constituent LLM model (GPT-4o or o1-mini) in each LLM agent.

<!-- image -->

the most recently guessed letter is in the correct position for the target word, in the target word but at some other position, or not present in the target word at all.

Our LLM-based PSRL agent ( κ sampling = κ π ⋆ = κ posterior = 1 ) is given an uninformative prior which describes all non-repeating codes/English words with the appropriate length as being equiprobable; the unlock code/target word is an environment proxy (Lu et al., 2023) such that knowledge of the proxy is a sufficient statistic for recovering the full MDP. In the combination lock environment, we also compute the Bayes-optimal policy with respect to the uninformative prior and plot its cumulative regret for comparison.

## 5.2.2. STOCHASTIC TRANSITION DYNAMICS

While the domains presented in the previous section confirm that an LLM-based implementation of PSRL retains efficient exploration in deterministic environments, we further demonstrate its efficacy in stochastic environments. As a simple illustration of this, we turn our focus to a truncated variant of the RiverSwim environment (Strehl &amp; Littman, 2008). RiverSwim is a tabular MDP given as a six-state chain where the agent begins in the leftmost state. The stochastic transition function mimics a water current that allows an agent to deterministically swim to the left (downstream with the current) but only stochastically swim to the right (upstream against the current) with a 35% chance of success. 4 Swimming downstream in the initial state results in a small reward of 0 005 . . Successfully swimming all the

4 We adhere to the specific transition dynamics presented by Osband et al. (2013) where swimming upstream has only a 60% chance of successfully advancing to the next state upstream and a small 5% chance of being pushed back one state downstream.

Figure 8. Cumulative regret curves for the RiverSwim environments with 3 (solid lines) and 4 (dashed lines) states, respectively. o1-mini is used exclusively with our LLM-based PSRL.

<!-- image -->

way upstream allows the agent to reach the rightmost state where it can collect a reward of 1 . As all other rewards are zero, a RiverSwim agent must explore the full length of the river to learn optimal behavior. To keep financial costs down, we truncate the environment to a river of length 3 (one initial state, intermediate state, and terminal state) with H = 6 .

We compare our LLM-based implementation of PSRL with a vanilla PSRL agent for a tabular MDP (Osband et al., 2013). The latter models epistemic uncertainty over the transition function as a collection of |S||A| Dirichlet distributions. This epistemic state representation allows for the computational conveniences of Dirichlet-multinomial conjugacy. We further model unknown rewards with a discrete uniform prior over { 0 0 005 1 , . , } . Cumulative regret curves shown in Figure 7 compare our LLM-based PSRL with a Dirichlet(0.1,0.1,0.1) prior against vanilla PSRL (with the standard uniform Dirichlet prior initialization of α 0 = 1 |S| ). We use κ π ⋆ = κ posterior = κ sampling = 1 and all agents are run for 40 independent trials, except the vanilla PSRL agent run for 1,000. Additional comparisons are made against our best-performing baselines in the combination lock environment: Reflexion and ICRL with p = 1 .

## 6. Discussion

In this section, we provide a detailed overview of our results as well as insight into the limitations of our proposed LLMbased implementation of PSRL.

## 6.1. Retaining Efficient Exploration

In the bandit setting, we observe our LLM-based PSRL obtains better cumulative regret curves than classic TS, for the limited time horizon of T = 100 . We find that supplying PSRL with an initial prior of Beta(1,1) in language automatically encourages the posterior update LLM to update binary reward observation counts for the chosen arm in each time period. Moreover, we find that the optimal sample policy LLM has little difficulty in examining the sequence of expected reward values for each arm generated by the posterior sampling LLM and adhering to select the perceived best action. Manipulating κ sampling shows that even values as large as 1 lead to greedy-like exploration in many trials where the resulting posterior sample favors the action observed to yield the most successes thus far. For a limited number of trials, this error proves to be not so catastrophic for temperatures of at least 1. We find that increasing κ sampling &gt; 1 yields exploratory behavior more aligned with TS where optimal actions more likely to be taken in the later time periods and a slowing of probability mass pulled away from other actions. Please see Appendix A.1 for more fine-grained analysis of this bandit problem.

The combination lock and Wordle environments represent separate instances of the same exploration problem at differing scales within a deterministic environment. Our results show that the LLM-based PSRL is able to most effectively explore the space of possible unlock codes/target words relative to the baseline methods. Crucially, none of the three constituent LLMs used by PSRL are prompted to explicitly encourage exploration. Rather, these results illustrate how prompting these LLMs to perform atomic functions of PSRL and allowing the algorithm to prescribe how those outputs should be orchestrated in the agent design can yield an effective exploration strategy.

The ICPI paper includes a dataset balancing scheme for ICL, presuming the requisite data has already been collected. While reasonable for some environments, exploration is fundamentally about governing data collection to synthesize optimal behavior and, in these domains, ICPI never observes non-zero reward and collapses to a random policy For ICRL, using all available data with p = 1 is equivalent to the 'LLM policy' evaluated by Klissarov et al. (2024), who also find poor performance in Wordle. While results in the combination lock domain are better, we find that decreasing the keep probability p is detrimental to the 'exploratory' ICRL of Monea et al. (2024). Reflexion is the strongest baseline, however we observe that self-reflections during the early stages of learning explicitly encourage exploration of untested digits/letters literally , assuming the agent knows how to explore upon simply being instructed to do so. Only once uncertainty has largely been resolved do reflections become more specific suggestions about how to explore with particular digits/letters and their ordering.

Our initial results with RiverSwim were negative as GPT-4o struggled to cope with maintaining and updating the verbose epistemic state representation describing reward information and next-state transitions across all 12 state-action pairs. Curiously, however, this negative result provided an opportunity to assess a claim of Section 4.2 that more-capable LLMs would allow our PSRL implementation to scale gracefully to more complex tasks. Indeed, by upgrading from GPT-4o to o1-mini, we observe that our LLM-based PSRL is capable of achieving sub-linear regret on par with vanilla PSRL. Reflexion is unable to persevere past failed attempts to swim upstream before settling for the smaller downstream reward of 0 005 . . ICRL has just over 25% of trials where it stumbles into the optimal policy and sticks with it while, for 60% of trials, it too falls back to pursuing the downstream reward. Moreover, the same LLM upgrade has little impact on the performance of Reflexion and actually manages to worsen the performance of ICRL; for the latter, we suspect the performance degradation stems from a combination of the stochastic transition dynamics coupled with the large quantity of ICL demonstrations that perhaps mesh poorly with the reasoning steps of o1-mini.

## 6.2. Limitations

## 6.2.1. SCALING UP STOCHASTIC ENVIRONMENTS

While the success of our LLM-based PSRL in RiverSwim after upgrading to o1-mini from GPT-4o is encouraging, we find that the scalability of such a substitution is shortlived. Recall that our version of RiverSwim used in the preceding section is a truncated variant down to a length-3 river. Unfortunately, as seen in Figure 8, just increasing the river by one additional intermediate state to obtain a length-4 RiverSwim environment ( H = 20 ) causes the performance of our LLM-based PSRL to degrade into linear regret.

This negative result underscores a crucial distinction in the choice of epistemic state between agents; that is, the statistical object Dirichlet (0 1 . , 0 1 . , 0 1 . , 0 1) . used by classic PSRL and the natural language string Dirichlet(0.1,0.1,0.1,0.1) used in LLM-based PSRL. For deterministic transitions in RiverSwim, classic PSRL is able to see eventual concentration to a Dirac delta distribution. Meanwhile the LLM-based PSRL agent, while successful at maintaining visitation counts, is slow to achieve the same convergence and, across many posterior samples, leaves non-negligible probability mass on nonexistent transitions with fictitious rewards. One plausible explanation would be that such concentration errors stem from a lack of familiarity by the LLMs, given that Dirichlet distributions with fractional parameters are encountered with less frequency (McCoy et al., 2024); however, our preliminary experiments with a Dirichlet(1,1,1,1)

Figure 9. Cumulative regret curves for the 11-armed informative action bandit (Example 2) of Russo &amp; Van Roy (2018).

<!-- image -->

prior showed no significant improvement.

Issues with posterior concentration notwithstanding, we also find that far too many episodes fail as the optimal sample policy LLM struggles to select optimal actions, even when supplied with posterior samples that have high fidelity to the true environment. Even with chain-of-thought prompting, we find a clear lack of understanding for long-term, value-based planning; the preliminary success with length3 RiverSwim suggests that this failure is connected to the increased verbosity of the epistemic state that, in turn, compromises the optimal sample policy LLM's ability to account for the value of traversing the full river over collecting the small downstream reward repeatedly. Altogether, while the overall result is negative, we anticipate that these issues may resolve organically in a manner similar to our early challenges with GPT-4o in length-3 RiverSwim; that is, by leveraging a more advanced alternative LLM. Even if recent open-source reasoning models (Jaech et al., 2024; Guo et al., 2025) prove ineffective at fulfilling this purpose, one might still naturally anticipate that such deficiencies will disappear with time assuming future LLM capabilities continue to expand.

## 6.2.2. BEYOND THOMPSON SAMPLING

While PSRL, through the use of TS, is known to yield a strong exploration strategy, it is by no means perfect. In the bandit literature, shortcomings of TS are well-known and naturally become more salient in the full RL problem (Russo &amp; Van Roy, 2018; Lu et al., 2023). In short, by only executing actions with some probability of being optimal, TS will never take deliberately sub-optimal actions that yield tremendous information gain. Figure 3 already illustrates how a PSRL agent's uncompromising execution of only potentially-optimal policies cripples exploration and solely

Figure 10. Cumulative regret curves for the combination lock environment including LLM-IDS.

<!-- image -->

allows for the testing of two unknown letters at a time.

One remedy is to seek out instantiations of informationdirected sampling (IDS) (Russo &amp; Van Roy, 2018). IDS is an algorithmic design principle that advocates for using a policy which balances between performance shortfall and information gain. While supported by a rigorous corroborating theory in both bandits and RL (Lu et al., 2023), concrete and practical instantiations of IDS are difficult to come by on account of the challenges surrounding information gain estimation (McAllester &amp; Stratos, 2020). Moreover, the temporally-delayed consequences absent from bandits but present in RL problems pose an additional challenge as a proper IDS agent must forecast future opportunities for knowledge acquisition several steps into the future when evaluating current actions.

We present an initial design for a IDS agent with LLMs. Our proposed LLM-IDS agent is myopic in that it only takes immediate information gain about optimal behavior at the next timestep into account. Nevertheless, the feedback structure of the combination lock environment allows such an agent to be unconcerned with temporally-delayed information. For a current state s h ∈ S , we define two |A| -dimensional vectors, ρ and I , where ρ a ( ) = E [ V ⋆ M ,h ( s h ) -Q ⋆ M ,h ( s h , a ) ] is the expected regret of taking action a ∈ A in s h under the agent's current posterior and I ( a ) = I ( π ⋆ ; R ,S h h +1 | A h = a, S h = s h ) is the information gained (formally, the conditional mutual information (Cover &amp; Thomas, 2012)) about the optimal policy by taking action a from state s h . IDS calls for sampling an action from the distribution that minimizes the information ratio: min π ∈ ∆( A ) E a ∼ π [ ρ a ( )] 2 E a ∼ π [ I ( a )] . Normally, computation of the ρ and I vectors would be done directly with the current posterior. Instead, we recycle the same posterior update LLM from our LLM-based PSRL

but incorporate two new LLMs for the provision of ρ and I ; each of these LLMs is prompted on a per-action basis to assess the expected regret or information gain, respectively, from each action in the current state. With these 2 |A| LLM-generated numerical values, the convex optimization problem of minimizing the information ratio is solved to compute the policy for action selection.

We offer two empirical evaluations to highlight the limitations of LLM-based PSRL exploration inherited from TS while also underscoring the future potential of our LLMIDS. The first is a contrived but transparent multi-armed bandit problem given as Example 2 of Russo &amp; Van Roy (2018). In this ( K + 1) -armed informative action bandit problem, there is a unique optimal action A ⋆ ∈ [ K ] that yields a deterministic reward of 1 while all other arms yield a reward of 0; additionally, there is an action 0 that deterministically provides a reward equal to (2 · A ⋆ ) -1 . Naturally, an agent willing to deliberately select sub-optimal actions to gain information would take action 0 immediately and then produce optimal behavior thereafter with the identity of A ⋆ in hand. Figure 9 shows across 10 trials that LLM-IDS succeeds in recovering this optimal exploration strategy (for further affirmation, please see the episodic regret curve in Appendix A.1) exactly for the K = 10 instance whereas LLM-based PSRL is incapable of doing so while exploring via TS. This result also highlights one simple instance of the flexibility that specifying natural-language priors to LLMbased PSRL affords as encoding prior knowledge about the informative action might prove difficult when limited to classic statistical distributions. Extending past this contrived yet transparent bandit example, Figure 10 shows that LLM-IDS is able to outperform LLM-based PSRL by more quickly testing for unknown digits while remaining unencumbered by known digits already discovered.

## 7. Conclusion

While much of the burgeoning literature surrounding LLM agents has felt compelled to design new algorithms for solving RL problems, we here have demonstrated that an existing algorithm, PSRL, can be implemented with LLMs. The main advantage of our proposed LLM-based implementation of PSRL is allowing agent designers to leverage the strong generalization and reasoning capabilities of LLMs in natural-language environments while simultaneously capitalizing on the well-studied exploration properties of TS.

Our work opens up multiple potential avenues for future work. Further study on how LLMs may successfully perform the requisite planning needed to cope with larger-scale stochastic environments is one natural area for improvement. In classic model-based RL, it is understood that inaccurate models of the world induce overfitting errors during planning and that various techniques exist for regularizing plan- ners to make the best use of such models anyways (Jiang et al., 2015; Arumugam et al., 2018); as some of these regularizers can be explicitly cast as skewed priors in a Bayesian RL setting (Rathnam et al., 2023), it may be worthwhile to explore novel regularization methods tailored to embrace, rather than resolve, the imperfect planning of LLM agents in stochastic domains. More broadly, examining the sensitivity of LLM-based PSRL to the input natural language prior and the resulting downstream impact on policy updates (Schaul et al., 2022) may have useful interpretability and safety applications. Finally, our preliminary results on recovering information-directed exploration with LLMs represents (what is likely to be) a very fruitful direction and further reinforces the potential benefits of implementing, rather than replacing, existing RL algorithms with LLMs.
<|endofpaper|>