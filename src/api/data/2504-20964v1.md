<|startofpaper|>
## OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification

Shangyu Li 1 Juyong Jiang 12 Tiancheng Zhao 3 Jiasi Shen 1 * 1 The Hong Kong University of Science and Technology 2 The Hong Kong University of Science and Technology (Guangzhou) 3 Georgia Institute of Technology {sliew,jjiang472}@connect.ust.hk tzhao350@gatech.edu , sjs@cse.ust.hk

## Abstract

We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyuhkust/OSVBench.

prove the absence of bugs in software (Dahl et al., 1972), which is essential in ensuring the correctness of software in safety-critical domains such as aerospace, healthcare, and nuclear energy (Klein et al., 2009; Amani et al., 2016; O'Connor et al., 2016), where software errors could lead to catastrophic economic losses or even endanger human lives. However, manual software verification is challenging and time-consuming, demanding advanced knowledge of formal methods and program analysis. Consequently, professionals capable of conducting verification are limited, underscoring the necessity for automation in this domain. Within this context, operating system kernel verification remains a valuable and significant niche requiring a high level of expertise.

## 1 Introduction

Large Language Models (LLMs) have shown great potential in software engineering tasks, such as code generation (Austin et al., 2021; Athiwaratkun et al., 2022; Zan et al., 2023; Jiang et al., 2024), code summarization (Ahmed et al., 2024), and bug repair (Jin et al., 2023). However, an important aspect of software engineering remains underexplored: software verification Software verification uses rigorous mathematical reasoning to

* Correspoinding Author.

In this paper, we examine the capabilities of LLMs in automating operating system (OS) kernel verification. We introduce a benchmark suite to evaluate the effectiveness of these models in verifying an OS kernel, a fundamental component of many critical infrastructures. Formally verifying an OS kernel typically involves first defining precise specifications that outline the properties the kernel must satisfy (Klein et al., 2009; Chajed et al., 2022; Chen et al., 2015, 2017), followed by constructing formal proofs using a theorem prover to demonstrate compliance with these specifications. Additionally, the inherent complexity, concurrency, and hardware interactions of OS kernels render the verification process highly challenging. For example, the verification of the well-known seL4 microkernel (Klein et al., 2009) required 11 person-years of effort for 10k lines of C code, while verifying two operations of the BilbyFs file system required 9.25 person-months of effort (Amani et al., 2016) for 1,350 lines of code. Customizing formal proofs is widely acknowledged as a challenging task, with existing research primarily focused on automating proof generation (Chen et al., 2024; Zhang et al., 2024), often overlooking the significant challenge

Figure 1: The workflow of OSVBench benchmark suite. This workflow consists of the generation stage and the specification quality evaluation stage. During the generation stage, the input to LLMs includes the verification assumptions, programming model, syscall examples (each comprising a functional description, potentially buggy code implementation, and corresponding state-machine specification), and a task question. The task question consists of the syscall's functional description, potentially buggy code implementation, and a question requiring the generation of the corresponding state-machine specification, as illustrated in Appendix D. Based on this input, the LLMs are tasked with generating the appropriate state-machine specification for the given syscall. In the evaluation stage, the kernel implementations represent various OS kernel versions with injected bugs, while the declarative specifications, fixed for the verifier, define overarching properties and invariants that the state-machine specifications must satisfy, as detailed in subsection 3.1. The oracle state-machine specifications represent the ground-truth specification. The kernel verifier takes a kernel implementation, the declarative specification, and the state-machine specifications as inputs, performs verification on the kernel implementation, and produces two verification results: one using the generated specification and the other using the oracle specification. Once the kernel verifier produces consistent verification results for all kernel implementations, the generated specification is deemed correct; otherwise, it is considered incorrect. Further details about the verifier are provided in subsection 3.1.

<!-- image -->

of specification development (Sammler et al., 2021; Leino, 2010; Jacobs and Piessens, 2008; Ma et al., 2024). This challenge is further exacerbated in the context of OS kernel verification, where specifications are typically ad hoc (Chen et al., 2017, 2015; Chajed et al., 2022; Sigurbjarnarson et al., 2016), requiring substantial domain expertise. For instance, creating the formal specification for seL4 (Klein et al., 2009) required 7 person-months of effort.

Given the advanced coding capabilities of LLMs, it is worth exploring their potential to generate OS kernel specifications-a specialized form of source code that formally defines OS kernel behaviors. To this end, we introduce OSVBench, a benchmark suite derived from the Hyperkernel project (Nelson et al., 2017), to evaluate the capabilities of LLMs in generating specifications for verifying the functional correctness of an OS kernel. The benchmark aims to facilitate automation in the generation of OS kernel specifications. It comprises of 245 specification generation tasks, each of which is a complex and intricate program synthesis task with long context, approximately 20k to 30k tokens. Figure 1 illustrates the workflow for each specification generation task.

We conducted comprehensive experiments on the formal specification generation from the functional description in natural language and the code implementation of the OS kernel with different types and numbers of vulnerabilities by injecting 5 real-world types of bugs into the OS kernel. The experiment results showcase the potential of LLMs in automating formal specification generation for OS kernel verification. Finally, we conduct rigorous data decontamination (Yang et al., 2023b) for the synthetic dataset to remove samples that closely resemble those in the test subset D (0) test of the coverage dataset. The main contributions of this paper can be summarized as below:

- · We initiate an exploration of LLMs in the context of OS verification tasks, which necessitate a deep understanding and manipulation of extensive contextual information and domainspecific knowledge.
- · We introduce OSVBench, a benchmark designed for OS verification, to evaluate the capabilities of LLMs in generating specifications for OS kernel verification.
- · We performed a comprehensive evaluation of the most advanced LLMs in generating spec-

ifications aimed at verifying the functional correctness of an OS kernel. We also discuss the impact of varying types and quantities of bugs on the quality and effectiveness of the generated specifications.

## 2 Related Work

LLMfor Software Verification. Software verification (D'silva et al., 2008) ensures that software conforms to specified properties or requirements, playing a critical role in guaranteeing software reliability and correctness. In the domain, operating system kernel verification (Klein et al., 2014) has been a central research goal in ensuring the reliability and security of critical software systems. Early foundational work includes efforts such as UCLA Secure Unix (Walker et al., 1980), PSOS (Feiertag et al., 1977), and KIT (Bevier, 1989), which laid the groundwork for formal approaches to kernel correctness. Recent progress has expanded to leveraging formal methods like theorem proving (Nelson et al., 2017) and model checking (Klein et al., 2009), aiming for high-assurance kernels with mathematically verified properties. Prior work on leveraging LLMs for software verification mainly focused on generating proofs from specifications (Chen et al., 2024; Zhang et al., 2024), which involves translating one deterministic formal semantic representation (specifications, in various forms) into another (proofs expressed in formal languages). Additionally, some studies have explored the task of specification generation. However, much of this work has concentrated on generalpurpose specification generation (Ma et al., 2024), which differs significantly from the generation of OS kernel specifications due to the distinct verification assumptions and requirements encountered in this domain.

LLM for Code Generation. The use of LLMs for code generation has gained significant attention in recent years, as these models have demonstrated remarkable capabilities in synthesizing code snippets from natural language descriptions (Austin et al., 2021; Athiwaratkun et al., 2022; Zan et al., 2023; Jiang et al., 2024). Several studies have explored the potential of LLMs in various code generation tasks, ranging from simple function generation (Chen et al., 2021; Luo et al., 2023) to more complex programming challenges (Jimenez et al., 2023; Ding et al., 2024). Despite these advancements, code generation for specific domains, such as OS kernel verification, poses unique challenges that are not fully addressed by general-purpose LLMs. The complexity and specificity of the syntax and semantics involved require models not only to understand programming languages but also to grasp domain-specific knowledge and verification assumptions. This challenge calls for a benchmark that evaluate the capabilities of LLMs in generating specifications for OS kernel verification.

LLMfor static/dynamic program analysis. Existing LLM-based static analysis methodologies primarily rely on prompting LLMs to perform sourcesink reachability analyses on programs (Wang et al., 2024b,c,a). However, limited research has explored LLM-based static analysis specifically for the Linux kernel, which presents a significant challenge due to its long-context reasoning requirements. This complexity arises from the kernel's intricate call graphs and alias relationships . In contrast, dynamic analysis techniques, such as fuzzing, have seen a broader application of LLMs across various domains, including smart contracts (Shou et al., 2024), Linux kernel (Yang et al., 2023a), and universal domains (Xia et al., 2024).

## 3 OSVBench

In this section, we will discuss the specification generation problem formulation and the benchmark construction details.

## 3.1 Preliminaries

Hyperkernel (Nelson et al., 2017) is an OS kernel verification project that includes both a realworld kernel implementation and a verification framework built on the automated theorem prover Z3. The kernel implementation supports 50 system calls, covering key functionalities such as process management, virtual memory, file descriptors, device interaction, inter-process communication, and scheduling. The entire codebase consists of approximately 18,000 lines of C and assembly, encompassing both the kernel implementation and associated user-space components.

As demonstrated in the specification quality evaluation section in Figure 1, the verifier for Hyperkernel requires two types of specifications as input. The first is a state-machine specification, which defines the functional correctness by describing the intended behavior of the OS kernel. The second is a higher-level declarative specification, which outlines overarching properties and invariants that the state-machine specification must satisfy. For example, one such property ensures that the number of children for any given process is always equal to the total number of processes identifying that process as their parent.

The verifier establishes two theorems. The first one proves that the kernel implementation is a refinement of the state-machine specification, expressed as ∀ σ Impl ∈ Σ Impl , ∀ c ∈ C, ∃ σ Spec ∈ Σ Spec , such that σ Impl ( c ) = σ Spec ( c ) , which states that for every kernel state in the implementation σ Impl , under any condition C , there exists a corresponding kernel state σ Spec defined in the state-machine specification that is equivalent under the same condition c . The second theorem demonstrates that the state-machine specification satisfies the properties and invariants defined within the declarative specifications.

In our formulation of the specification generation task, the declarative specification is provided and the role of the LLM is to synthesize the statemachine specification, as detailed in subsection 3.2. Once the state-machine specification is prepared, its verifier performs symbolic execution (Cadar et al., 2008) on the compiled LLVM IR (Lattner and Adve, 2004) of the OS kernel and invokes the z3 (De Moura and Bjørner, 2008) solver to perform the equivalence checking on the real kernel state transitions in the concrete implementation and those defined in the state-machine specification. Additionally, the verifier ensures that the state-machine specification adheres to the high-level declarative specifications. Any detected inconsistency indicates either the presence of a bug in the OS kernel code implementation or that the state-machine specification fails to accurately describe the intended functionality of the system call.

We select the Hyperkernel as the benchmark for the following reasons: 1) Hyperkernel adopts a standardized approach (Klein et al., 2010) to model kernel execution as a state machine, represented through a set of Python classes with deterministic semantics. Crafting these specifications requires significant expertise and non-trivial effort, making it a suitable and challenging benchmark for evaluating specification generation tasks. 2) Hyperkernel employs an automated theorem prover, specifically the Z3 solver, to formally verify the functional correctness of the OS kernel instead of using the interactive theorem provers, such as Isabelle (Isabelle, 2025), Coq (Coq, 2025), and Dafny (Dafny, 2025). By utilizing an automated solver, the focus can remain on the specification generation tasks, streamlining the verification process.

## 3.2 Problem Formulation

Formally verifying an OS kernel is inherently complex, requiring the coordination of numerous components, which makes it challenging for LLMs to address directly and difficult to assess the accuracy of the generated specifications. Specifically, the inherent issues in Hyperkernel undermine the validity of LLMs in synthesizing correct specifications:

- · Various verification assumptions. Hyperkernel incorporates numerous implicit and explicit verification assumptions, including those related to the theorems used for proofs, hardware behavior, and memory layout, among others.
- · Interdependence between the statemachine specification and the declarative specification. In the design of Hyperkernel, users are required to define both the declarative specification and the state-machine specification. As stated before, the verifier ensures that the state-machine specification satisfies the declarative specification, which encapsulates a set of properties and invariants. However, this interdependence complicates the verification of LLM-generated specifications. Even if the verifier produces a successful verification result, it does not guarantee the correctness of the kernel implementation, as the initial declarative specification itself may contain errors.
- · Infinite search space. The state-machine specifications rely not only on kernel behavior modeling but also on a set of implicit constants and external functions. These include constants derived from the compiled LLVM IR of the kernel implementation, Z3 functions, utility functions, and others. Any omission or incomplete definition of these components results in an incomplete synthesis domain, which can lead to incorrect specifications and an unbounded, infinite search space.

To address the above challenges and ensure tractability for LLMs, we reformulate specification generation as a program synthesis problem by introducing: (1) Explicit verification assumptions: Systematically documenting relevant assumptions to guide LLMs in performing reliable verification. (2) Fixed declarative specifications: Restricting declarative specifications to enable LLMs to focus solely on generating state-machine specifications, thereby streamlining the verification process. (3) Deterministic synthesis domain: Defining constants, external functions, and classes to constrain the search space and ensure the synthesis process remains tractable for LLMs. Specifically, Figure 2 formally defines the abstract synthesis domain, facilitating the generation of state-machine specifications by LLMs.

The specification generation task is formally defined as a code generation task aimed at synthesizing complete specification code to verify the functional correctness of an OS kernel, constrained by the scoped semantics of the programming model. This process is guided by accurate high-level functional descriptions of system calls and their potentially flawed implementations, with fixed verification assumptions and declarative specifications. After formally defining the problem, addressing the challenges of synthesizing accurate specifications becomes the responsibility of the LLMs. These challenges are programming languages-agnostic and stem from the following factors: (a) accurately mapping the semantics of functional descriptions into corresponding specification code, as exemplified in Appendix A, (b) addressing the challenges posed by diverged kernel state synthesis, as also exemplified in Appendix A, and (c) processing and understanding contextual information, averaging 20k to 30k tokens, which requires advanced long-context learning capabilities to ensure accurate specification generation.

## 3.3 Benchmark Task Construction

By the definition of the specification generation problem, each task requires a functional description and its potentially buggy implementation. Thus, starting from the 49 system calls of Hyperkernel, we manually drafted multiple versions of the functional descriptions for each system call based on potential understandings of their intended behavior, purpose, and interactions within the OS kernel. Then, we selected the best version according to clarity and technical accuracy (Hao et al., 2023). Furthermore, we begin with a correct implementation of the OS kernel and systematically generate a set of faulty implementations, which reflects real-world scenarios where OS kernel implementations are not guaranteed to be correct and may contain various types and numbers of bugs. This

Specification := State

State := if cond State , State ′ | State |

State . ( field i ← Expression ) ∗

Expression := if cond Expression , Expression ′ | Param | Expression op Expression | Const | State . field i cond := and cond cond , | or cond cond , | Param op Const | State . field i op Const | State . field i op Param op := + | -| × | ÷ | == | != | &gt; | &lt; | &gt;= | &lt;=

Figure 2: Abstract domain for specification synthesis. In this domain, State represents the kernel state, while Param denotes the parameters provided to the system call. Const refers to the available constants and op encompasses the set of algorithmic and logical operators. Expression denotes expressions formed using constants, kernel state fields, parameters, and algorithmic operators, whereas cond refers to conditions derived by applying logical operators to kernel state fields, constants, or parameter values, and combining them using conjunction ( and ) or disjunction ( or ). A specification defines the subsequent kernel state resulting from a system transition based on the current state. The transitions or field assignments of a kernel state will be performed under different conditions.

is achieved by randomly introducing five types of real-world bugs, derived from the xv6 kernel (Cox et al., 2011), into the Hyperkernel codebase, as exemplified in Appendix B. This approach allows us to evaluate the impact of various vulnerabilities on the performance of LLMs in generating accurate state-machine specifications.

Finally, we create a total of 245 specification generation tasks, each consisting of a correct high-level functional description of a system call paired with its corresponding potentially buggy code implementation. Among the 245 code implementations, some are correct, while others contain varying numbers of bugs, ranging from one to five.

## 4 Evaluation

## 4.1 Experimental Setup

State-of-the-art LLMs. We conduct an evaluation of the current state-of-the-art Large Language Models (LLMs) developed by six leading institutions: OpenAI, DeepSeek, Meta, Anthropic, ByteDance, and the Qwen Team. Specifically, our evaluation includes the o1, o3-mini, and GPT-4o models from OpenAI; the DeepSeekR1 and DeepSeek-Chat models from DeepSeek; the Llama-3.1-70B-Instruct and Llama-3.1-8B-

Table 1: Performance comparison (Pass@1 %) of various models with 5-shots prompt. Models marked with ∗ denote reasoning LLMs, while △ indicates closed-source models; all unmarked models are open-source. The columns from Incorrect Pointer to Bounds Checking correspond to specific types of bugs injected into the syscall code within the task prompts. The column Correct indicates cases where the provided code implementations are bug-free. Lastly, the column Total reports the overall Pass@1 rate across all 245 tasks.

| Institution   | Model                     |   Incorrect Pointer |   Incorrect Privilege |   Memory Leak |   Buffer Overflow |   Bounds Checking |   Correct | Total       |
|---------------|---------------------------|---------------------|-----------------------|---------------|-------------------|-------------------|-----------|-------------|
| OpenAI        | o1 ∗△                     |               12.68 |                 21.43 |         13.51 |             20.37 |             23.15 |     28.57 | 23.67 22.04 |
|               | o3-mini ∗△                |               19.72 |                 18.75 |         18.92 |             12.96 |             15.74 |     26.53 |             |
|               | GPT-4o △                  |               33.8  |                 34.82 |         32.43 |             33.33 |             36.11 |     42.86 | 38.78       |
| DeepSeek      | DeepSeek-R1 ∗             |               32.39 |                 21.43 |         13.51 |             20.37 |             23.15 |     42.86 | 40.00       |
| DeepSeek      | DeepSeek-Chat             |               38.02 |                 39.29 |         36.49 |             44.44 |             43.52 |     51.02 | 46.53       |
| Meta          | Llama-3.1-70b-instruct    |               12.68 |                 18.75 |         12.16 |             16.67 |             22.22 |     22.45 | 22.45       |
| Meta          | Llama-3.1-8B-Instruct     |                0    |                 11.61 |          0    |             12.96 |              9.26 |     10.2  | 10.61       |
| Qwen Team     | QwQ-32B-Preview ∗         |               14.08 |                 23.21 |         20.27 |             20.37 |             23.15 |     22.45 | 24.08       |
|               | Qwen2.5-72b-instruct      |               25.35 |                 26.79 |         24.32 |             25.93 |             30.56 |     34.69 | 32.24       |
|               | Qwen2.5-Coder-7B-Instruct |                0    |                  8.04 |          0    |              3.7  |              5.56 |      4.08 | 4.90        |
| Anthropic     | Claude-3.5-sonnet △       |               39.44 |                 41.96 |         39.19 |             48.15 |             39.81 |     46.94 | 44.90       |
| ByteDance     | Doubao-1.5-pro △          |               50.7  |                 48.21 |         45.95 |             40.74 |             52.78 |     63.27 | 55.1        |

Figure 3: Performance comparison of Pass@1, Pass@3, and Pass@5 of various models. The average performance of the models exhibits consistent improvement as N increases in Pass@N.

<!-- image -->

Instruct models from Meta; the QwQ-32B-Preview, Qwen2.5-72B-Instruct, and Qwen2.5-Coder-7BInstruct models from the Qwen Team; Claude3.5-sonnet from Anthropic; and Doubao-1.5-pro from ByteDance. The evaluation leverages the OSVBench framework to systematically assess the performance of these models in the task of generating OS kernel verification specifications. These LLMs differ in key characteristics such as the number of parameters, open-source availability, data cutoff dates, and pretraining objectives. For all models, we employ a greedy search decoding strategy with pass@1 for consistency in evaluation.

emplified in Appendix D. The few-shot examples are meticulously selected by OS kernel verification experts to ensure their representativeness.

Prompt design. As illustrated in Figure 1, the prompt for each task is specifically designed for a particular system call of the OS kernel. The prompt is structured into four key components: the system verification assumptions, the programming model, the few-shot examples, and the task question, as ex-

Specification quality metrics. To systematically evaluate the performance of LLMs on the tasks, we define several metrics. As outlined in the specification quality evaluation stage in Figure 1, we deliberately created multiple OS kernel implementations with known, artificially inserted bugs. The metric Pass@N indicates that at least one of the N generated specifications is correct, meaning the inconsistencies it identifies in all OS kernel implementations align with those detected by the oracle specification. The Syntax Error metric denotes cases where the generated specification fails to execute correctly or terminates with an exception. Finally, the Semantic Error refers to instances where the verifier successfully translates the specification into SMT (De Moura and Bjørner, 2008) and per-

Table 2: Syntax and semantic error rates (%) in the specifications generated by LLMs across all 245 tasks. ∗ denotes reasoning LLMs. A lower error rate indicates better performance. △ indicates closed-source models, while unmarked models are open-source.

| Model                     |   Syntax Error |   Semantic Error |
|---------------------------|----------------|------------------|
| o1 ∗△                     |          52.65 |            23.67 |
| o3-mini ∗△                |          51.02 |            26.94 |
| GPT-4o △                  |          35.1  |            26.53 |
| DeepSeek-R1 ∗             |          32.65 |            26.53 |
| DeepSeek-Chat             |          31.02 |            24.9  |
| Llama-3.1-70b-instruct    |          44.9  |            32.65 |
| Llama-3.1-8B-Instruct     |          67.76 |            23.67 |
| QwQ-32B-Preview ∗         |          66.53 |             9.39 |
| Qwen2.5-72b-instruct      |          42.25 |            25.31 |
| Qwen2.5-Coder-7B-Instruct |          86.12 |            11.02 |
| Claude-3.5-sonnet △       |          22.45 |            32.65 |
| Doubao-1.5-pro △          |          23.67 |            21.22 |

forms verification on the OS kernel implementation, but the pinpointed inconsistencies differ from those specified by the oracle specification.

## 4.2 Main Results

Table 1 presents results of the performance of LLMs across institutions and bug categories. The best-performing closed-source LLM, Doubao-1.5pro, outperforms the best-performing open-source LLM, DeepSeek-Chat, with the highest average pass@1 rate (55.1%) and a superior ability to generate correct specifications (63.27%), showcasing robust performance across all bug types.

In general, models with larger parameter sizes tend to outperform their smaller counterparts. For instance, Llama-3.1-8B-Instruct and Qwen2.5Coder-7B-Instruct exhibit significantly weaker performance compared to their larger counterparts, such as Llama-3.1-70B-Instruct and Qwen2.5-72BInstruct, which have over 70 billion parameters. The results also highlight the performance degradation caused by the presence of various bug types, with the impact varying across models and bug categories. For example, memory leak bugs have the most pronounced effect on the DeepSeek-R1 model, while incorrect pointer bugs most significantly impact the o1 model.

Surprisingly, widely regarded reasoning models, such as o1 and DeepSeek-R1, do not consistently outperform other models in this task. In particular, the o1 model demonstrates weak performance, performing worse than the QwQ-32B-Preview model, which challenges assumptions about the superior-

Figure 4: Performance comparison (Pass@1 %) of 12 LLMs on specification generation tasks using syscall implementations injected with varying numbers of bugs.

<!-- image -->

ity of certain reasoning models in these tasks. We speculate that the advanced reasoning models being which could pose challenges to the long-context learning capabilities in OS verification scenarios.

utilized produce lengthy chains of reasoning traces, Pass@k performance. The performance of Pass@k is evaluated using GPT-4o, DeepSeekChat, and Llama-3.1-70b-instruct, as illustrated in Figure 3. The results demonstrate a consistent improvement in average pass rates as k increases from 1 to 3 to 5. Additionally, GPT-4o outperforms DeepSeek-Chat at k = 3 and k = 5.

## 4.3 Error Analysis and Self-repair

Table 2 presents the syntax and semantic error rates of various LLMs. Notably, the worst-performing LLM, Qwen2.5-Coder-7B-Instruct, is more prone to syntax errors compared to the best-performing LLM, Doubao-1.5-pro. This is evidenced by the higher semantic-to-syntax error rate ratio for Doubao-1.5-pro (21.22% / 23.67%) relative to Qwen2.5-Coder-7B-Instruct (11.02% / 86.12%).

To further evaluate the error-handling capabilities of LLMs, a two-round self-repair process was conducted using GPT-4o and DeepSeek-Chat, as presented in Table 3. In the first round, the models utilized their own generated specifications along with the corresponding verification errors as inputs for repair. If the specifications from the first round remained incorrect, the models leverage all their incorrect versions and the associated verification errors for subsequent repairs in the second round. Notably, the self-repairing method consistently improves the performance of specification generation tasks for OS kernel verification. However, the repair success rate declines with additional rounds of

Table 3: Comparison of two-round self-repair performance (repair success rate %) for models using a 5-shot prompt. The column Syntax error and Semantic error denote the repair success rates for specifications that produce the type of errors. The columns from Incorrect Pointer to Correct represent the repair success rates for specifications derived from the task prompt, categorized by the types of buggy code implementations in the prompt. The Total column reports the overall repair success rate for all erroneous specifications addressed in each round. Values outside parentheses denote the success rate in the first repair round, while values within parentheses indicate the success rate in the second round.

| Model         | Syntax Error   | Semantic Error   | Incorrect Pointer   | Incorrect Privilege   | Memory Leak   | Buffer Overflow   | Bounds Checking   | Correct      | Total        |
|---------------|----------------|------------------|---------------------|-----------------------|---------------|-------------------|-------------------|--------------|--------------|
| GPT-4o        | 16.47 (1.25)   | 12.31 (2.08)     | 17.02 (5.13)        | 17.81 (3.33)          | 6.12 (2.17)   | 8.33 (3.03)       | 17.39 (1.75)      | 17.86 (0.00) | 14.67 (1.56) |
| DeepSeek-Chat | 8.57 (6.25)    | 14.75 (0.00)     | 2.27 (4.65)         | 8.82 (1.61)           | 6.52 (0.00)   | 10 (0.00)         | 6.56 (3.51)       | 20.83 (0.00) | 11.45 (2.59) |

Figure 5: Performance of various models on different number of few-shot examples. As the number of demonstrations increases, the performance across five state-of-the-art models gains improvements by a large margin.

<!-- image -->

repair. In addition, we conducted a detailed case study in Appendix C on two representative error cases to further investigate the root causes of these errors.

## 4.4 Impact of Number of Vulnerabilities

In practical applications, OS kernels typically exhibit a limited number of potential vulnerabilities, with few instances of severe vulnerabilities. Consequently, we further explore the impact of varying numbers of vulnerabilities on the performance of specification synthesis, as illustrated in Figure 4. Our observations yield several insights: 1) The pass@1 performance of LLMs tends to decline as the number of vulnerabilities increases. This decline is likely because the presence of more vulnerabilities in the kernel implementation complicates the models' ability to accurately comprehend the functional descriptions. 2) Advanced reasoning models underperform compared to traditional instruction-following models. For instance, GPT4o consistently outperforms o1 and o3-mini across all levels of vulnerability. Similarly, DeepSeek-R1 is less effective than DeepSeek-Chat. These findings align with the observations presented in Table 1. Therefore, reasoning-enhanced models may encounter greater challenges due to the long-context limitations inherent in OS verification scenarios.

## 4.5 Impact of Number of Demonstrations

Recent studies have demonstrated that in-context learning (ICL) significantly enhances the ability of LLMs to acquire new tasks from a limited set of examples (Brown et al., 2020; Dong et al., 2022). In the realm of OS verification, which is inherently complex, the provision of examples illustrating the generation of specifications from functional descriptions and code implementations exerts a substantial influence on performance outcomes. In this study, we explore various ICL settings, specifically zero-shot, one-shot, three-shot, and five-shot learning, as illustrated in Figure 5. It is noteworthy that we exclude discussions on o1 and DeepSeek-R1, owing to their prohibitive cost and time-intensive nature. Our observations reveal that the zero-shot setting (without being shown in the Figure 5) results in complete task failure, with a success rate of 0% across all models, underscoring the critical importance of demonstrations in OS verification contexts. As anticipated, the pass@1 performance of LLMs tends to improve with the provision of additional demonstrations. Notably, DeepSeekChat appears to derive greater benefits from increased demonstrations. While o3-mini surpasses

DeepSeek-Chat in the one-shot context, it underperforms compared to GPT-4o and DeepSeek-Chat in the three- and five-shot scenarios. We hypothesize that the advanced reasoning models currently in use generate extensive chains of reasoning traces, which may challenge the long-context learning capabilities in OS verification scenarios.

## 5 Conclusion

We introduce OSVBench, a robust benchmark for evaluating the performance of LLMs in generating specifications for verifying OS kernels. By framing the specification generation as a program synthesis problem, the benchmark challenges LLMs to navigate complex syntax and semantics within longcontext tasks. Our comprehensive evaluation of 12 powerful LLMs reveals limitations in their current ability to handle these tasks effectively, with notable disparities in performance across models. These findings underscore the need for further advancements in LLM technology to enhance their understanding and generation capabilities in complex domains. OSVBench not only highlights existing gaps but also serves as a valuable tool for guiding future research aimed at improving verification processes in operating system development.

## Limitations

While our OSVBench offers a significant advancement in evaluating LLMs for operating system kernel verification tasks, several limitations must be considered. The benchmark is specifically designed around the Hyperkernel operating system, which may not capture the full diversity of kernel architectures, potentially limiting the generalizability of results to other systems. The complexity of tasks, consisting of approximately 20k to 30k tokens each, poses significant challenges in context management for LLMs, possibly overshadowing other capabilities like logical reasoning. Additionally, the confined scope of syntax and semantics within the benchmark may not fully reflect the dynamic nature of real-world operating system development environments. Current evaluation metrics may not capture qualitative aspects of successful specification generation, such as readability and adaptability, which are crucial for practical implementation. Furthermore, the benchmark lacks realworld feedback loops, such as iterative testing and debugging, limiting its ability to simulate realistic development conditions. Lastly, given the fixed na- ture of tasks and reliance on a single kernel, there's a risk of LLMs overfitting to specific tasks rather than developing broader, adaptable understanding, which can constrain insights into their general capabilities. These limitations can guide future efforts to enhance benchmarks for evaluating LLMs in complex, real-world programming and verification tasks.

## Acknowledgments

We express our gratitude to Sizhe Zhong and Huiri Tan for their valuable comments and feedback. Additionally, we thank Jipeng Zhang for insightful discussions on the initial problem formulation.
<|endofpaper|>