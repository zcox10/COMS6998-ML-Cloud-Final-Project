<|startofpaper|>
## SVD BASED LEAST SQUARES FOR X-RAY PNEUMONIA CLASSIFICATION USING DEEP FEATURES

Mete Erdogan, Sebnem Demirtas

Faculty of Engineering, Koc University, Istanbul, Turkey { merdogan18, sdemirtas20 @ku.edu.tr }

## ABSTRACT

Accurate and early diagnosis of pneumonia through X-ray imaging is essential for e ff ective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and e ffi cient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures e ffi ciency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while o ff ering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications. 1

<!-- image -->

Index Terms -Pneumonia Classification, Regularized Least Squares, Singular Value Decomposition, Transfer Learning, Self-Supervised Learning

## 1. INTRODUCTION

Pneumonia is a critical respiratory condition and one of the foremost causes of morbidity and mortality worldwide, especially among children under five and the elderly [1]. Prompt and accurate diagnosis is vital to ensuring e ff ective treatment and reducing the risk of complications. Chest X-ray imaging remains a standard, cost-e ff ective diagnostic modality in clinical practice. However, the manual interpretation of chest X-ray images is time-intensive and subject to inter-observer variability, motivating the development of automated tools that can assist radiologists in clinical decision-making.

Recent advances in computer vision have led to the widespread adoption of deep learning models such as Convolutional Neural Networks (CNNs) [2, 3] and Vision Transformers (ViTs) [4] for pneumonia detection from X-ray images. These models achieve impressive performance but often rely on

1 The implementation is available at: github.com meterdogan07 SVD-LS / /

Fig. 1 : Showcasing the SVD-LS workflow for classifying pneumonia in chest X-rays. The process involves extracting features using self-supervised (SSL) or transfer learning (TL) models, applying Singular Value Decomposition (SVD) for dimensionality reduction, and performing closed-form regularized least-squares to classify into categories: healthy, viral, or bacterial pneumonia.

large-scale annotated datasets, intensive training, and or fine-/ tuning procedures. Such computational and data demands may limit their deployment in resource-constrained healthcare settings or real-time applications.

In this paper, we propose a method that combines the representational power of modern feature learning with the e ffi ciency and analytical tractability of classical machine learning. Specifically, we introduce a Singular Value Decomposition-based Least Squares (SVD-LS) scheme for multi-class pneumonia classification. Our method leverages pretrained feature extractors from self-supervised learning (SSL) [5, 6] and transfer learning [7] to obtain robust image representations. These features are then fed into a closed-form SVD-based least squares classifier, enabling e ffi cient, noniterative training and inference (see Figure 1 for illustration).

Compared to conventional end-to-end fine-tuning, our approach o ff ers substantial computational savings and improved numerical stability, while maintaining competitive accuracy. Moreover, the explicit linear modeling of our framework enhances interpretability, an essential requirement in clinical decision-support systems. We validate the e ffi cacy of our method through comprehensive experiments on publicly available CXR datasets, demonstrating its potential for scalable and reliable pneumonia classification.

Our key contributions are as follows:

- · We propose a novel application of Singular Value Decomposition-based Least Squares (SVD-LS) estimation for multi-class pneumonia classification using chest X-ray images, o ff ering an e ffi cient alternative to iterative deep learning classifiers.
- · We integrate rich visual representations extracted from state-of-the-art self-supervised and transfer learning models, demonstrating that our closed-form classification approach can e ff ectively leverage these features without additional gradient-based fine-tuning.
- · We conduct comprehensive experiments comparing the performance of SVD-LS against conventional deep learning pipelines, highlighting its competitive accuracy in real-world medical imaging scenarios.

## 2. RELATED WORK

Self-supervised learning has proven to be an e ff ective strategy, especially in situations when obtaining labeled data is costly or di ffi cult. Through the utilization of self-supervised learning techniques, models can acquire valuable representations without substantial human annotation, by capitalizing on the underlying structure and patterns contained in unlabeled data. This paradigm has shown remarkable success in fields such as computer vision and natural language processing [8, 9, 5]. In particular, DINO (self-DIstillation with NO labeling) transformer models have attracted a lot of interest in the area of vision tasks [10]. DINO employs a self-distillation framework where a student network learns from a progressively updated teacher network. Though initially identical, the two networks evolve independently during training, facilitating the learning of rich and semantically meaningful visual features. Transformer-based DINO models excel at capturing both local and global structures in images, making them highly e ff ective for self-supervised classification, segmentation, and representation learning tasks.

Furthermore, transfer learning has emerged as a fundamental method in machine learning, allowing pretrained models to adapt to new tasks with limited labeled data. Transfer learning uses the information gathered from large-scale datasets and challenging tasks to help models improve generalization and training e ffi ciency when applied to new tasks. This greatly minimizes the requirement for large labeled datasets and computational resources. In particular, ResNet models, presented by He et al. [3], transformed deep learning by using residual connections to solve the vanishing gradient problem. ResNet has shown impressive results in applications involving image detection and classification, by enabling training deeper architectures. Pretrained ResNet models on massive datasets, like ImageNet, have been widely adopted as strong backbones for various downstream applications [11]. For instance, Chouhan et al. [12] successfully applied an ensemble of transfer-learned CNNs for pneumonia detection, demonstrating the e ff ectiveness of such models in medical imaging. Recent advances also combine transfer learning with self-supervised pretraining to further enhance performance in low-data regimes [13].

Extreme Learning Machines (ELMs) [14] are machine learning models known for their rapid training and strong generalization. They operate by randomly initializing the feature extractor, fixing input weights and biases, and then computing output weights with a least-squares solution. This method avoids iterative optimization, but relies on random projections, limiting the quality of the extracted features. Other works sought to improve e ffi ciency by refining these random features using SVD [15]; however, they still depend on random feature extractors that may lack the robustness and discriminative power of pretrained models. In our work, rather than randomly initialized feature extractor networks, we use either the transfer learning and self-supervised learning based models. This approach also sidesteps the significant computational demands of methods like the hybrid CNN-PCA strategy used for pneumonia classification [16], which requires training the feature extractor on the target dataset before applying dimensionality reduction.

Our SVD-LS method combines the strengths of e ffi cient linear classifiers with better feature representations. After extracting discriminative features from pretrained models, we apply SVD for e ff ective dimensionality reduction and then solve a regularized least squares problem to train the classifier. This pipeline not only preserves the computational e ffi ciency of traditional ELM frameworks but also enhances classification accuracy, thereby providing an improved alternative for pneumonia classification.

## 3. METHODOLOGY

## 3.1. SVD-Based Least Squares for Classification

We formulate multi-class pneumonia classification as a regularized linear regression problem. Let X ∈ R N × d denote the matrix of d -dimensional deep feature vectors for N samples, and let Y ∈ R N × m be the corresponding one-hot encoded label matrix for m classes. The objective is to learn a linear mapping W ∈ R d × m that minimizes the regularized least squares loss:

$$\min _ { W } \| X W - \mathbf Y \| _ { F } ^ { 2 } + \lambda \| \mathbf W \| _ { F } ^ { 2 },$$

where λ &gt; 0 is a regularization parameter and ∥ · ∥ F denotes

Algorithm 1 Classification with SVD-based Least Squares

Require: Pretrained feature extractor F , training dataset D train , test dataset D test , regularization parameter λ , number of principal components k

Ensure: Trained SVD-LS classifier W

- 1: Extract features: X train ← F ( D train ), X test ← F ( D test )
- 2: Compute SVD: X train = U V Σ T
- 3:
- Reduce dimensionality: ˜ X train = X train V [1 : k ], ˜ X test = X test V [1 : k ]
- 4: Solve closed-form SVD-LS:

$$W = ( \tilde { X } _ { \text{train} } ^ { T } \tilde { X } _ { \text{train} } + \lambda I ) ^ { - 1 } \tilde { X } _ { \text{train} } ^ { T } \tilde { Y } _ { \text{train} }$$

- 5: Classify test samples: ˆ Y = ˜ X test W
- 6: return Predicted labels ˆ Y

the Frobenius norm. This yields the standard ridge regression:

$$W = ( X ^ { T } X + \lambda I ) ^ { - 1 } X ^ { T } Y.$$

To improve computational e ffi ciency and generalization, we apply Singular Value Decomposition (SVD) to X :

$$\mathbf X = U \Sigma \mathbf V ^ { T },$$

where V ∈ R d × d contains the right singular vectors (principal directions), and Σ is a diagonal matrix of singular values. We retain the topk singular vectors V 1: k , projecting the features into a lower-dimensional subspace:

$$\tilde { X } = X V _ { 1 \colon k }.$$

We then solve the least squares problem in this compact space:

$$W = ( \tilde { X } ^ { T } \tilde { X } + \lambda I ) ^ { - 1 } \tilde { X } ^ { T } Y. \quad \quad ( 1 ) \quad ^ { \text{dev} } _ { \mathbb { m } A }$$

This projection enhances both e ffi ciency and robustness, especially when d is large or when training data is limited. The use of SVD can be viewed as a form of implicit kernel mapping. Traditional kernel methods project inputs via a nonlinear function ϕ ( ) into a high-dimensional feature space that would · enable linear separation:

$$K ( { \mathbf x } _ { i }, { \mathbf x } _ { j } ) = \phi ( { \mathbf x } _ { i } ) ^ { T } \phi ( { \mathbf x } _ { j } ). \quad \quad ( 2 ) \quad \text{for}$$

Instead of explicitly defining ϕ , the SVD projects data onto an orthonormal basis defined by the directions of greatest variance (i.e., principal components). This projection captures the most discriminative structure in the data, similar to how kernel PCA [17, 18] captures variance in a nonlinear space. Thus, the SVD-based projection behaves like a data-adaptive kernel, enabling more e ff ective linear classification in the reduced space. Therefore, the SVD-LS classifier provides a computationally e ffi cient alternative to iterative gradient-based methods while maintaining robustness to feature noise. Moreover, the same

Least-Squares solution as in Eq. 1 can also be derived from a statistical viewpoint as the (Linear) Minimum Mean Squared Error (MMSE) estimator [19, 20, 21]. When the features and labels are centered, the optimal weights are given:

$$\mathbf W = C _ { Y X } C _ { X X } ^ { - 1 },$$

where C XX = E [ X X T ] is the input covariance matrix and C YX = E [ Y X T ] is the cross-covariance matrix between labels and inputs. This view reinforces that our approach leverages second-order statistics to learn an optimal linear predictor, o ff ering improved interpretability and sample e ffi ciency compared to black-box classifier solutions. These principles also form the basis of classical recursive estimators like the Kalman filter [22, 23], which have been e ff ectively combined with machine learning techniques; for instance, event detection tasks [24, 25].

Our complete SVD-LS pipeline combines four key steps to achieve e ffi cient and scalable classification with minimal training overhead. First, we extract deep features from a pretrained model; then, we apply SVD to obtain a low-rank representation. Next, we solve a closed-form regularized least squares problem to learn optimal weights, and finally, we classify new samples using these learned weights. To further clarify the procedure, Algorithm 1 summarizes the entire workflow. By following this structured approach, we e ffi ciently classify pneumonia cases while maintaining computational e ffi ciency.

## 3.2. Dataset and Preprocessing

We use the publicly available Guangzhou Women and Children's Medical Center dataset [26]. For the preprocessing of the images, we started by resizing to 224x224 pixels to ensure uniform input dimensions. Then, we apply mean and standard deviation normalization by calculating training data statistics and performed adaptive histogram equalization (CLAHE) to enhance the model performance and reliability. In later steps, we use the training portion of the dataset to perform k-fold cross validation where we apply data augmentation to the training set of each fold for balancing the class distributions, including random horizontal flip, random rotation (10 degrees) and gaussian blur. Also, while giving the greyscale inputs to the models we duplicate the image tensors to three channels for model compatibility.

## 3.3. Baselines with Pretraining and Fine Tuning Using Gradient Descent

By using the gradient descent, we train the following models on our pneumonia classification dataset: 1) ResNet-18 trained from scratch, 2) ResNet-18 where all parameters trained after transfer learning initialization, 3) ResNet-18 where the last linear layer is fine-tuned after transfer learning initialization and 4) DINO ViT-B 16 model fine-tuned using a linear classifier. / The results of these baseline models are presented in Table 1.

|                      | Train        | Train        | Train      | Train         | Test         | Test         | Test       | Test          |
|----------------------|--------------|--------------|------------|---------------|--------------|--------------|------------|---------------|
| Model                | F1-Score (%) | Accuracy (%) | Recall (%) | Precision (%) | F1-Score (%) | Accuracy (%) | Recall (%) | Precision (%) |
| M1: ResNet-18 Rand.  | 96.17        | 97.51        | 95.99      | 96.36         | 69.74        | 81.20        | 71.20      | 74.71         |
| M2: ResNet-18 TL     | 97.34        | 98.22        | 97.31      | 97.38         | 73.54        | 83.76        | 74.65      | 77.56         |
| M3: ResNet-18 FT     | 78.63        | 86.12        | 79.13      | 78.22         | 77.82        | 86.00        | 79.35      | 79.37         |
| M4: ResNet-18 SVD-LS | 83.19        | 89.12        | 82.57      | 84.05         | 74.69        | 84.29        | 76.53      | 78.68         |
| M5: ResNet-50 SVD-LS | 66.85        | 80.99        | 67.56      | 70.68         | 65.69        | 79.06        | 65.23      | 70.70         |
| M6: DINO FT          | 77.99        | 86.45        | 77.87      | 79.10         | 71.37        | 81.84        | 71.59      | 74.09         |
| M7: DINO SVD-LS      | 83.59        | 89.61        | 83.40      | 84.22         | 84.27        | 90.28        | 84.49      | 85.49         |

Table 1 : Average performance metrics for our models on the train and test datasets in a three-class pneumonia classification task. We evaluated several approaches: (M1) ResNet-18 trained from scratch with random initialization; (M2) ResNet-18 fully trained using transfer learning; (M3) ResNet18 with fine-tuning of the final layer via gradient descent; (M4) ResNet-18 combined with our SVD-LS method; (M5) ResNet-50 with SVD-LS; (M6) a DINO ViT-B 16-based linear classifier fine-tuned with gradient / descent; and (M7) DINO ViT-B 16 with SVD-LS. Best-performing metrics are given in bold, and the second-best as underlined. /

(a) ResNet-18 TL

<!-- image -->

Fig. 2 : t-SNE visualization of feature vectors extracted from (a) ResNet-18 with transfer learning and (b) DINO ViT-B 16. / Each point represents a sample, colored by class label.

<!-- image -->

## 3.4. Training Specifications

In the training of each of the gradient descent based methods, we used the Adam [27] optimizer with weight decay. We performed a grid-search to find the optimal values for the hyperparameters, and k-fold cross validation with k = 5 on each grid-search combination. The training parameters found are presented in Table 2. As we have a fixed test set, for each k-fold we divided the training set into 80% (training) and 20% (validation) portions and reported the test set results of the model with the best validation F1-score. Furthermore, for the SVD-LS classification framework, we obtain a linear classifier on top of the following models: 1) ResNet-18, 2) ResNet-50, and 3) DINO ViT-B 16. Extracted features from these models / are used as inputs to our SVD-LS classifier. This allows us to leverage the representation power of deep networks while retaining the e ffi ciency by closed-form solutions. We again perform grid-search and k-fold cross-validation with k = 5 to determine the optimal number of principal components to retain after SVD-based dimensionality reduction.

## 4. RESULTS

The results of our experiments, shown in Table 1, demonstrate clear di ff erences in performance across various models for multi-class pneumonia classification on chest X-ray images. Among the evaluated models, the DINO SVD-LS model, which combines a self-supervised transformer with SVD-based Least Squares, outperformed all other models on the test set, achieving an F1-score of 84.27% and an accuracy of 90.28%. This highlights the superior generalization capabilities of self-supervised models using SVD-LS. ResNet-18 fine-tuning (M3), which involves training only the last layer, demonstrated the second-best test performance with an F1score of 77.82% and an accuracy of 86.00%. While ResNet-18 SVD-LS (M4) and ResNet-50 SVD-LS (M5) also utilized SVD-LS, their performance was inferior to DINO SVD-LS, with M5 underperforming, indicating higher model complexity without proper feature extraction may hurt performance.

Table 2 : Training details of gradient descent based models.

| Model                    |   Epoch | Optimizer   |   Learning Rate |   Weight Decay |   Batch Size | Loss          | Scheduler   |
|--------------------------|---------|-------------|-----------------|----------------|--------------|---------------|-------------|
| ResNet-18 From Scratch   |      30 | Adam        |          0.0001 |         1e-05  |           16 | Cross Entropy | StepLR      |
| ResNet-18 Transfer Learn |      30 | Adam        |          0.0005 |         1e-05  |           32 | Cross Entropy | StepLR      |
| ResNet-18 Fine-Tuning    |      30 | Adam        |          0.001  |         1e-05  |           16 | Cross Entropy | StepLR      |
| DINO Fine-Tuning         |      30 | Adam        |          0.001  |         0.0001 |           16 | Cross Entropy | StepLR      |

To better understand the quality of learned features, we visualize the t-SNE [28] projections of feature vectors from both the DINO ViT-B 16 and ResNet-18 transfer-learned / models (see Figure 2). While both show some class separation, DINO features form more compact and well-separated clusters, indicating stronger class-discriminative structure. This makes DINO features particularly suitable for linear classifiers. Despite their separability, these high-dimensional features may still contain redundant or noisy components. SVD addresses this by projecting them onto a lower-dimensional subspace spanned by the most informative directions, reducing overfitting and improving numerical stability. The t-SNE plots qualitatively suggest a low-dimensional manifold structure, which SVD can capture e ff ectively, enabling the least squares classifier to generalize better in this reduced space.

Table 3 : Pairwise McNemar-Bowker Test [29] results comparing classifier predictions. Significant p-values ( &lt; 0.05) indicate meaningful performance di ff erences.

| Model   | M1    | M2    | M3    | M4    | M5    | M6    | M7    |
|---------|-------|-------|-------|-------|-------|-------|-------|
| M1      | -     | 0.000 | 0.013 | 0.000 | 0.000 | 0.000 | 0.000 |
| M2      | 0.000 | -     | 0.082 | 0.000 | 0.000 | 0.000 | 0.000 |
| M3      | 0.013 | 0.082 | -     | 0.000 | 0.005 | 0.000 | 0.001 |
| M4      | 0.000 | 0.000 | 0.000 | -     | 0.117 | 0.000 | 0.074 |
| M5      | 0.000 | 0.000 | 0.005 | 0.117 | -     | 0.000 | 0.001 |
| M6      | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | -     | 0.000 |
| M7      | 0.000 | 0.000 | 0.001 | 0.074 | 0.001 | 0.000 | -     |
<|endofpaper|>