<|startofpaper|>
## LTL f Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains

## Giuseppe De Giacomo 1, 2 , Gianmarco Parretti , Shufang Zhu 2 3

1 University of Oxford 2 3

University of Rome 'La Sapienza' University of Liverpool giuseppe.degiacomo@cs.ox.ac.uk, parretti@diag,uniroma1.it, shufang.zhu@liverpool.ac.uk

## Abstract

Westudy a variant of LTL f synthesis that synthesizes adaptive strategies for achieving a multi-tier goal, consisting of multiple increasingly challenging LTL f objectives in nondeterministic planning domains. Adaptive strategies are strategies that at any point of their execution ( ) enforce the satisfaction of i as many objectives as possible in the multi-tier goal, and ( ii ) exploit possible cooperation from the environment to satisfy as many as possible of the remaining ones. This happens dynamically: if the environment cooperates ( ii ) and an objective becomes enforceable ( ), then our strategies will enforce it. i We provide a game-theoretic technique to compute adaptive strategies that is sound and complete. Notably, our technique is polynomial, in fact quadratic, in the number of objectives. In other words, it handles multi-tier goals with only a minor overhead compared to standard LTL f synthesis.

2006; Bienvenu, Fritz, and McIlraith 2006; Jorge, McIlraith et al. 2008) addresses this challenge by allowing users to specify preferences over the plans. The aim in PBP is to produce plans that satisfy as many user-specified preferences as possible, such as goal preferences, action preferences, state preferences and temporal preferences (Baier, Bacchus, and McIlraith 2009). Planning with soft goals (Keyder and Geffner 2009) is a simple model of PBP, which aims to maximize utility in cases where the agent may not be able to achieve all its goals. PBP frameworks optimize the selection of objectives, deciding which to achieve and which to discard, with the discarded ones being those that do not contribute to optimal preference satisfaction.

## 1 Introduction

There has been a growing interest in Reasoning about Actions, Planning, and Sequential Decision Making, to techniques from formal methods for strategic reasoning such as reactive synthesis (Pnueli and Rosner 1989; Fijalkow et al. 2024), which can play a crucial role in developing autonomous AI systems capable of self-programming their actions to complete desired goals. Such capabilities are particularly important when these systems operate within complex, dynamic environments, in particular, with high levels of nondeterminism.

Most of the literature on strategy synthesis (Fijalkow et al. 2024; De Giacomo and Vardi 2015; De Giacomo and Rubin 2018; Camacho, Bienvenu, and McIlraith 2019) and planning (Ghallab, Nau, and Traverso 2016; Haslum et al. 2019) assumes that the AI system or the agent , is working towards a singular goal (typically specified, e.g., in standard reachability (Haslum et al. 2019), or in Temporal Logics (Aminof et al. 2019; Camacho, Bienvenu, and McIlraith 2019)). The focus of planners/synthesizers is generally on finding a plan/strategy as quickly as possible (Haslum et al. 2019; Geffner and Bonet 2013; Fijalkow et al. 2024). However, in real-world applications, users often struggle to specify an agent goal accurately using only high-level characteristics. Preferences-based planning (PBP) (Son and Pontelli

Copyright © 2025, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

In this paper we focus on Fully Observable Nondeterministic Domains (FOND) (Geffner and Bonet 2013) and consider temporal preferences that are ordinal (Son and Pontelli 2006; Bienvenu, Fritz, and McIlraith 2006). Specifically, the agent goals are multi-tier goals consisting of a multi-tier hierarchy of increasingly challenging objectives expressed in Linear Temporal Logic on finite traces (LTL f ) (De Giacomo and Vardi 2013). This enables users to specify when a strategy is better than another by simply considering additional tasks, e.g., a strategy that enforces 'clean room A and deliver package B, φ 2 = ♢ ( clean A ) ∧ ♢ ( deliver B ) ' is better than another strategy that only enforces 'clean room A φ 1 = ♢ ( clean A ) '. If we simply aim at satisfying as many objectives as possible, considering that they are in a hierarchy of increasingly challenging objectives, the problem reduces to finding the most challenging objective for which a winning strategy can be synthesized. This can be done with off-the-shelf techniques (De Giacomo and Rubin 2018).

However, as observed in (Shaparau, Pistore, and Traverso 2006) in the context of FOND for reachability goals, in nondeterministic domains it is interesting to make the optimality criteria depend on the state of the domain and the response of the environment. Importantly, all objectives must remain under consideration at all instants, as objectives that are currently unachievable could become feasible later due to nondeterministic responses. For example while currently only φ 1 = ♢ ( clean A ) is achievable for any environment response, a specific environment response might make φ 2 = ♢ ( clean A ) ∧ ♢ ( deliver B ) achievable as well.

Embracing this intuition, we seek adaptive strate-

gies 1 (plans) that at any point during execution ( ) enforce i the satisfaction of as many LTL f objectives as possible, and ( ii ) exploit possible environment cooperation to satisfy as many remaining ones as possible. This happens dynamically: if the environment cooperates ( ii ) and an objective becomes enforceable ( ), then our strategies will enforce it. i

Formally we base our approach on best-effort strategies (Aminof et al. 2020, 2021; Aminof, De Giacomo, and Rubin 2021). Best-effort strategies are strategies that enforce objective satisfaction whenever possible, and do nothing that needlessly prevent satisfying the objective, otherwise. Intuitively, if an objective that is not enforceable now but could still become enforceable later during execution due to possible cooperative environment response, best-effort strategies will exploit such response to enforce objective satisfaction. In this sense we can think best-effort strategies as adaptive strategies for a single objective. Note, if winning strategies exist, then best-effort strategies are exactly the winning strategies. However while winning strategies may not exist, best effort strategies always do. Best-effort strategies for LTL f objectives can be computed in worst-case 2EXPTIME, just as for winning strategies (best-effort synthesis is 2EXPTIME-complete, just as in standard synthesis) (Aminof, De Giacomo, and Rubin 2021). Note that such worst-case blowup depends on the need to build a DFA for the LTL f formula, and occurs only rarely in practice, see, e.g. (De Giacomo and Vardi 2015; Camacho et al. 2019; Zhu et al. 2020; Geatti, Montali, and Rivkin 2024).

Leveraging results on best-effort strategies, we study synthesis of adaptive strategies for multi-tier goals. Specifically we want to synthesize a strategy that in every history fulfills a maximal number of objectives. Hence, an adaptive strategy for a multi-tier goal keeps all objectives active by winning as many objectives as possible, regardless of environment responses, and not ruling out completion for as many as possible of the remaining ones. In particular, we prove that such strategies (as best-effort strategies for single objectives) always exist (see Theorem 1).

Our main contribution is a game-theoretic technique to compute an adaptive strategy for multi-tier goals that is sound and complete, as well as practically efficient. Our technique first identifies the most challenging objective that the agent can win in spite of the environment, which is referred to as maximally winning objective . This objective represents the highest tier, i.e., most challenging, that the agent can enforce against all possible environment behaviors, hence also enforcing lower-tier, i.e., easier, objectives in the multi-tier goal hierarchy. Then, we identify the maximally winning-pending objective , which is higher than (or equal to) the maximally winning one. In this way we can win for the maximally winning objective and leave open the possibility to further win as many as possible of the highertier objectives if the environment cooperates. To compute the maximally winning-pending objective, our approach re-

1 We use the term adaptive to qualify strategies with the three properties above, but the term 'adaptive' can be used to denote very different concepts, see e.g., (D'Ippolito et al. 2014; Ciolek et al. 2020; Rodr´ ıguez and S´nchez 2024). a

quires consideration of only the maximally winning objective and each of those more challenging objectives. As a result, our technique remains worst-case 2EXPTIMEcomplete in the size of the objectives in the LTL f multi-tier goal as standard LTL f synthesis, but notably it is polynomial , in fact quadratic, in the number of objectives (Theorem 5). In this way, we are able to handle multi-tier goals with only a minor overhead compared to standard LTL f synthesis.

## 2 Preliminaries

A trace over an alphabet of symbols Σ is a finite or infinite sequence of elements from Σ . The empty trace is λ . Traces are indexed starting at zero, and we write π = π π 0 1 · · · . The length of a trace is | π | . For a finite trace π , we denote by lst ( π ) the index of the last element of π , i.e., | π | -1 .

LTL . f Linear Temporal Logic on finite traces (LTL f ) is a specification language to express temporal properties for finite and non-empty traces (De Giacomo and Vardi 2013). LTL f shares the syntax with LTL (Pnueli 1977), but is interpreted over finite non-empty traces. Given a set of atoms AP , the LTL f formulas over AP are generated as follows:

p ∈ AP is an atom , ◦ ( Next ), and U ( Until ) are temporal operators. We use standard Boolean abbreviations such as ∨ (or) and ⊃ (implies), true and false . Moreover, we define the following abbreviations Weak Next · φ ≡ ¬ ◦ ¬ φ , Eventually ♢ φ ≡ true U φ and Always □ φ ≡ ¬ ♢ ¬ φ . The size of φ , written | φ | , is the number of all its subformulas.

$$\varphi \colon = p \, | \, \varphi _ { 1 } \wedge \varphi _ { 2 } \, | \, \neg \varphi \, | \, \circ \varphi \, | \, \varphi _ { 1 } \mathcal { U } \varphi _ { 2 }.$$

LTL f formulas are interpreted over finite non-empty traces π over the alphabet Σ = 2 AP , i.e., the alphabet consisting of the propositional interpretations of the atoms. Thus, for i ≤ lst ( π ) , we have that π i ∈ 2 AP is the i -th interpretation of π . An LTL f formula φ holds at instant i of a trace π is defined inductively on the structure of φ as:

- · π, i | = p iff p ∈ π i ;

̸

- · π, i | = ¬ φ iff π, i | = φ ;
- · π, i | = φ 1 ∧ φ 2 iff π, i | = φ 1 and π, i | = φ 2 ;
- · π, i | = ◦ φ iff i &lt; lst ( π ) and π, i +1 = | φ ;
- · π, i | = φ 1 U φ 2 iff ∃ j such that i ≤ j ≤ lst ( π ) and π, j | = φ 2 , and ∀ k, i ≤ k &lt; j we have that π, k | = φ 1 .

We say that π satisfies φ , written as π | = φ , if π, 0 = | φ .

↦

Nondeterministic Planning Domains. Following (De Giacomo, Parretti, and Zhu 2023b), a nondeterministic planning domain is a tuple P = (2 F , s 0 , Act, React, α, β, δ ) , where: F is a finite set of fluents, |F| is the size of P , and 2 F is the state space; s 0 ∈ 2 F is the initial state; Act is a finite set of agent actions; React is a finite set of environment reactions; α : 2 F → 2 Act denotes agent action preconditions; β : 2 F × Act → 2 React denotes environment reaction preconditions; and δ : 2 F × Act × React → 2 F is the transition function such that δ s, a, r ( ) is defined iff a ∈ α s ( ) and r ∈ β s, a ( ) . As in (De Giacomo, Parretti, and Zhu 2023b), we require nondeterministic planning domains to satisfy: · Existence of agent action : ∀ s ∈ 2 F . ∃ a ∈ α s ( ) ; · Existence of environment reaction : ∀ s ∈ 2 F , a ∈ α s . ( ) ∃ r ∈ β s, a ( ) ; · Uniqueness of environment reaction : ∀ s ∈ 2 F , a ∈ α s .δ s, a, r ( ) ( 1 ) = δ s, a, r ( 2 ) ⊃ r 1 = r 2 .

These properties allow the defined nondeterministic planning domain to capture classical FOND domains (Cimatti, Roveri, and Traverso 1998; Geffner and Bonet 2013) expressed in PDDL (Haslum et al. 2019) by introducing reactions corresponding to oneof clauses of agent actions (De Giacomo, Parretti, and Zhu 2023a).

A trace of P is a finite or infinite sequence τ = s 0 ( a , r 1 1 , s 1 ) · · · , where s 0 is the initial state of P , and s i , a i , and r i , are the state, agent action, and environment reaction, respectively, at the i -th time step. A trace is legal if for every i &gt; 0 : ( ) i a i ∈ α s ( i -1 ) ; ( ii ) r i ∈ β s ( i -1 , a i ) ; and ( iii ) s i = ( δ s i -1 , a i , r i ) . We denote by H P the set of legal traces of P . Given a trace τ = s 0 ( a , r 1 1 , s 1 ) · · · ( a n , r n , s n ) , we denote τ projected on the states by τ | 2 F = s 0 · · · s n , and τ projected on agent actions and environment reactions by τ | Act × React = ( a , r 1 1 ) · · · ( a n , r n ) .

An agent strategy is a partial function σ : (2 F ) + → Act that maps sequences of states of the domain to agent actions. We write σ τ ( ) = ⊥ to denote that σ is undefined in τ , which we think of as the strategy terminating its execution. Note that such agent strategies definitions are more in line with existing works on planning and reasoning about actions. However, in synthesis agent strategies are typically defined as σ ′ : React ∗ → Act that map sequences of environment reactions to agent actions. However, for the planning domains we consider, we can equivalently define agent strategies as σ : (2 F ) + → Act or σ ′ : React ∗ → Act . The equivalence follows by noting that: ( ) i every strategy σ : (2 F ) + → Act directly corresponds to a strategy σ ′ : React ∗ → Act by the requirement of uniqueness of environment reaction; ( ii ) every strategy σ ′ : React ∗ → Act directly corresponds to a strategy σ : (2 F ) + → Act since the transition function δ is deterministic. An agent strategy σ is legal if, for every legal trace τ = s 0 ( a , r 1 1 , s 1 ) · · · ( a n , r n , s n ) , if σ τ ( | 2 F ) is defined, then σ τ ( | 2 F ) ∈ α s ( n ) , so that a legal agent strategy always satisfies action preconditions.

An environment strategy is a total function γ : ( Act ) + → React mapping sequences of agent actions to environment reactions. An environment strategy γ is legal if, for every legal trace τ = s 0 ( a , r 1 1 , s 1 ) · · · ( a n , r n , s n ) and agent action a n +1 ∈ α s ( n ) , we have that γ a ( 1 · · · a n +1 ) ∈ β s ( n , a n +1 ) , so that a legal environment strategy always satisfies reaction preconditions. In the rest of the paper, we consider only legal agent strategies and legal environment strategies.

An agent strategy σ and an environment strategy γ induce a unique legal trace on P that is consistent with both, defined as Play ( σ, γ ) = s 0 ( a , r 1 1 , s 1 ) · · · , where: ( ) i s 0 is the initial state; ( ii ) for every i &gt; 0 , a i = σ s ( 0 · · · s i -1 ) , r i = γ a ( 1 · · · a i ) , and s i = ( δ s i -1 , a i , r i ) ; and ( iii ) if Play ( σ, γ ) is finite, say Play ( σ, γ ) = s 0 ( a , r 1 1 , s 1 ) · · · ( a n , r n , s n ) , then σ s ( 0 · · · s n ) = ⊥ . Note that Play ( σ, γ ) always exists by the properties of Existence of agent action and Existence of environment reaction.

Winning and Cooperative Strategies. Given a domain P , an objective φ as an LTL f formula over the fluents in P . A finite legal trace τ satisfies φ in P , written τ | = P φ , if τ | 2 F | = φ . We denote by L P ( φ ) the set of legal traces of P

satisfying φ . An agent strategy σ is cooperative for φ in P if Play ( σ, γ ) is finite and Play ( σ, γ ) | = P φ for some environment strategy γ . Furthermore, σ is winning for (or enforces ) φ in P if Play ( σ, γ ) is finite and Play ( σ, γ ) | = P φ for every environment strategy γ .

LTL f Best-Effort Synthesis. LTL f best-effort synthesis in nondeterministic domains computes a strategy that enables the agent to do its best to satisfy an LTL f objective in a nondeterministic domain (Aminof, De Giacomo, and Rubin 2021; De Giacomo, Parretti, and Zhu 2023a).

̸

The notion of best-effort strategy bases on the gametheoretic relation of dominance . An agent strategy σ 1 dominates an agent strategy σ 2 (for φ in P ), written σ 1 ≥ φ |P σ 2 if, for every environment strategy γ , Play ( σ , γ 2 ) is finite and Play ( σ , γ 2 ) | = P φ implies Play ( σ , γ 1 ) is finite and Play ( σ , γ 1 ) | = P φ . Furthermore, σ 1 strictly dominates σ 2 , written σ 1 &gt; φ |P σ 2 , if σ 1 ≥ φ |P σ 2 and σ 2 ≥ φ |P σ 1 .

Intuitively, σ 1 &gt; φ |P σ 2 means that σ 1 does at least as well as σ 2 against every environment strategy and strictly better against at least one such strategy. An agent using σ 2 is not doing its 'best' to satisfy the goal. If the agent used σ 1 instead, it could achieve the goal against a strictly larger set of environment strategies. As a result, a best-effort strategy σ is one that is not strictly dominated by any other strategy, i.e., no other strategy σ ′ exists such that σ ′ &gt; φ |P σ .

Best-effort strategies also admit an alternative characterization that describes their behavior when executed after a history, i.e., a finite legal trace. Given a domain P , an agent strategy σ , and a history h , we denote by Γ P ( σ, h ) the set of environment strategies γ such that h is a prefix of Play ( σ, γ ) . Also, we denote by H P ( σ ) the set of histories h such that Γ P ( σ, h ) is non-empty, i.e., the set of histories that are consistent with σ and some environment strategy γ . Given an agent goal φ , we define:

- · val φ |P ( σ, h ) = win ( σ is winning for φ from h ) if Play ( σ, γ ) is finite and Play ( σ, γ ) | = P φ for every γ ∈ Γ P ( σ, h ) ;
- · val φ |P ( σ, h ) = pend ( σ is pending for φ from h ) if, Play ( σ, γ ) is finite and Play ( σ, γ ) | = P φ for some γ ∈ Γ P ( σ, h ) , but not all;
- · val φ |P ( σ, h ) = lose ( σ is losing for φ from h ), otherwise. The value of h , written val φ |P ( h ) 2 , is the maximum of val φ ( σ, h ) for every agent strategy σ such that h ∈ H P ( σ ) . The history-based characterization of best-effort strategies is as follows: an agent strategy σ is best-effort for φ in P iff val φ |P ( σ, h ) = val φ |P ( h ) for every h ∈ H P ( σ ) .

By the history-based characterization, a best-effort strategy σ behaves as follows. Starting from every history h ∈ H P ( σ ) : ( i ) if φ is enforceable regardless of the nondeterminism in the domain, σ enforces φ ; else, ( ii ) if φ is satisfiable only depending on how the nondeterminism in the domain unfolds, σ satisfies φ if the environment cooperates; else, ( iii ) if φ is unsatisfiable, σ prescribes some action which does not violate action preconditions. Note that a best-effort strategy adapts so that if a goal becomes enforceable due to environment cooperation, then the strategy will enforce it.

2 We define val φ ( h ) only if there exists σ s.t. h ∈ H ( σ ) .

By the definition of best-effort strategy, it follows that, if a winning strategy exists, the best-effort strategies are exactly the winning strategies. Furthermore, best-effort strategies have the notable property that they always exist. LTL f best-effort synthesis is 2EXPTIME-complete in the size of the goal φ and EXPTIME-complete in the size of the domain P (De Giacomo, Parretti, and Zhu 2023b).

## 3 Multi-Tier Goals

In this paper, we address synthesis with multi-tier goals consisting of a multi-tier hierarchy of increasingly challenging LTL f objectives: the initial objective is the simplest, with each subsequent objective adding more obligations, making the final objective the most challenging. Formally, we define a multi-tier goal specified in LTL f as follows.

Definition 1 (Multi-Tier Goal) . Given a nondeterministic planning domain P with fluents F , a multi-tier LTL f goal is a sequence of LTL f objectives Φ = ⟨ φ , 1 · · · , φ n ⟩ over F such that L P ( φ 1 ) ⊇ · · · ⊇ L P ( φ n ) .

Multi-tier LTL f goals can capture ordinal temporal preferences (Son and Pontelli 2006), which enable users to specify when a strategy is better than another. Given a multitier goal Φ = ⟨ φ , 1 · · · , φ n ⟩ , we can view φ 1 as the 'hard' objective that the agent should enforce, and the various φ i (where i &gt; 1 ) as 'soft' objectives that refine φ 1 , which the agent should satisfy if possible (Keyder and Geffner 2009). As a result, the strategies that satisfy φ i are preferred to those that only satisfy φ j for j &lt; i .

It follows by Definition 1 that if an agent strategy enforces (resp. cooperates for) φ i , then it also enforces (resp. cooperates for) every φ j such that j &lt; i .

One important aspect of synthesis in nondeterministic domains is that, at every time step, an objective φ can be:

- · Enforceable , i.e., there exists an agent strategy that satisfies φ regardless of adversarial environment responses;
- · Pending , i.e., there exists an agent strategy that satisfies φ should the environment cooperatively respond;
- · Unsatisfiable , i.e., no agent strategy exists that satisfies φ regardless of environment responses.

It is worth noting that an objective that is currently pending might become enforceable during execution should the environment cooperate (or unsatisfiable should the environment be adversarial). Hence in order to fully exploit the nondeterminism in a nondeterministic domain considering a multi-tier goal, the agent should employ an adaptive strategy , i.e., a strategy with the following properties:

- · Property 1. The strategy enforces the satisfaction of all objectives that are currently enforceable;
- · Property 2. The strategy exploits possible cooperation from the environment to satisfy as many as possible of the currently pending objectives;
- · Property 3 . Dynamically, at each time step, if the environment cooperates and a pending objective becomes enforceable, the strategy will enforce its satisfaction.

Synthesizing a strategy that achieves Properties 1, 2, and 3, requires more sophisticated techniques than those developed for PBP in deterministic domains (Son and Pontelli 2006; Baier, Bacchus, and McIlraith 2009; Keyder and

Geffner 2009), where the environment response is fixed and objectives are only either enforceable or unsatisfiable. Moreover handling ordinal temporal objectives in FOND requires significant advancements compared to PBP in FOND planning addressed in (Shaparau, Pistore, and Traverso 2006) for static preferences (which can only express additional 'soft' reachability objectives). In particular, the machinery developed for best-effort strategies for singular objectives (Aminof, De Giacomo, and Rubin 2021) plays a prominent role in synthesizing adaptive strategies for multi-tiers goals.

## 4 LTL f Adaptive Synthesis for Multi-Tier Goals

Wenowformalize the notion of adaptive strategies for multitier goals that comply with Properties 1, 2, and 3 discussed in the previous section.

We define adaptive strategies for a multi-tier goal by leveraging the notion of best-effort strategy for a single objective. Specifically, we leverage the history-based characterization of best-effort strategies to satisfy the three properties above when considering multi-tier goals.

Note that an adaptive strategy requires identifying all enforceable objectives at every point in time (cf. Property 1 and Property 3). Therefore, we define the maximally winning objective as the highest tier that the agent can enforce, regardless of an adversarial environment in a nondeterministic planning domain, for any given history.

̸

Definition 2 (Maximally Winning Objective) . Let P be a nondeterministic planning domain, Φ = ⟨ φ , 1 · · · , φ n ⟩ a multi-tier goal, and h ∈ H P a history. We say that φ ℓ is the maximally winning objective wrt h if val φ ℓ |P ( h ) = win and val φ j |P ( h ) = win for every j &gt; ℓ .

One can define the maximally pending objective analogously, i.e., the highest tier that the agent can satisfy should the environment cooperate.

An adaptive strategy must always leverage possible environment cooperation to fulfill all objectives that are currently not enforceable but satisfiable, while continuously enforcing those objectives that are enforceable. To this end, we define the maximally winning-pending objective , referring to the highest tier objective the agent can satisfy with environment cooperation, while simultaneously enforcing those currently enforceable (cf. Property 1 and Property 2).

Definition 3 (Maximally Winning-Pending Objective) . Let P be a nondeterministic planning domain, Φ = ⟨ φ , 1 · · · , φ n ⟩ a multi-tier goal, h ∈ H P a history, and φ ℓ the maximally winning objective of Φ wrt h . We say that φ k ( k &gt; ℓ ) is the maximally winning-pending objective wrt h if ( ) there exists a strategy i σ consistent with h that is winning for φ ℓ such that val φ k |P ( σ, h ) = pend, and ( ii ) it does not exist a strategy η consistent with h and winning for φ ℓ such that val φ j |P ( η, h ) = pend for some j &gt; k .

We now define our notion of adaptive strategy for multitier goals. Specifically:

- · To achieve Property 1, the adaptive strategy will always enforce the maximally winning objective, if one exists,

regardless of the adversarial environment, thus enforcing every objective that is currently enforceable;

- · To achieve Property 2, the adaptive strategy will always satisfy the maximally winning-pending objective or maximally pending objective (if the maximally winning objective does not exist), if one exists, should the environment cooperate, thus satisfying every pending objective;
- · To achieve Property 3, Properties 1 and 2 should hold for every history consistent with the adaptive strategy.

Formally, adaptive strategies are defined as follows:

Definition 4 (Adaptive Strategy for Multi-Tier Goals) . Let P be a nondeterministic planning domain and Φ = ⟨ φ , 1 · · · , φ n ⟩ a multi-tier goal. An agent strategy σ is an adaptive strategy for Φ in P if, for every h ∈ H P ( σ ) , one of the following holds:

- 1. Suppose both the maximally winning objective φ ℓ and the maximally winning-pending objective φ k exist, then: ( ) i σ is winning for φ ℓ in P from h , and ( ii ) there exists an agent strategy σ ′ such that val φ k |P ( σ , h ′ ) = pend and σ h ( ) = σ ′ ( h )
- 2. Suppose the maximally winning objective φ ℓ exists, yet no maximally winning-pending exists, then σ is winning for φ ℓ in P from h ;
- 3. Suppose neither the maximally winning objective nor the maximally winning-pending objective exists, yet there exists a maximally pending objective φ p , then there exists an agent strategy σ ′ such that val φ p |P ( σ , h ′ ) = pend and σ h ( ) = σ ′ ( h ) .

In this paper, we study synthesis of adaptive strategies for LTL f multi-tier goals in nondeterministic planning domains.

Definition 5 (LTL f Adaptive Synthesis for Multi-Tier Goals) . Given a nondeterministic planning domain P and an LTL f multi-tier goal Φ , adaptive synthesis for multi-tier goals is to compute an adaptive strategy for Φ in P .

The major contribution of this paper is a game-theoretic synthesis technique that computes adaptive strategies for LTL f multi-tier goals, which we will show in Section 7. The correctness of the synthesis technique also shows that LTL f adaptive strategies for multi-tier goals always exist (analogously to LTL f best-effort strategies for single objectives).

Theorem 1. An adaptive strategy always exists for an LTL f multi-tier goal Φ in a planning domain P .

## 5 Illustration Example.

We present an example drawn from robot navigation to illustrate the notions presented in previous sections and show how multi-tier goals can capture temporal preferences.

Consider a cleaning robot operating in a circular building, as shown in Figure 1. The robot can freely access Offices A, B, C, and D. However, entry to Labs I and II requires passing through secure gates in the hallway, shown in brown in Figure 1. These gates restrict access when hazardous materials are present, managed by the building manager. Solid arrows indicate areas with unrestricted access, while dashed arrows represent limited-access zones. Assume the robot starts in Office A, with the secure gates initially open. Consider the multi-tier goal Φ = ⟨ φ , φ 1 2 , φ 3 ⟩ , where:

Figure 1: Robot working in an office building.

<!-- image -->

- · φ 1 = ♢ ( Office D clean ) ;
- · φ 2 = ♢ ( Office D clean ) ∧ ♢ ( Lab II clean ) ;
- · φ 3 = ♢ ( Lab II clean ∧ ◦ ( ♢ ( Office D clean ))) ;

The multi-tier goal Φ specifies the robot should clean Office D ( φ 1 ), but cleaning Lab II also is preferable ( φ 2 ), and cleaning Lab II before Office D ( φ 3 ) is even more preferable.

Initially, φ 1 is enforceable, while φ 2 and φ 3 are pending. Thus, φ 1 is the maximally winning objective at this step. A winning strategy for φ 1 is to clean Office D without selecting a path that traverses the secure gates. This is because if the robot moves through the secure gates, the building manager might close the gates and lock the robot in one of the Labs, thus preventing φ 1 from being satisfied.

With φ 1 as the maximally winning objective, we have that φ 2 is the maximally winning-pending objective, as any winning strategy for φ 1 avoids clean Lab II before Office D such that not satisfying φ 3 . A strategy σ that navigates through Offices A, B, C, and D, cleans Office D, and proceeds to cleans Lab II if the secure gate is open, is winning for φ 1 and cooperative for φ 2 . Once the robot finishes cleaning Office D, φ 2 becomes enforceable if the building manager left secure gates open, in which case φ 2 becomes the maximally winning objective and σ enforces φ 2 as well. Thus σ is a strategy that satisfies Definition 4 and is adaptive for Φ .

This example also shows that a best-effort strategy for any single objective in Φ is not guaranteed to be an adaptive strategy for the multi-tier goal Φ . A best-effort (winning) strategy for φ 1 can navigate to Offices A, B, C, and D, then clean Office D, and stay there, so that φ 2 would never be satisfied. Instead, a best-effort strategy for φ 2 can navigate to and clean Lab II and thereafter navigate to and clean Office D. This might get the robot stuck between secure gates so that not satisfying φ 1 (analogously for φ 3 ).

## 6 Building Blocks

We develop a game-theoretic technique to address adaptive synthesis for multi-tier goals, involving two-player games over deterministic finite automata, reviewed briefly below.

- A deterministic transition system is a tuple T = (Σ , Q, ι, ϱ ) , where: Σ is a finite input alphabet; Q is a finite set of states; ι ∈ Q is the initial state; and ϱ : Q × Σ → Q is the transition function. The size of T is | Q | . Given a finite trace π = π 0 · · · π n over Σ , we extend ϱ to a func-

tion ϱ : Q × Σ ∗ → Q as follows: ϱ q, λ ( ) = q and, if q n = ϱ q, π ( 0 · · · π n -1 ) , then ϱ q, π ( 0 · · · π n ) = ϱ q ( n , π n ) .

Definition 6. The product of two transition systems T i = (Σ , Q , ι , ϱ i i i ) (for i = 1 2 , ) is the transition system PRODUCT ( T 1 , T 2 ) = (Σ , Q 1 × Q , 2 ( ι 1 , ι 2 ) , ϱ ) , where ϱ (( q 1 , q 2 ) , a, r ) = ( ϱ 1 ( q 1 , a, r ) , ϱ 2 ( q 2 , a, r )) .

Given a transition system T = (Σ , Q, ι, ϱ ) and a set of states V ⊆ Q , the complement of V wrt Q is V = Q \ V .

A deterministic finite automaton (DFA) is a tuple A = ( T , R ) , where T = (Σ , Q, ι, ϱ ) is a transition system, and R ⊆ Q is a set of final states . A word π ∈ Σ ∗ is accepted by A if ϱ ι, π ( ) ∈ R . The language recognized by A , written L A ( ) , is the set of words that the automaton accepts.

Theorem 2. (De Giacomo and Vardi 2013) Given an LTL f formula φ we can build a DFA , denoted TODFA ( φ ) , with size at most double-exponential in | φ | and whose language is the set of finite traces that satisfy φ .

A DFA game is a DFA with alphabet Act × React , where Act and React are two disjoint sets under control of agent and environment, respectively. Formally, a DFA game is a pair G = ( T , R ) , where: T = ( Act × React, Q, ι, ϱ ) is a transition system and R ⊆ Q is a set of final states.

A game strategy is a partial function κ : Q → Act that maps states of the game to agent actions. Given a game strategy κ , a sequence of environment reactions ⃗ r = r r 0 1 . . . ∈ React ω , and a DFA game G , we denote by Run ( κ,⃗ r, G ) , the run q 0 q 1 · · · of states of G induced by (or consistent with ) κ and ⃗ r as follows: ( ) i q 0 is the initial state of G ; ( ii ) for every i ≥ 0 , we have q i +1 = ϱ q , a , r ( i i i ) , where a i = κ q ( i ) ; and ( iii ) if Run ( κ,⃗ r, G ) = q 0 · · · q n is finite, then κ q ( n ) = ⊥ . A game strategy is winning (resp. cooperative) in G if ρ = Run ( κ,⃗ r, G ) is finite and lst ( ρ ) ∈ R for every ⃗ r ∈ React ω (resp. for some ⃗ r ∈ React ω ). A state q ∈ Q is a winning (resp. cooperative ) state if the agent has a winning (resp. cooperative) game strategy in the game G ′ = ( T ′ , R ) , where T ′ = ( Act × React, Q, q, ϱ ) , i.e., the same game as G , but with initial state q . The winning (resp. cooperative ) region W (resp. C ) is the set of winning (resp. cooperative) states. A game strategy that is winning from every state in the winning (resp. cooperative) region is called uniform winning (resp. uniform cooperative ). We observe that while a game strategy is not formally an agent strategy (i.e., a function from environment reactions to agent actions), it induces one as follows.

Definition 7. Given a transition system T = ( Act × React, Q, ι, δ ) , a game strategy κ : Q → Act induces an agent strategy σ ′ : React ∗ → Act as follows: for every h ∈ ( Act × React ) ∗ , σ ′ ( h | React ) = κ δ ι, h ( ( )) . The pair ( T , κ ) denotes a transducer , i.e., a transition system with output function κ .

(We also recall: in our setting every strategy σ ′ : React ∗ → Act is equivalent to a strategy σ : (2 F ) + → Act .)

Solving a DFA game G aims at computing the winning (resp. cooperative) region and a uniform winning (resp. cooperative) strategy, written (W , κ ) = SOLVEADV ( G ) (resp. (C , ν ) = SOLVECOOP ( G ) ). DFA games can be solved in linear time in their size by a fixpoint computation over the state space of the game (Apt and Gr¨ adel 2011). For states not in W (resp. C ), we assume that κ (resp. ν ) is undefined.

Given two DFAs A i = ( T i , R i ) for ( i = 1 2) , and the product T of their transition systems, we can lift the final states R i to the product T as follows.

Definition 8. For DFA s A i = ( T i , R i ) and T = PRODUCT ( T 1 , T 2 ) , the lifting of R i to T is the set of states LIFT ( T , R i ) = { ( q 1 , q 2 ) ∈ Q 1 × Q 2 s.t. q i ∈ R i } .

We now show how to transform planning domains into DFAs by introducing two error states s ag err and s env err , denoting that agent and environment violate their respective preconditions, as in (De Giacomo, Parretti, and Zhu 2023b).

̸

̸

Definition 9. Given a nondeterministic planning domain P = (2 F , s 0 , Act, React, α, β, δ ) , we define ( P + , { s ag err } { , s env err } ) = TODFA ( P ) 3 , where P + = ( Act × React, 2 F ∪ { s ag err , s env err } , s 0 , δ ′ ) is a transition system with transition function δ ′ such that δ ′ ( s, a, r ) = δ s, a, r ( ) if a ∈ α s ( ) and r ∈ β s, a ( ) ; or δ ′ ( s, a, r ) = s ag err if a ∈ α s ( ) ; or δ ′ ( s, a, r ) = s env err if a ∈ α s ( ) and r ∈ β s, a ( ) .

Our synthesis technique utilizes the synthesis approach developed for LTL f synthesis in nondeterministic domains (De Giacomo, Parretti, and Zhu 2023a). This approach involves constructing the product of the transition systems of the domain, defined over Act × React and the DFA of the LTL f objective, defined over 2 F .

Definition 10. Let P + = ( Act × React, 2 F ∪ { s ag err , s env err } , s 0 , δ ′ ) be the transition system of a domain P and T φ = (2 F , Q, ι, ϱ ) the transition system of the DFA of an LTL f objective. The product of P + and T φ is PRODUCT ( P + , T φ ) = ( Act × React, (2 F ∪{ s ag err , s env err } ) × Q, ( s , ϱ 0 ( ι, s 0 )) , ∂ ) , where:

̸

$$\bar { R } _ { \begin{array} { c } R \\ \text{e} \\ \text{ng} \end{array} } & & \mathcal { Q }, ( s _ { 0 }, \varrho ( \iota, s _ { 0 } ) ), \mathcal { O } ), w h e r e \colon \\ \text{ate} \\ \text{ent} \\ \end{array} & & \partial ( ( s, q ), a, r ) = \begin{cases} ( s ^ { \prime }, \varrho ( q, s ^ { \prime } ) ) & i f s ^ { \prime } \not \in \{ s _ { e r r } ^ { a g }, s _ { e r r } ^ { e n v } \} \\ ( s _ { e r r } ^ { a g }, q ) & i f s ^ { \prime } = s _ { e r r } ^ { a g } \\ ( s _ { e r r } ^ { e n v }, q ) & i f s ^ { \prime } = s _ { e r r } ^ { e n v } \\ \text{th} \, s ^ { \prime } = \delta ^ { \prime } ( s, a, r ). \end{cases} \\ \tilde { \text{ng} } _ { \begin{array} { c } R \\ \text{ed} \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \cdots \end{array} }$$

Intuitively, T is a transition system that simultaneously retains the progression in P and T φ .

## 7 Synthesis Technique

In this section, we present a game-theoretic technique to address adaptive synthesis for LTL f multi-tier goals in nondeterministic planning domains. This technique relies on solving and combining on-the-fly the solutions of several DFA games, constructed from both the planning domain and the objectives within the multi-tier goal.

We first review the key steps for best-effort synthesis of a single LTL f objective (De Giacomo, Parretti, and Zhu 2023b), as outlined in Algorithm 1. That solves two distinct games over the same transition system T (with different final states) constructed from the domain and the LTL f objective. Solving these games returns the following: the winning region W with a uniform winning game strategy κ , and the

3 For simplicity, we extend the notation TODFA to return two DFAs ( P + , { s ag err } { , s env err } ) and ( P + , { s ag err } { , s env err } ) , which share the same transition system but differ in their final states.

## Algorithm 1: SYNTHDSSINGLEOBJ ( P , φ )

## Input: Domain P and LTL f goal φ

Output: Trans. sys. T , win. region W , win. strategy κ , coop. region C , and coop. strategy ν

- 1: ( P + , { s ag err } { , s env err } ) = TODFA ( P )
- 2: ( T φ , R φ ) = TODFA ( φ )
- 3: T = PRODUCT ( P + , T φ )
- 5: EnvErr = LIFT ( T , { s env err } )
- 4: AgErr = LIFT ( T , { s ag err } )
- 6: R ′ φ = LIFT ( T , R φ )
- 7: Adv = AgErr ∩ ( EnvErr ∪ R ′ φ )
- 8: Coop = AgErr ∩ EnvErr ∩ R ′ φ
- 9: (W , κ ) = SOLVEADV ( T , Adv )
- 10: (C , ν ) = SOLVECOOP ( T , Coop )
- 11: Return ( T , W C , , κ, ν )

cooperative region C with a uniform cooperative game strategy ν . We have that W , κ , C , and ν satisfy the following: paths ending in W correspond to histories with value win , as witnessed by κ ; paths ending in C \ W correspond to histories with value pend , as witnessed by ν ; the remaining paths correspond to histories with value lose . This property will later serve to detect maximally winning and maximally pending objectives when synthesizing adaptive strategies.

A crucial step in adaptive synthesis for multi-tier goals is to determine, for every history, the current maximally winning-pending objective. To this end, we provide in Algorithm 2 an auxiliary procedure that, for a pair of LTL f objectives ⟨ φ , φ 1 2 ⟩ such that L P ( φ 1 ) ⊇ L P ( φ 2 ) , computes a strategy σ that satisfies the following: for every history h , if there exists a strategy to enforce φ 1 from h and to satisfy φ 2 from h with environment cooperation, i.e., currently, φ 1 is a winning objective and φ 2 is a winning-pending objective, then σ should fulfill both φ 1 and φ 2 if the environment cooperates; if fulfilling both is not feasible, i.e., currently, φ 2 is not a winning-pending objective, then σ enforces only φ 1 . Intuitively, σ prioritizes satisfying both φ 1 and φ 2 when possible, but defaults to enforcing φ 1 if necessary.

Concretely, Algorithm 2 first computes the following: the winning region W 1 and a winning strategy κ 1 for the game obtained from φ 1 ; and the cooperative region C 2 for the game obtained from φ 2 (Lines 1-4). Next, Algorithm 2 identifies game states, where paths ending at these states indicate that currently φ 1 is the maximally winning objective and φ 2 is the maximally winning-pending objective within the multi-tier goal ⟨ φ , φ 1 2 ⟩ . Such states are collected in WP through the fixpoint computation in Lines 5-7, in a game that aims to satisfy φ 1 adversarially, meanwhile satisfying φ 2 cooperatively. A state q ′ is added to WP i +1 when there exists an agent action a such that: ( ) some environment rei action moves the agent towards satisfying both φ 1 and φ 2 , written ∃ r.∂ ( q , a, r ′ ) ∈ WP i ; and ( ii ) every environment reaction either moves the agent towards satisfying both φ 1 and φ 2 or prevents φ 2 from being satisfied, but still ensures φ 1 , written ∀ r.∂ ( q , a, r ′ ) ∈ WP i ∪ (W 1 × C ) 2 . Paths ending in WP directly corresponds to histories that admit a strategy

## Algorithm 2: SYNTHDSWINPEND ( P , φ 1 , φ 2 )

Input: Domain P ; LTL f objs φ , φ 1 2 st L P ( φ 1 ) ⊇ L P ( φ 2 ) Output: Win-coop region WP ; agent strategy σ

1:

(

P

+

, s ag

err

, s env

err

) =

TODFA

(

P

)

- 2: For i = 1 2 , :

2.1: ( T φ i , R φ i ) = TODFA ( φ )

2.2: T i = PRODUCT ( P + , T φ i )

2.3: AgErr i = LIFT ( T i , { s ag err } )

/* Q ′ i is the state space of T i */

2.4: EnvErr i = LIFT ( T i , { s env err } )

2.5: R ′ φ = LIFT ( T , R φ )

2.6: Adv i = AgErr i ∩ ( EnvErr i ∪ R ′ φ i )

2.7: Coop i = AgErr i ∩ EnvErr i ∩ R ′ φ i

- 3: (W 1 , κ 1 ) = SOLVEADV ( T 1 , Adv 1 )
- 4: (C 2 , -) = SOLVECOOP ( T 2 , Coop 2 )
- 5: T = PRODUCT ( T 1 , T 2 )

/* Q ′ = Q ′ 1 × Q ′ 2 is the state space of T */

/* ∂ is the transition function of T */

6:

Let

WP

-

1

=

{∅}

and

WP =

0

Adv

1

×

Coop

2

̸

Let WP i +1 = { q ′ ∈ Q ′ | ∃ a. ∃ r.∂ ( q , a, r ′ ) ∈ WP i ∧

- 7: While WP i +1 = WP i :

∀ r.∂ ( q , a, r ′ ) ∈ WP i ∪ (W 1 × C ) 2 }

- 8: Define strategy σ on T as follows. For every ( q ′ 1 , q ′ 2 ) :

$$& \quad \ a n a v e g y \, \chi \, \circ \, \circ \, \text{$n$ as nvows, $n$} \, \chi \, \chi _ { 1 }, \chi _ { 2 } \,. \\ & q _ { 2 } ^ { \prime } ) ) = \begin{cases} a & \text{ if } ( q _ { 1 } ^ { \prime }, q _ { 2 } ^ { \prime } ) \in \text{WP} _ { i + 1 } \, \searrow \text{WP} _ { i } \\ & \text{ and } \exists r. \partial ( ( q _ { 1 } ^ { \prime }, q _ { 2 } ^ { \prime } ), a, r ) \in \text{WP} _ { i } \\ \kappa _ { 1 } ( q _ { 1 } ^ { \prime } ) & \text{ otherwise} \end{cases}$$

- 9:

$$\begin{matrix} o. \, \text{em usage} \, y \, \upsilon \, \text{on} \, \prime \, \text{ as} \, \iota \\ \sigma ( ( q _ { 1 } ^ { \prime }, q _ { 2 } ^ { \prime } ) ) = \begin{cases} a & \text{i} \\ a \\ \kappa _ { 1 } ( q _ { 1 } ^ { \prime } ) & 0 \end{cases} \\ \underline { 9 \colon Return (WP, \sigma ) } \end{matrix}$$

that simultaneously wins φ 1 and cooperates for φ 2 . Algorithm 2 constructs the output strategy σ by combining κ 1 for enforcing φ 1 by default and the fixpoint computation information while collecting WP (Line 8). The construction is the following: for every state in WP , for which there exists an action that definitely advances φ 1 and can advance φ 2 if the environment cooperates, then σ follows this action (first case, Line 8); otherwise σ follows κ 1 , advancing φ 1 only (second case, Line 8).

Let val ⟨ φ ,φ 1 2 ⟩|P ( h ) denote ⟨ val φ 1 |P ( h , ) val φ 2 |P ( h ) ⟩ (and similarly for val ⟨ φ ,φ 1 2 ⟩|P ( σ, h ) ). The following theorem shows the correctness of Algorithm 2.

Theorem 3. Let P be a domain, φ 1 and φ 2 two LTL f objectives such that L P ( φ 1 ) ⊇ L P ( φ 2 ) , and σ the strategy returned by Algorithm 2. For every h ∈ H P ( σ ) , if val ⟨ φ ,φ 1 2 ⟩|P ( h ) = ⟨ win pend , either: , ⟩

- 1. val ⟨ φ ,φ 1 2 ⟩|P ( σ, h ) = ⟨ win pend ; or , ⟩
- 2. If no agent strategy η exists s.t. val ⟨ φ ,φ 1 2 ⟩|P ( η, h ) = ⟨ win pend , then , ⟩ val φ 1 |P ( σ, h ) = win.

With Algorithms 1 and 2 in place, we present our complete adaptive synthesis technique for LTL f multi-tier goals, outlined in Algorithm 3. Algorithm 3 begins by computing all the winning and cooperative game strategies for every objective in the multi-tier goal Φ using Algorithm 1. It then proceeds to compute strategies for winning-pending objectives with Algorithm 2, covering all combinations of objective pairs ( φ , φ i j ), where j &gt; i . The output adap-

i

## Algorithm 3: SYNTHMULTITIER ( P , Φ)

Input: Domain P ; Multi-tier goal Φ = ⟨ φ , 1 · · · , φ n ⟩ .

Output: Agent strategy that is adaptive for Φ in P

- 1: For i = 1 , · · · , n :

1.1:

(

T

i

,

W C

i

,

i

, κ , ν

i

i

) =

SYNTHDSSINGLEOBJ

(

1.2:

For

j

=

i

+1

,

· · ·

, n

:

(WP ij , ω ij ) = SYNTHDSWINPEND ( P , φ , φ i j )

- 2: Return σ : (2 F ) + → Act defined as follows.

Let h be the input history:

- · For i = 1 , · · · , n : let q ′ i = ∂ i ( ι , h i | Act × React ) , where ι i and ∂ i are the initial state and transition function of T i , respectively.
- · ℓ = max { i s.t. j &gt; i and ( q , q ′ i ′ j ) ∈ WP ij }
- · j = max { i s.t. q ′ i ∈ W i }
- · m = max { i s.t. q ′ i ∈ C i }

$$\bullet \ m - m \alpha _ { \mathcal { U } } \varepsilon \varsigma. \ q _ { i } \in \\ \sigma ( h ) = \begin{cases} \omega _ { j \ell } ( q ^ { \prime } _ { j }, q ^ { \prime } _ { \ell } ) \\ \kappa _ { j } ( q ^ { \prime } _ { j } ) \\ \nu _ { m } ( q ^ { \prime } _ { m } ) \\ \perp \end{cases}$$

 ′ ′ ) if ℓ &gt; j &gt; 0 exists else if j &gt; 0 exists else if m&gt; 0 exists otherwise tive strategy is generated in the form of an executable program that selects, at each time step, the next action onthe-fly from among the actions suggested by all the previously computed strategies. The selection procedure involves checking the state corresponding to the input history h to identify which objectives are currently winning, pending, and winning-pending. Recall that paths ending in W i (resp. C i ) are with value win (resp. pend ). Hence we can determine the maximally winning objective φ j , where j = max { i s.t. q ′ i ∈ W i } . Subsequently, we can determine the maximally winning-pending objective φ ℓ , where ℓ = max { i s.t. j &gt; i and ( q , q ′ i ′ j ) ∈ WP ij } . Analogously for the maximally pending objective φ m . Based on this, the strategy selects an action following Properties 1,2&amp;3. In details, at each time step, the strategy selects the output action as follows: ( ) it selects the action returned by the strategy i ω jℓ that enforces the maximally winning objective φ j and cooperates for the maximally winning-pending objective φ ℓ , if any; else ( ii ) it selects the action returned by the strategy κ j that enforces the maximally winning objective, if any; else ( iii ) it selects the action returned by the strategy ν m that cooperates for the maximally pending objective φ m , if any; else ( iv ) is undefined.

Concretely, the output strategy is implemented by executing simultaneously all transducers (representing the strategies) computed in Line 1. For the current history h , the output strategy σ evaluates the response of each such transducer, and selects the output action σ h ( ) based on the tests in Line 2 and discussed above. The action σ h ( ) , together with the environment reaction, extends h , thus generating a new history, and the process starts again. The strategy terminates when it selects ⊥ , i.e., it is undefined, because either no maximally pending objective exists or the selected transducer outputs ⊥ . Figure 2 sketches the implementation of the strategy computed with Algorithm 3.

Theorem 4. Alg. 3 returns an adaptive strategy for Φ in P .

P

, φ

i

)

;

Figure 2: Implementation of the strategy returned by Alg. 3.

<!-- image -->

Regarding complexity, Algorithm 3 solves games with size 2EXPTIME and EXPTIME in that of objectives and domain, respectively. In fact, the number of solved games is quadratic in the number of objectives in the multi-tier goal. The following theorem shows the complexity characterization of adaptive synthesis for LTL f multi-tier goals in nondeterministic planning domains.

Theorem 5. LTL f adaptive synthesis for multi-tier goals is:

- · 2 EXPTIME -complete in the size of the objectives;
- · EXPTIME -complete in the size of the domain;
- · polynomial in n , the number of objectives.

We also note that Line 1 of Algorithm 3 is fully parallelizable. Synthesizing strategies for each objective can be done in parallel with n processors; synthesizing strategies for pairs of objectives can be done in parallel with n 2 processors. As a result, if n + n 2 processors are available, adaptive synthesis for multi-tier goals is virtually for free, i.e., it costs as handling the most expensive objectives. These nice computational results confirm that adaptive synthesis for multitier goals brings a minimal overhead to standard synthesis.

## 8 Conclusion and Future Work

In this paper, we introduced the problem of LTL f adaptive for multi-tier goals in nondeterministic planning domains. We developed a synthesis technique that is both sound and complete for computing an adaptive strategy in this context. This technique allows for straightforward utilization of symbolic synthesis techniques (Zhu et al. 2017; De Giacomo, Parretti, and Zhu 2023a), aiming for promising performance and scalability. Currently, our framework considers a single environment model. However, it is also of interest to consider the setting that involves multiple environment models (Aminof et al. 2020; Ciolek et al. 2020; Aminof et al. 2021), accounting for varying nondeterminism of the environment. For instance, as the environment becomes more nondeterministic, we might anticipate achieving a less challenging objective. We believe that the general approach presented here can be extended to handle this setting as well.

## Acknowledgments

This work is supported in part by the ERC Advanced Grant WhiteMech (No. 834228), the PRIN project RIPER (No. 20203FFYLK), the PNRR MUR project FAIR (No. PE0000013), and the UKRI Erlangen AI Hub on Mathematical and Computational Foundations of AI. Gianmarco Parretti is supported by the Italian National Ph.D. on Artificial Intelligence at 'La Sapienza'
<|endofpaper|>