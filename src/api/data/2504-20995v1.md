<|startofpaper|>
<!-- image -->

Haoyu Zhen 1 ∗ Qiao Sun 1 ∗ Hongxin Zhang 1 Junyan Li 1 Siyuan Zhou 2 Yilun Du 3 Chuang Gan 1

1 UMass Amherst 2 HKUST 3 Harvard University https://TesserActWorld.github.io

Figure 1. We propose TesserAct, the 4D Embodied World Model, which takes an input image and text instruction to generate RGB, depth, and normal videos, reconstructing a 4D scene and predicting actions. Our model not only achieves strong performance on in-domain data (right) but also generalizes effectively to unseen scenes, novel objects (top left), and cross-domain scenarios (bottom left).

<!-- image -->

## Abstract

This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.

∗ Equal contribution.

## 1. Introduction

Learned world models [21, 64, 67, 77], which simulate environmental dynamics, play a crucial role in enabling embodied intelligent agents. Such models enable flexible policy synthesis [15, 40], data simulation and generation [67, 80], and long-horizon planning [14, 28, 74]. However, while the physical world is three-dimensional in nature, existing world models operate in the space of 2D pixels. This limitation leads to an incomplete representation of spatial relationships, impeding tasks that require precise depth and pose information. For instance, without accurate depth and 6-DoF pose estimations, robotic systems struggle to determine the exact position and orientation of objects. Furthermore, existing 2D models can produce unrealistic results, such as inconsistent object sizes and shapes across time steps, which limits their use in data-driven simulations and robust policy learning.

In this paper, we explore how we can instead learn a 4D embodied world model, TesserAct, which simulates the dynamics of a 3D world. This 4D embodied world model allows us to generate realistic 3D interactions, such as grasping objects or opening drawers, with a level of detail that traditional 2D-based models cannot achieve. By modeling spatial and temporal dimensions, our model provides the depth and pose information essential for robotic manipulation. However, the task of learning a 4D embodied world model is challenging as the dynamics of the world are extremely computationally expensive to train and learn, requiring models to generate outputs in three-dimensional space and time. To efficiently represent and predict the dynamics of the world, we propose a substantially more lightweight representation of the 4D world, consisting of predicting a sequence of RGB, depth, and normal maps of the scene. This combined representation accurately captures the appearance, geometry, and surface of a scene while being substantially lower dimensional than explicitly predicting world dynamics. Furthermore, such a representation shares substantial similarities to existing video models, allowing us to directly use the generative capabilities of existing video models to effectively construct our 4D world model.

Given this intermediate representation, we present an efficient algorithm to reconstruct accurate 4D scenes from generated RGB-DN videos. For each frame, we use a combination of both depth and normal prediction to integrate a smooth 3D surface of the scene. We then use optical flow between generated frames to distinguish between background and dynamic regions in the reconstructed 3D scene across frames and introduce two novel loss functions to enforce consistency across scenes over time. As shown in Figure 1, our 4D Embodied World Model enables the construction of highfidelity 4D-generated scenes, facilitating strong-performance action planning for downstream tasks.

A key challenge for training TesserAct is a lack of access to existing large-scale datasets with high-quality 4D

annotations, or the RGB image, depth, and normal information needed to train our approach. To overcome this, we collect a 4D embodied video dataset consisting of synthetic data with ground truth depth and normal information and real-world data with estimated depth and normal information with off-the-shelf estimators.

Overall, our paper has the following contributions:

- · We collect a 4D embodied video dataset with compact and high-quality RGB, depth, and normal annotations and learn a 4D embodied world model.
- · Wepresent an algorithm to convert the generated RGB-DN video into high-quality 4D scenes coherent across both time and space.
- · Extensive experiments demonstrate our 4D embodied world model can predict high-fidelity 4D scenes and achieve superior performance in downstream embodied tasks compared to traditional video-based world models.

## 2. Related Work

Embodied Foundation Models A flurry of recent work has focused on constructing foundation models for general purpose agents [18, 68]. One line of work has focused on constructing multimodal language models that operate over images [13, 20, 29, 39, 51, 63, 73] as well as 3D inputs [24, 25] and output text describing the actions of an agent. Other works have focused on the construction of vision-languageaction (VLA) models that directly output action tokens [7, 34, 76]. Both of the previous approaches aim to construct foundation model policies (over text or continuous actions). In contrast, our work aims to instead construct a foundation 4D world model for embodied agents, which can then be used for downstream applications such as planning [14, 74] or policy synthesis [15, 40].

Learning World Models Learning dynamics model of the world given control inputs has been a long-standing challenge in system identification [42], model-based reinforcement learning [56], and optimal control [4, 81]. A large body of work focused on learning world models in the low dimensional state space [1, 17, 38], which while being efficient to learn, is difficult to generalize across many environments. Other works have explored how world models may be learned over pixel-space images [11, 12, 21, 22, 43], but such models are trained on simple game environments. With advances in generative modeling, a large flurry of recent research has focused on using video models as foundation world models [8, 46, 64, 67, 77, 79] but such models operate over the space of 2D pixels which does not fully simulate the 3D world. Most similar to our work are [76], which predicts only the 3D goal state for robotic tasks, and [57], an geometric-aware world model trained purely on synthetic data without language grounding or downstream robotic tasks; in contrast, our approach models the 4D scene from RGB-DN videos and supports language-conditioned control.

Table 1. Overview of the 4D embodied video datasets.

| Dataset                                                    | Domain    | Depth Source       | Normal Source    | Embodiment                     | # of videos   |
|------------------------------------------------------------|-----------|--------------------|------------------|--------------------------------|---------------|
| RLBench [26]                                               | Synthetic | Simulator          | Depth2Normal [2] | Franka Panda                   | 80k           |
| RT1 Fractal Data [6] Bridge [61] SomethingSomethingV2 [19] | Real      | Rolling Depth [31] | Marigold [32]    | Google Robot WidowX Human Hand | 80k 25k 100k  |

4D Video Generation The task of 4D video generation has gained increasing attention in recent years [55, 71], driven by advancements in diffusion models [23, 47, 54], neural radiance fields [44], and 3D Gaussian splatting [33]. However, existing methods often suffer from slow optimization due to hybrid frameworks [3, 41, 65, 75] and the convergence challenges of SDS loss [30, 53]. Instead, we represent 4D scenes using RGB-DN videos, which offer more efficient training and provide high-accuracy 3D information crucial for embodied tasks. Furthermore, our approach is the first to directly predict 4D scenes from the current frame and the embodied agent's action described in the text.

## 3. Preliminaries

## 3.1. Latent Video Diffusion Models

Diffusion models [23, 54] are capable of learning the data distribution p x ( ) by progressively adding noise to the data until it resembles a Gaussian distribution through a forward process. During inference, a denoiser ϵ is trained to recover the data from this noisy state. Latent video diffusion models [77] utilize a Variational Autoencoder (VAE) [35, 60] to encode the data in the latent space, maintaining high-quality outputs while more efficiently modeling the data distribution.

We formulate the task of RGB V , depth D , and normal N video generation as a conditional denoising generation task, i.e., we model the distribution p ( v d n v , , | 0 , d 0 , n 0 , T ) , where v d n , , represent the predicted future latent sequences of RGB, depth, and normal maps, respectively; the conditions v 0 , d 0 , n 0 , T are the latent of RGB image, depth and normal maps, and embodied agent's action in text.

The forward diffusion process adds Gaussian noise to the latent z ∈ { v d n , , } over T timesteps, defined as:

$$q ( z _ { t } | z _ { t - 1 } ) = \mathcal { N } ( z _ { t } ; \sqrt { \alpha _ { t } } z _ { t - 1 }, ( 1 - \alpha _ { t } ) \mathbf I ) \quad ( 1 ) \quad \frac { \ a r e } { \ s }$$

where t ∈ { 1 2 , , . . . , T } denotes the diffusion step, α t is a parameter controlling the noise influence at each step, and I is the identity matrix. In the reverse process, the model aims to recover the original latent from the noise. Let x = [ v n d , , ] denoting the concatenation of v n d , , , a denoising network ϵ θ ( x t , t, x 0 , T ) with learning parameters θ is trained to predict the noise added at each timestep. The reverse process is defined as:

$$| \ r a _ { \mu } \quad p _ { \theta } ( { \boldsymbol x } _ { t - 1 } | { \boldsymbol x } _ { t }, { \boldsymbol x } ^ { 0 }, { \mathcal { T } } ) = { \mathcal { N } } \left ( { \boldsymbol x } _ { t - 1 } ; \mu _ { \theta } ( { \boldsymbol x } _ { t }, t, { \boldsymbol x } ^ { 0 }, { \mathcal { T } } ), \Sigma _ { \theta } ( { \boldsymbol x } _ { t }, t ) \right ) \ ( 2 )$$

Once the denoised latent x 0 is obtained, the model maps it back to the pixel space to obtain the final RGB-DN video.

## 3.2. Depth Optimization via Normal Integration

As discussed in [9, 66], normal maps provide essential information about surface orientation, which is vital for enforcing geometric constraints and imposing surface smoothness and continuity during depth optimization. This spatial optimization leads to more accurate and reliable depth estimates that closely align with the true 3D geometry and capture fine surface details.

To formalize the process, we use the perspective camera model to set constraints on the depth and surface normal. In the coordinate system of the 2D image at frame i , a pixel position is given as u = ( u, v ) T ∈ V i , and its corresponding depth scalar, normal vector is d ∈ D i , n = ( n , n x y , n z ) ∈ N i . Under the assumption of a perspective camera with focal length f and the principal point ( c u , c v ) T , as proposed by [16], the log-depth ˜ = log( d d ) should satisfy the following equations: ˜ n ∂ d z u ˜ + n x = 0 and ˜ n ∂ d z v ˜ + n y = 0 where ˜ n z = n x ( u -c u ) + n y ( v -c v ) + n f z . In addition, we can add the assumption that all locations are smooth surfaces [9]. We can convert the above constraint to the quadratic loss function, allowing us to find the optimized depth map:

$$\stackrel { s } { \longleftarrow } \min _ { d } \iint _ { \Omega } ( \tilde { n } _ { z } \partial _ { u } \tilde { d } + n _ { x } ) ^ { 2 } + ( \tilde { n } _ { z } \partial _ { u } \tilde { d } + n _ { y } ) ^ { 2 } \text{du} v. \quad ( 3 )$$

Following [9], we can convert the above objective to an iteratively optimized loss objective. At iteration step t , we can compute the matrix W d ( ˜ ) t and iteratively optimize for a refined depth prediction ˜ d t +1 :

$$\L ^ { \prime }, \tilde { d } _ { t + 1 } = \arg \min _ { \tilde { d } } ( A \tilde { d } - b ) ^ { T } W ( \tilde { d } _ { t } ) ( A \tilde { d } - b ) \stackrel { \text{def} } { = } \arg \min _ { \tilde { D } } \mathcal { L } _ { s } ( \tilde { \mathcal { D } }, \mathcal { N } ^ { i } )$$

where A and b are defined by normals and camera intrinsic.

## 4. Learning a 4D Embodied World Model

Learning how 3D scenes may change over time based on the current observation and action is crucial for embodied agents.

Ƹ

<!-- image -->

Ƹ

Figure 2. Architecture and Training Overview of TesserAct.

We propose to learn a 4D embodied world model, TesserAct, by training on RGB-DN videos and reconstructing the 4D scenes from it. We first introduce the 4D embodied video dataset we collected in Sec. 4.1, then discuss the model architecture and training strategy in Sec. 4.2. In Sec. 4.3, we propose an efficient optimization algorithm with two novel loss functions to convert the generated RGB-DN videos into 4D scenes. Finally, in Sec. 4.4, we demonstrate how the 4D world model can help downstream embodied tasks.

## 4.1. 4D Embodied Video Dataset

Learning 4D embodied world models requires large-scale 4D datasets, which are expensive to collect in the real world. In this section, we present a data collection and annotation pipeline that enables us to automatically construct 4D datasets from existing video datasets.

Simulator-synthesized data provide ground truth depth information, so we first selected 20 tasks of relatively high difficulty from RLBench [26] and generated 1000 instances captured from 4 different views for each task, making 80k synthetic 4D embodied videos in total. Although the simulator provides metric depth information, it lacks surface normal data. To estimate normals, we use the depth2normal function from DSINE [2]. To enhance the generalization, we adopt the scene randomization techniques from the Colosseum data generation pipeline [48], which alters the background, table texture and light of the scene.

While synthetic data provides depth and normal data of high quality, their diversity is limited, resulting in a gap compared to real-world scenarios. To bridge this gap, we also incorporate real-world video datasets. As most of these datasets lack depth and normal annotations, we employ the state-of-the-art video depth estimator RollingDepth [31] to annotate the videos with affine-invariant depth. As affineinvariant depth map does not directly yield normal map as metric depth does and a reliable video normal estimator is currently unavailable, we annotate normal maps using Temporal-Consistent Marigold-LCM-normal 1 . These two approaches allow us to obtain high-quality, sharp, and temporally consistent video depth and normal annotations. Specifically, we select two high-quality datasets from OpenX [58], the Fractal data [6], and the Bridge [61] dataset. Moreover, to further increase the diversity of the instructions, we incorporated the human-object interaction dataset, Something Something V2 [19]. Detailed statistics are shown in Table 1.

## 4.2. Model Architecture and Training Strategy

Training a diffusion model to generate temporal RGB-DN data is challenging. To effectively train RGB video models, large-scale video datasets containing billions of high-quality samples are typically employed [69, 77]. In contrast, our RGB-DNdataset, even with automatic annotation, comprises only about 200k data points, which is insufficient to train a world model from scratch. To overcome this limitation, we modify and fine-tune CogVideoX [69] to serve as our RGBDN prediction model, leveraging the pre-trained knowledge within it to effectively bootstrap our 4D model.

Our architecture is illustrated in Figure 2. First, we utilize the 3D VAE [35, 60] from CogVideoX to separately encode the RGB ( v ), depth ( d ), and normal ( n ) videos, without any additional fine-tuning of the V AE. These latent

https : / / huggingface . co / docs / diffusers / en / using- diffusers/marigold\_usage#frame- by- framevideo-processing-with-temporal-consistency

Figure 3. Effect of consistency and regularization loss on 4D scene reconstruction. The red boxes highlight the inconsistent regions.

<!-- image -->

representations are perturbed with noise to get x t , and are then fed into our model along with the corresponding image latent head x 0 . For the input design, we introduce three separate projectors for each modality to extract the embeddings: f z = InputProj ( z t , z 0 ) , where z ∈ { v d n , , } . DiT then takes the sum of these embeddings as input, conditioned on the textual input T and denoising step t , to obtain the hidden state: h = DiT ( ∑ f , t, z T ) . To distinguish between different robot arms, T is defined as [action instruction] + [robot arm name], e.g., 'pick up apple google robot '. On the output side, we retain the original RGB output method: ϵ ∗ v = OutputProj ( h ) . However, we introduce additional modules for depth and normal prediction. A Conv3D layer is used to encode the concatenation of the input latents and the predicted RGB denoised output. These are then combined with the hidden states produced by the DiT backbone and passed through the output projector to obtain the denoised predictions for depth and normal: ϵ ∗ d n , = DNProj ( h, Conv3D ( ϵ ∗ v , [ z t ; z 0 ] z ∈{ v,d,n } )) . To preserve the pretrained knowledge, we initialize our model with the CogVideoX weights. All other modules are initialized with zeros, ensuring that the RGB output at the beginning of training matches the output of CogVideoX. During training, we randomly select samples from the 4D embodied dataset ( V D N T , , , ) constructed above and apply Eq.1 to add noise ϵ v , ϵ d , and ϵ n to the RGB-DN data at timestep t , minimizing the following objective:

$$L = \mathbb { E } _ { \mathbf v _ { 0 }, \mathcal { T }, t, \epsilon } \left [ \left \| \left [ \epsilon _ { \mathbf v }, \epsilon _ { \mathbf d }, \epsilon _ { \mathbf n } \right ] - \epsilon _ { \theta } ( \mathbf x _ { t }, t, \mathbf x ^ { 0 }, \mathcal { T } ) \right \| ^ { 2 } \right ] \quad ( 4 ) \quad \text{optic}$$

## 4.3. 4D Scene Reconstruction

After obtaining the RGB-DN video, we further optimize the depth and reconstruct the 4D scene. Similar to prior works [32, 70], our depth representation for each image is given by a relative map in the range [0 , 1] , and thus cannot directly reconstruct the entire scene. While past work has sidestepped this by assuming either a default scale for depth [72] or by directly predicting metric depth [10, 76], such reconstructions from depth are often coarse and often cause reconstructed planes or walls to be tilted.

With the normal maps N i , we can optimize the depth maps D i via normal integration for refined depth maps ˆ D i as introduced in Sec. 3.2 with a loss term L s for spatial consistency. However, this approach optimizes depth frame by frame, which lacks temporal consistency across the dynamic scene. To address this, we compute optical flow between frames [59] F = RAFT ( V ) and enforce consistency of depth across frames. We define the static regions of each frame as the pixels with the magnitude of optical flow smaller than threshold c and obtain its mask by M i s = ∥F ∥ ≤ i c . We then define the dynamic parts of an image as M i d = ¬M i s . We further define the background of an image as static regions that are fixed across image frames, M i b = M ∩M i s i -1 s

Since optical flow represents the movement of objects in the 2D-pixel space, we can retrieve the depth at any position from the previous frame to impose consistency constraints. To compute the depth values from the previous frame at positions corresponding to the current frame, we utilize the optical flow F i → -( i 1) . For each pixel ( u, v ) in frame i , the optical flow provides the displacement (∆ u, ∆ ) v , allowing us to find the corresponding pixel in frame i -1 at position ( u -∆ u, v -∆ ) v . Based on this mapping, we define the

Table 2. Main results on the 4D scene generation. All metrics are averaged over 10 runs for each of the samples. The best results are in bold , and the second best are in underlined. TesserAct predicts the depth and normal maps most accurately without hurting RGB much and thus achieves the best accuracy of reconstructed 4D point clouds across real and synthetic image domains.

| Domain   | Method           | RGB   | RGB    | RGB    | Depth    | Depth   | Depth   | Normal   | Normal   | Normal      | Point Cloud Chamfer L 1 ↓   |
|----------|------------------|-------|--------|--------|----------|---------|---------|----------|----------|-------------|-----------------------------|
| Domain   | Method           | FVD ↓ | SSIM ↑ | PSNR ↑ | AbsRel ↓ | δ 1 ↑   | δ 2 ↑   | Mean ↓   | Median ↓ | 11 . 25 ◦ ↑ | Point Cloud Chamfer L 1 ↓   |
|          | 4D Point-E       | -     | -      | -      | -        | -       | -       | -        | -        | -           | 0.2211                      |
|          | OpenSora         | 23.67 | 71.31  | 19.25  | 31.41    | 60.39   | 79.97   | 41.82    | 32.15    | 13.58       | 0.3013                      |
|          | CogVideoX        | 20.64 | 79.38  | 22.39  | 26.17    | 64.82   | 81.62   | 19.53    | 10.09    | 22.70       | 0.2191                      |
|          | TesserAct (Ours) | 21.59 | 75.86  | 20.27  | 22.07    | 66.80   | 82.60   | 15.74    | 7.32     | 27.80       | 0.2030                      |
|          | 4D Point-E       | -     | -      | -      | -        | -       | -       | -        | -        | -           | 0.1086                      |
|          | OpenSora         | 54.11 | 65.90  | 19.28  | 18.40    | 65.02   | 91.20   | 12.94    | 7.58     | 25.02       | 0.2570                      |
|          | CogVideoX        | 41.23 | 76.60  | 20.87  | 19.81    | 60.07   | 80.16   | 20.36    | 10.47    | 26.04       | 0.2884                      |
|          | TesserAct (Ours) | 40.01 | 77.59  | 19.73  | 16.02    | 69.26   | 93.03   | 14.75    | 6.34     | 36.85       | 0.0811                      |

D i → -( i 1) such that: D i → -( i 1) ( u, v ) = D i -1 ( u -∆ u, v -∆ ) v . We then introduce the consistency loss over the dynamic and background region of the image L c :

## 5. Experiments

$$\mathcal { L } _ { c } ( \tilde { D }, \hat { \mathcal { D } } ^ { i - 1 }, \mathcal { F } ^ { i }, \mathcal { F } ^ { i - 1 } ) = & \lambda _ { c d } \left \| \tilde { \mathcal { D } } ^ { i } \circ \mathcal { M } _ { d } ^ { i } - \mathcal { D } ^ { i \to ( i - 1 ) } \circ \mathcal { M } _ { d } ^ { i } \right \| ^ { 2 } + \left ( 5 \right ) \text{ \quad \text{cond} } \text{ \quad \text{for} } \\ & \lambda _ { c b } \left \| \tilde { \mathcal { D } } ^ { i } \circ \mathcal { M } _ { b } ^ { i } - \mathcal { D } ^ { i \to ( i - 1 ) } \circ \mathcal { M } _ { b } ^ { i } \right \| ^ { 2 } \text{ \quad \text{more} } \text{the u.s.}$$

In addition to the consistency loss, we also incorporate the regularization loss L r , enforcing that optimized depths are similar to the generated depth map D i :

$$\mathcal { L } _ { r } ( \tilde { \mathcal { D } }, \mathcal { D } ^ { i } ) = \lambda _ { r d } \overset { \mu } { \left \| \tilde { \mathcal { D } } ^ { i } \circ \mathcal { M } _ { d } ^ { i } - \mathcal { D } ^ { i } \circ \mathcal { M } _ { d } ^ { i } \right \| } ^ { \mu } + \lambda _ { r b } \left \| \tilde { \mathcal { D } } ^ { i } \circ \mathcal { M } _ { b } ^ { i } - \mathcal { D } ^ { i } \circ \mathcal { M } _ { b } ^ { i } \right \| } ^ { 2 } \quad ( 6 ) \quad \in \,.$$

The overall loss objective we optimize is given by:

$$\arg \min _ { \tilde { \mathcal { D } } } \mathcal { L } _ { s } ( \tilde { \mathcal { D } }, \mathcal { N } ^ { i } ) + \mathcal { L } _ { c } ( \tilde { \mathcal { D } }, \hat { \mathcal { D } } ^ { i - 1 }, \mathcal { F } ^ { i }, \mathcal { F } ^ { i - 1 } ) + \mathcal { L } _ { r } ( \tilde { \mathcal { D } }, \mathcal { D } ^ { i } ) \ \ ( 7 ) \quad \text{aaa} _ { \text{ and } }$$

Starting from the first frame, we iteratively refine the depth map by optimizing the loss above, and initialize the depth map at frame i with the generated depth map ˜ D 0 = D i . With refined depth maps ˜ D and RGB Images V , we can reconstruct 4D point clouds representing the world that are consistent over both space and time.

## 4.4. Embodied Action Planning with 4D Scenes

After generating 4D scenes, which encapsulate both spatial and temporal information, we extract geometric details that can significantly enhance downstream tasks in robotics. The detailed geometry captured by these scenes plays a crucial role in tasks including robotic grasping.

To achieve this, we employ an inverse dynamics model built on the 4D point clouds, predicting the appropriate robot action a i based on the current state s i , the predicted future state s i +1 and the instruction T . Mathematically, this relationship is expressed as a i = ID ( s , s i i +1 , T ) , where s i represents the scene at the time step i . Specifically, we use PointNet [49] to encode the point cloud and extract 3D features. These features are then combined with the instruction text embeddings and further processed by an MLP to generate the final 7-DoF action.

We first evaluate the quality of 4D scene predictions from our model across real and synthetic datasets in Sec. 5.1, then conduct experiments in RLBench to demonstrate how the 4D information helps the embodied tasks in Sec. 5.2. We provide more qualitative results and videos in the Supplementary and the website.

## 5.1. 4D Scene Prediction

## 5.1.1. Setup

Datasets. We conduct experiments on the real domain with 400 unseen samples in RT1 Fractal [6] and Bridage [61] dataset where depth and normal are estimated as in Sec. 4.1, and the synthetic domain with 200 unseen samples in RLBench [26], where ground truth depth and normal maps are directly accessible.

Metrics. We evaluate the video quality with FVD, SSIM, and PSNR; depth quality with AbsRel, δ 1 , and δ 2 ; normal maps quality with Mean, Median, and 11 25 . ◦ ; reconstructed point cloud quality with the L 1 Chamfer Distance. We generate 10 times per sample and report the average.

Baselines. We compare our method to two video diffusion models and a 4D point cloud diffusion model.

- · OpenSora [77], video diffusion model fine-tuned with LoRA on the same dataset without depth and normal annotations. To construct the full 4D scene, depth and normal are additional estimated given the predicted video with Rolling Depth and Marigold.
- · CogVideoX [69], video diffusion model fine-tuned with LoRA on the same dataset without depth and normal annotations. The 4D scene is obtained similarly to the above.
- · 4D Point-E, since no prior work directly generates dynamic scenes from the first frame and text inputs, we implemented a 4D point cloud diffusion model as the baseline, where we modify the Point-E [45] model by conditioning it on the mean of CLIP [50] features extracted from both text and image inputs, outputting T point clouds

| Methods     |   close box |   open drawer |   open jar |   open microwave |   put knife |   sweep to dustpan |   lid off |   weighing off |   water plants |
|-------------|-------------|---------------|------------|------------------|-------------|--------------------|-----------|----------------|----------------|
| Image-BC    |          53 |             4 |          0 |                5 |           0 |                  0 |        12 |             21 |              0 |
| UniPi ∗     |          81 |            67 |         38 |               72 |          66 |                 49 |        70 |             68 |             35 |
| 4DWM (Ours) |          88 |            80 |         44 |               70 |          70 |                 56 |        73 |             62 |             41 |

Table 3. TesserAct boosts the performance of action planning. We report the success rate averaged over 100 episodes for each task here.

| Method   |   PSNR |   SSIM |   LPIPS |   CLIP Score |   CLIP Aesthetic | Time Costs   |
|----------|--------|--------|---------|--------------|------------------|--------------|
| SoM[62]  |  10.94 |  24.02 |   73.82 |        66.67 |             3.61 | ∼ 2 hours    |
| Ours     |  12.99 |  42.62 |   60.51 |        83.02 |             3.73 | ∼ 1 mins     |

icantly less time. A qualitative result on the Bridge dataset is shown in Figure 4 (c).

Table 4. Novel view synthesis results on RLBench.

of size n , where T is set to 4 and n is set to 8192 due to computational constraints.

Implementation Details. We train our model on the collected 4D embodied video dataset using a multi-resolution training approach, predicting 49 frames at a time. For more details, please kindly refer to the Supplementary Materials.

## 5.1.2. Results and Analysis

TesserAct predicts high quality 4D scenes. As shown in Table 2, our model accurately predicts the depth and normal maps compared to video diffusion models with postestimation, verifying the effectiveness of our learned model. With better depth and normal maps, the 4D point clouds reconstructed with our method achieve the lowest Chamfer distances across real and synthetic datasets. The 4D Point-E method performs better than video diffusion models, particularly on RLBench, but still lags behind our approach. Additionally, training directly with point clouds is computationally expensive, restricting the number of frames used. In contrast, our model leverages an efficient representation with RGB-DN videos to generate more precise 4D scenes, particularly in capturing fine-grained details in dynamic scenes. We also show qualitative results in Figure 4 (a).

TesserAct synthesizes novel views efficiently. Our method could also perform monocular video to 4D tasks by predicting depth and normal sequences and generating point clouds. We conduct experiments on the task of novel view synthesis on RLBench and compare with Shape of Motion [62], a state-of-the-art video reconstruction approach that utilizes Gaussian splatting [33]. The input is a monocular front camera video, and we compare results from the overhead and left shoulder cameras. We report PSNR (reconstruction accuracy), SSIM (structural similarity), LPIPS (perceptual difference), CLIP Score (semantic match) [78], CLIP aesthetic (visual quality) [37], and Time costs in Table 4. The results show our method can synthesize novel views of higher visual quality and better alignment in signif-

Consistency and Regularization Loss are effective. We conduct ablation experiments on our newly designed loss terms in Sec. 4.3. The results are shown in Figure 3. The first two rows demonstrate the effect of the consistency loss, where we render frames from the same camera view at different time steps. The results show that the robot arm's movements are more coherent with the consistency loss applied. The last row highlights the role of the regularization loss. We display images of the same frame from three different views, revealing that this loss term helps improve the geometric accuracy of the reconstruction.

TesserAct shows generalization across scenes and embodiments. Our model demonstrates strong generalization capabilities. Benefiting from the knowledge of CogVideoX, the model achieves good generation on unseen scenes and unseen objects. Additionally, it performs well in crossembodiment scenarios, such as using the Bridge dataset robotic arm in the RT-1 environment. Figure 4 (b) shows a generalization result on unseen scenes and objects. More results can be found in the Supplementary Materials.

## 5.2. Embodied Action Planning

Dataset. We select 9 challenging tasks from RLBench [26] including tasks requiring high-precision grasping.

Metric. The success rate averaged over 100 episodes.

Baseline. We compare our method to a behavior cloning agent and a video-based world model.

- · Image-BC [27]: a behavior cloning agent that takes in an image and task instruction and outputs the 7-DoF actions.
- · UniPi ∗ [15]: a method that takes the task instruction and current image, predicts the future video, and uses a 2Dbased inverse dynamic policy to predict actions. For this baseline, we re-implement it by replacing the backbone with fine-tuned CogVideoX [69] for fair comparison.

Implementation Details. Wecollected 500 samples for each task to train the inverse dynamic model. Given an initial state during inference, we first predict and record all future keyframes. In subsequent actions, we only query the inverse dynamic model to obtain the corresponding actions by the current state and the predicted future state. We post-trained the model from Sec. 5.1, allowing the model to predict only the keyframes for each task in RLBench. Our maximum

Figure 4. Qualitative results of (a) In-domain 4D generation results. (b) Generalization over unseen scenes and objects. (c) Novel view synthesis.

<!-- image -->

frame length is 13, with a fixed resolution of 512 × 512.

## 5.2.1. Results and Analysis

The results are shown in Table 3, where our method outperforms video diffusion models and image behavior cloning agents in most of the tasks. This is because, in most tasks, 4D point clouds can reveal the geometry of objects, providing better spatial guidance for robotics planning, as seen in tasks like close box and open jar . At the same time, 3D information can assist with tool use, such as in tasks like sweep to dustpan and water plants . However, in the open microwave and weighing off tasks, the performance is not as good as the baseline, possibly because these tasks already have sufficient information in the 2D front image. Overall, these results highlight the potential of combining 4D scene prediction with inverse dynamic models to improve robotics task execution.

## 6. Conclusion

We learn a 4D generative world model, TesserAct, using a collected 4D embodied video dataset, which consists of robotic manipulation videos annotated with depth and normal information. To ensure both temporal and spatial consistency in scene reconstruction, we introduce two novel loss terms. Our experiments across synthetic and real-world datasets demonstrate that our model generates high-quality 4D scenes and significantly enhances the performance of downstream embodied tasks by leveraging 3D information. We believe that such world models will become increasingly powerful and essential, serving as a foundation for simulating the physical world and advancing the development of intelligent embodied agents. These models will enable fully offline policy training in the real world and facilitate planning through imagined rollouts within the learned world representation.

## 7. Limitations

While our RGB-DN representation of a 4D world model is cheap and easy to predict, it only captures a single surface of the world. To construct a more complete 4D world model, it may be interesting in the future to have a generative model that generates multiple RGB-DN views of the world, which can then be integrated to form a more complete 4D world model.
<|endofpaper|>